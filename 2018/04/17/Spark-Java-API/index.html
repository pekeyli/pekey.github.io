<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    
<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">



  <meta name="description" content="Spark Java API"/>













  <link rel="alternate" href="/default" title="Pekey">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.6.0" />



<link rel="canonical" href="http://yoursite.com/2018/04/17/Spark-Java-API/"/>


<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.6.0" />






  



  <script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>





  <script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
  <script id="leancloud">
    AV.init({
      appId: "qjhxkQG6FwXDocEOjCFjSJXX-gzGzoHsz",
      appKey: "l9zmzMJqV7WdUXj4PbIGEAzb"
    });
  </script>





    <title> Spark Java API - Pekey </title>
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/." class="logo">Pekey</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            首页
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            归档
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Pekey</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              首页
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              归档
            
          </a>
        </li>
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          Spark Java API
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-17
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/17/Spark-Java-API/"
             data-title="Spark Java API">
            posts.visits
          </div>
        
      </div>
    </header>

    
    
  <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">文章目录</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD如何创建"><span class="toc-text">RDD如何创建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#常用算子"><span class="toc-text">常用算子</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#filter"><span class="toc-text">filter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#map"><span class="toc-text">map</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#flatMap"><span class="toc-text">flatMap</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#distinct"><span class="toc-text">distinct</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#union"><span class="toc-text">union</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#intersection"><span class="toc-text">intersection</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#subtract"><span class="toc-text">subtract</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cartesian"><span class="toc-text">cartesian</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mapValues"><span class="toc-text">mapValues</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#flatMapValues"><span class="toc-text">flatMapValues</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsTextFile"><span class="toc-text">saveAsTextFile</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsSequenceFile"><span class="toc-text">saveAsSequenceFile</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsObjectFile"><span class="toc-text">saveAsObjectFile</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsHadoopFile"><span class="toc-text">saveAsHadoopFile</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsHadoopDataset"><span class="toc-text">saveAsHadoopDataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsNewAPIHadoopFile"><span class="toc-text">saveAsNewAPIHadoopFile</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsNewAPIHadoopDataset"><span class="toc-text">saveAsNewAPIHadoopDataset</span></a></li></ol></li></ol>
    </div>
  </div>


    <div class="post-content">
      
        <h2 id="RDD如何创建"><a href="#RDD如何创建" class="headerlink" title="RDD如何创建"></a>RDD如何创建</h2><p>　　　　<br>首先创建JavaSparkContext对象实例sc</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaSparkContext  sc = new JavaSparkContext(&quot;local&quot;,&quot;SparkTest&quot;);</span><br></pre></td></tr></table></figure>
<p>接受2个参数：<br>第一个参数表示运行方式（local、yarn-client、yarn-standalone等）<br>第二个参数表示应用名字</p>
<blockquote>
<p>直接从集合转化 <code>sc.parallelize(List(1,2,3,4,5,6,7,8,9,10))</code><br>从HDFS文件转化  <code>sc.textFile(&quot;hdfs://&quot;)</code><br>从本地文件转化  <code>sc.textFile(&quot;file:/&quot;)</code></p>
</blockquote>
<p>下面例子中list2就是根据data2List生成的一个RDD</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; list1 = new ArrayList&lt;String&gt;();</span><br><span class="line">list1.add(&quot;11,12,13,14,15&quot;);</span><br><span class="line">list1.add(&quot;aa,bb,cc,dd,ee&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; list2 = sc.parallelize(list1);</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="常用算子"><a href="#常用算子" class="headerlink" title="常用算子"></a>常用算子</h2><hr>
<h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><p>举例，在F:\sparktest\sample.txt 文件的内容如下</p>
<blockquote>
<p>aa bb cc aa aa aa dd dd ee ee ee ee<br>ff aa bb zks<br>ee kks<br>ee  zz zks<br>我要将包含zks的行的内容给找出来<br>scala版本</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val lines = sc.textFile(&quot;F:\\sparktest\\sample.txt&quot;).filter(line=&gt;line.contains(&quot;zks&quot;))</span><br><span class="line">//打印内容</span><br><span class="line">lines.collect().foreach(println(_));</span><br><span class="line">-------------输出------------------</span><br><span class="line">ff aa bb zks</span><br><span class="line">ee  zz zks</span><br></pre></td></tr></table></figure>
<p>java版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;F:\\sparktest\\sample.txt&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; zksRDD = lines.filter(new Function&lt;String, Boolean&gt;() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Boolean call(String s) throws Exception &#123;</span><br><span class="line">        return s.contains(&quot;zks&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">//打印内容</span><br><span class="line">List&lt;String&gt; zksCollect = zksRDD.collect();</span><br><span class="line">for (String str:zksCollect) &#123;</span><br><span class="line">    System.out.println(str);</span><br><span class="line">&#125;</span><br><span class="line">----------------输出-------------------</span><br><span class="line">ff aa bb zks</span><br><span class="line">ee  zz zks</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p>map() 接收一个函数，把这个函数用于 RDD 中的每个元素，将函数的返回结果作为结果RDD编程 ｜ 31<br>RDD 中对应元素的值 map是一对一的关系<br>举例，在F:\sparktest\sample.txt 文件的内容如下</p>
<blockquote>
<p>aa bb cc aa aa aa dd dd ee ee ee ee<br>ff aa bb zks<br>ee kks<br>ee  zz zks</p>
</blockquote>
<p>把每一行变成一个数组<br>scala版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//读取数据</span><br><span class="line">scala&gt; val lines = sc.textFile(&quot;F:\\sparktest\\sample.txt&quot;)</span><br><span class="line">//用map，对于每一行数据，按照空格分割成一个一个数组，然后返回的是一对一的关系</span><br><span class="line">scala&gt; var mapRDD = lines.map(line =&gt; line.split(&quot;\\s+&quot;))</span><br><span class="line">---------------输出-----------</span><br><span class="line">res0: Array[Array[String]] = Array(Array(aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee), Array(ff, aa, bb, zks), Array(ee, kks), Array(ee, zz, zks))</span><br><span class="line"></span><br><span class="line">//读取第一个元素</span><br><span class="line">scala&gt; mapRDD.first</span><br><span class="line">---输出----</span><br><span class="line">res1: Array[String] = Array(aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee)</span><br></pre></td></tr></table></figure>
<p>java版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Iterable&lt;String&gt;&gt; mapRDD = lines.map(new Function&lt;String, Iterable&lt;String&gt;&gt;() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Iterable&lt;String&gt; call(String s) throws Exception &#123;</span><br><span class="line">        String[] split = s.split(&quot;\\s+&quot;);</span><br><span class="line">        return Arrays.asList(split);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">//读取第一个元素</span><br><span class="line">System.out.println(mapRDD.first());</span><br><span class="line">---------------输出-------------</span><br><span class="line">[aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee]</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><p>有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap()<br>faltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器(想要了解更多，请参考scala的flatMap和map用法)<br>例如我们将数据切分为单词<br>scala版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val lines = sc.textFile(&quot;F:\\sparktest\\sample.txt&quot;)</span><br><span class="line">scala&gt; val flatMapRDD = lines.flatMap(line=&gt;line.split(&quot;\\s&quot;))</span><br><span class="line">scala&gt; flatMapRDD.first()</span><br><span class="line">---输出----</span><br><span class="line">res0: String = aa</span><br></pre></td></tr></table></figure>
<p>java版本，spark2.0以下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;F:\\sparktest\\sample.txt&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; flatMapRDD = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Iterable&lt;String&gt; call(String s) throws Exception &#123;</span><br><span class="line">        String[] split = s.split(&quot;\\s+&quot;);</span><br><span class="line">        return Arrays.asList(split);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">//输出第一个</span><br><span class="line">System.out.println(flatMapRDD.first());</span><br><span class="line">------------输出----------</span><br><span class="line">aa</span><br></pre></td></tr></table></figure>
<p>java版本，spark2.0以上<br>spark2.0以上，对flatMap的方法有所修改，就是flatMap中的Iterator和Iteratable的小区别</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; flatMapRDD = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Iterator&lt;String&gt; call(String s) throws Exception &#123;</span><br><span class="line">        String[] split = s.split(&quot;\\s+&quot;);</span><br><span class="line">        return Arrays.asList(split).iterator();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h3><p>distinct用于去重， 我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销<br>scala版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var RDD1 = sc.parallelize(List(&quot;aa&quot;,&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;,&quot;dd&quot;))</span><br><span class="line"></span><br><span class="line">scala&gt; RDD1.collect</span><br><span class="line">res3: Array[String] = Array(aa, aa, bb, cc, dd)</span><br><span class="line"></span><br><span class="line">scala&gt; var distinctRDD = RDD1.distinct</span><br><span class="line"></span><br><span class="line">scala&gt; distinctRDD.collect</span><br><span class="line">res5: Array[String] = Array(aa, dd, bb, cc)</span><br></pre></td></tr></table></figure></p>
<p>java版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));</span><br><span class="line">JavaRDD&lt;String&gt; distinctRDD = RDD1.distinct();</span><br><span class="line">List&lt;String&gt; collect = distinctRDD.collect();</span><br><span class="line">for (String str:collect) &#123;</span><br><span class="line">    System.out.print(str+&quot;, &quot;);</span><br><span class="line">&#125;</span><br><span class="line">---------输出----------</span><br><span class="line">aa, dd, bb, cc,</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><p>两个RDD进行合并<br>scala版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var RDD1 = sc.parallelize(List(&quot;aa&quot;,&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;,&quot;dd&quot;))</span><br><span class="line">scala&gt; var RDD2 = sc.parallelize(List(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;))</span><br><span class="line"></span><br><span class="line">scala&gt; RDD1.collect</span><br><span class="line">res6: Array[String] = Array(aa, aa, bb, cc, dd)</span><br><span class="line"></span><br><span class="line">scala&gt; RDD2.collect</span><br><span class="line">res7: Array[String] = Array(aa, dd, ff)</span><br><span class="line"></span><br><span class="line">scala&gt; RDD1.union(RDD2).collect</span><br><span class="line">res8: Array[String] = Array(aa, aa, bb, cc, dd, aa, dd, ff)</span><br></pre></td></tr></table></figure></p>
<p>java版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));</span><br><span class="line">JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));</span><br><span class="line">JavaRDD&lt;String&gt; unionRDD = RDD1.union(RDD2);</span><br><span class="line">List&lt;String&gt; collect = unionRDD.collect();</span><br><span class="line">for (String str:collect) &#123;</span><br><span class="line">    System.out.print(str+&quot;, &quot;);</span><br><span class="line">&#125;</span><br><span class="line">-----------输出---------</span><br><span class="line">aa, aa, bb, cc, dd, aa, dd, ff,</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h3><p>RDD1.intersection(RDD2) 返回两个RDD的交集，并且去重<br>intersection 需要混洗数据，比较浪费性能<br>scala版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var RDD1 = sc.parallelize(List(&quot;aa&quot;,&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;,&quot;dd&quot;))</span><br><span class="line">scala&gt; var RDD2 = sc.parallelize(List(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;))</span><br><span class="line"></span><br><span class="line">scala&gt; RDD1.collect</span><br><span class="line">res6: Array[String] = Array(aa, aa, bb, cc, dd)</span><br><span class="line"></span><br><span class="line">scala&gt; RDD2.collect</span><br><span class="line">res7: Array[String] = Array(aa, dd, ff)</span><br><span class="line"></span><br><span class="line">scala&gt; var insertsectionRDD = RDD1.intersection(RDD2)</span><br><span class="line">scala&gt; insertsectionRDD.collect</span><br><span class="line"></span><br><span class="line">res9: Array[String] = Array(aa, dd)</span><br></pre></td></tr></table></figure></p>
<p>java版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));</span><br><span class="line">JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));</span><br><span class="line">JavaRDD&lt;String&gt; intersectionRDD = RDD1.intersection(RDD2);</span><br><span class="line">List&lt;String&gt; collect = intersectionRDD.collect();</span><br><span class="line">for (String str:collect) &#123;</span><br><span class="line">    System.out.print(str+&quot; &quot;);</span><br><span class="line">&#125;</span><br><span class="line">-------------输出-----------</span><br><span class="line">aa dd</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h3><p>RDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重<br>scala版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;,&quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));</span><br><span class="line"></span><br><span class="line">scala&gt; var substractRDD =RDD1.subtract(RDD2)</span><br><span class="line"></span><br><span class="line">scala&gt;  substractRDD.collect</span><br><span class="line">res10: Array[String] = Array(bb, cc)</span><br></pre></td></tr></table></figure></p>
<p>java版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;,&quot;cc&quot;, &quot;dd&quot;));</span><br><span class="line">JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));</span><br><span class="line">JavaRDD&lt;String&gt; subtractRDD = RDD1.subtract(RDD2);</span><br><span class="line">List&lt;String&gt; collect = subtractRDD.collect();</span><br><span class="line">for (String str:collect) &#123;</span><br><span class="line">    System.out.print(str+&quot; &quot;);</span><br><span class="line">&#125;</span><br><span class="line">------------输出-----------------</span><br><span class="line">bb  cc</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="cartesian"><a href="#cartesian" class="headerlink" title="cartesian"></a>cartesian</h3><p>RDD1.cartesian(RDD2) 返回RDD1和RDD2的笛卡儿积，这个开销非常大</p>
<p>scala版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  var RDD1 = sc.parallelize(List(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;))</span><br><span class="line"></span><br><span class="line">scala&gt; var RDD2 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;))</span><br><span class="line"></span><br><span class="line">scala&gt; var cartesianRDD = RDD1.cartesian(RDD2)</span><br><span class="line"></span><br><span class="line">scala&gt; cartesianRDD.collect</span><br><span class="line">res11: Array[(String, String)] = Array((1,a), (1,b), (1,c), (2,a), (2,b), (2,c), (3,a), (3,b), (3,c))</span><br></pre></td></tr></table></figure></p>
<p>java版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;));</span><br><span class="line">JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;));</span><br><span class="line">JavaPairRDD&lt;String, String&gt; cartesian = RDD1.cartesian(RDD2);</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;String, String&gt;&gt; collect1 = cartesian.collect();</span><br><span class="line">for (Tuple2&lt;String, String&gt; tp:collect1) &#123;</span><br><span class="line">    System.out.println(&quot;(&quot;+tp._1+&quot; &quot;+tp._2+&quot;)&quot;);</span><br><span class="line">&#125;</span><br><span class="line">------------输出-----------------</span><br><span class="line">(1 a)</span><br><span class="line">(1 b)</span><br><span class="line">(1 c)</span><br><span class="line">(2 a)</span><br><span class="line">(2 b)</span><br><span class="line">(2 c)</span><br><span class="line">(3 a)</span><br><span class="line">(3 b)</span><br><span class="line">(3 c)</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="mapValues"><a href="#mapValues" class="headerlink" title="mapValues"></a>mapValues</h3><p>def mapValues<a href="f: (V" target="_blank" rel="noopener">U</a> =&gt; U): RDD[(K, U)]</p>
<p>同基本转换操作中的map，只不过mapValues是针对[K,V]中的V值进行map操作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((1,&quot;A&quot;),(2,&quot;B&quot;),(3,&quot;C&quot;),(4,&quot;D&quot;)),2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[27] at makeRDD at :21</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.mapValues(x =&gt; x + &quot;_&quot;).collect</span><br><span class="line">res26: Array[(Int, String)] = Array((1,A_), (2,B_), (3,C_), (4,D_))</span><br></pre></td></tr></table></figure>
<h3 id="flatMapValues"><a href="#flatMapValues" class="headerlink" title="flatMapValues"></a>flatMapValues</h3><p>def flatMapValues<a href="f: (V" target="_blank" rel="noopener">U</a> =&gt; TraversableOnce[U]): RDD[(K, U)]</p>
<p>同基本转换操作中的flatMap，只不过flatMapValues是针对[K,V]中的V值进行flatMap操作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.flatMapValues(x =&gt; x + &quot;_&quot;).collect</span><br><span class="line">res36: Array[(Int, Char)] = Array((1,A), (1,_), (2,B), (2,_), (3,C), (3,_), (4,D), (4,_))</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile"></a>saveAsTextFile</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def saveAsTextFile(path: String): Unit</span><br><span class="line"></span><br><span class="line">def saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec]): Unit</span><br></pre></td></tr></table></figure>
<p>saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。</p>
<p>codec参数可以指定压缩的类名。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">scala&gt; rdd1.saveAsTextFile(&quot;hdfs://cdh5/tmp/lxw1234.com/&quot;) //保存到HDFS</span><br><span class="line">hadoop fs -ls /tmp/lxw1234.com</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   2 lxw1234 supergroup        0 2015-07-10 09:15 /tmp/lxw1234.com/_SUCCESS</span><br><span class="line">-rw-r--r--   2 lxw1234 supergroup        21 2015-07-10 09:15 /tmp/lxw1234.com/part-00000</span><br><span class="line"></span><br><span class="line">hadoop fs -cat /tmp/lxw1234.com/part-00000</span><br></pre></td></tr></table></figure></p>
<p>注意：如果使用rdd1.saveAsTextFile(“file:///tmp/lxw1234.com”)将文件保存到本地文件系统，那么只会保存在Executor所在机器的本地目录。<br>指定压缩格式保存<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rdd1.saveAsTextFile(&quot;hdfs://cdh5/tmp/lxw1234.com/&quot;,classOf[com.hadoop.compression.lzo.LzopCodec])</span><br><span class="line"></span><br><span class="line">hadoop fs -ls /tmp/lxw1234.com</span><br><span class="line">-rw-r--r--   2 lxw1234 supergroup    0 2015-07-10 09:20 /tmp/lxw1234.com/_SUCCESS</span><br><span class="line">-rw-r--r--   2 lxw1234 supergroup    71 2015-07-10 09:20 /tmp/lxw1234.com/part-00000.lzo</span><br><span class="line"></span><br><span class="line">hadoop fs -text /tmp/lxw1234.com/part-00000.lzo</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="saveAsSequenceFile"><a href="#saveAsSequenceFile" class="headerlink" title="saveAsSequenceFile"></a>saveAsSequenceFile</h3><p>saveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。</p>
<p>用法同saveAsTextFile。</p>
<hr>
<h3 id="saveAsObjectFile"><a href="#saveAsObjectFile" class="headerlink" title="saveAsObjectFile"></a>saveAsObjectFile</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def saveAsObjectFile(path: String): Unit</span><br><span class="line"></span><br><span class="line">saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。</span><br><span class="line"></span><br><span class="line">对于HDFS，默认采用SequenceFile保存。</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">rdd1.saveAsObjectFile(&quot;hdfs://cdh5/tmp/lxw1234.com/&quot;)</span><br><span class="line"></span><br><span class="line">hadoop fs -cat /tmp/lxw1234.com/part-00000</span><br><span class="line">SEQ !org.apache.hadoop.io.NullWritable&quot;org.apache.hadoop.io.BytesWritableT</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="saveAsHadoopFile"><a href="#saveAsHadoopFile" class="headerlink" title="saveAsHadoopFile"></a>saveAsHadoopFile</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def saveAsHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[_ &lt;: OutputFormat[, ]], codec: Class[_ &lt;: CompressionCodec]): Unit</span><br><span class="line"></span><br><span class="line">def saveAsHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[_ &lt;: OutputFormat[, ]], conf: JobConf = …, codec: Option[Class[_ &lt;: CompressionCodec]] = None): Unit</span><br><span class="line"></span><br><span class="line">saveAsHadoopFile是将RDD存储在HDFS上的文件中，支持老版本Hadoop API。</span><br></pre></td></tr></table></figure>
<p>可以指定outputKeyClass、outputValueClass以及压缩格式。</p>
<p>每个分区输出一个文件。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">var rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;A&quot;,1),(&quot;B&quot;,6),(&quot;B&quot;,3),(&quot;B&quot;,7)))</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.mapred.TextOutputFormat</span><br><span class="line">import org.apache.hadoop.io.Text</span><br><span class="line">import org.apache.hadoop.io.IntWritable</span><br><span class="line"></span><br><span class="line">rdd1.saveAsHadoopFile(&quot;/tmp/lxw1234.com/&quot;,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]])</span><br><span class="line"></span><br><span class="line">rdd1.saveAsHadoopFile(&quot;/tmp/lxw1234.com/&quot;,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]],</span><br><span class="line">                      classOf[com.hadoop.compression.lzo.LzopCodec])</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="saveAsHadoopDataset"><a href="#saveAsHadoopDataset" class="headerlink" title="saveAsHadoopDataset"></a>saveAsHadoopDataset</h3><p>def saveAsHadoopDataset(conf: JobConf): Unit</p>
<p>saveAsHadoopDataset用于将RDD保存到除了HDFS的其他存储中，比如HBase。</p>
<p>在JobConf中，通常需要关注或者设置五个参数：</p>
<p>文件的保存路径、key值的class类型、value值的class类型、RDD的输出格式(OutputFormat)、以及压缩相关的参数。<br>使用saveAsHadoopDataset将RDD保存到HDFS中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line">import org.apache.hadoop.mapred.TextOutputFormat</span><br><span class="line">import org.apache.hadoop.io.Text</span><br><span class="line">import org.apache.hadoop.io.IntWritable</span><br><span class="line">import org.apache.hadoop.mapred.JobConf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;A&quot;,1),(&quot;B&quot;,6),(&quot;B&quot;,3),(&quot;B&quot;,7)))</span><br><span class="line">var jobConf = new JobConf()</span><br><span class="line">jobConf.setOutputFormat(classOf[TextOutputFormat[Text,IntWritable]])</span><br><span class="line">jobConf.setOutputKeyClass(classOf[Text])</span><br><span class="line">jobConf.setOutputValueClass(classOf[IntWritable])</span><br><span class="line">jobConf.set(&quot;mapred.output.dir&quot;,&quot;/tmp/lxw1234/&quot;)</span><br><span class="line">rdd1.saveAsHadoopDataset(jobConf)</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">hadoop fs -cat /tmp/lxw1234/part-00000</span><br><span class="line">A       2</span><br><span class="line">A       1</span><br><span class="line">hadoop fs -cat /tmp/lxw1234/part-00001</span><br><span class="line">B       6</span><br><span class="line">B       3</span><br><span class="line">B       7</span><br></pre></td></tr></table></figure></p>
<p>保存数据到HBASE<br>HBase建表：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">create ‘lxw1234′,&#123;NAME =&gt; ‘f1′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f2′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f3′,VERSIONS =&gt; 1&#125;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line">import org.apache.hadoop.mapred.TextOutputFormat</span><br><span class="line">import org.apache.hadoop.io.Text</span><br><span class="line">import org.apache.hadoop.io.IntWritable</span><br><span class="line">import org.apache.hadoop.mapred.JobConf</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration</span><br><span class="line">import org.apache.hadoop.hbase.mapred.TableOutputFormat</span><br><span class="line">import org.apache.hadoop.hbase.client.Put</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line"></span><br><span class="line">var conf = HBaseConfiguration.create()</span><br><span class="line">    var jobConf = new JobConf(conf)</span><br><span class="line">    jobConf.set(&quot;hbase.zookeeper.quorum&quot;,&quot;zkNode1,zkNode2,zkNode3&quot;)</span><br><span class="line">    jobConf.set(&quot;zookeeper.znode.parent&quot;,&quot;/hbase&quot;)</span><br><span class="line">    jobConf.set(TableOutputFormat.OUTPUT_TABLE,&quot;lxw1234&quot;)</span><br><span class="line">    jobConf.setOutputFormat(classOf[TableOutputFormat])</span><br><span class="line"></span><br><span class="line">    var rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;B&quot;,6),(&quot;C&quot;,7)))</span><br><span class="line">    rdd1.map(x =&gt;</span><br><span class="line">      &#123;</span><br><span class="line">        var put = new Put(Bytes.toBytes(x._1))</span><br><span class="line">        put.add(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;c1&quot;), Bytes.toBytes(x._2))</span><br><span class="line">        (new ImmutableBytesWritable,put)</span><br><span class="line">      &#125;</span><br><span class="line">    ).saveAsHadoopDataset(jobConf)</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">hbase(main):005:0&gt; scan &apos;lxw1234&apos;</span><br><span class="line">ROW     COLUMN+CELL                                                                                                </span><br><span class="line"> A       column=f1:c1, timestamp=1436504941187, value=\x00\x00\x00\x02                                              </span><br><span class="line"> B       column=f1:c1, timestamp=1436504941187, value=\x00\x00\x00\x06                                              </span><br><span class="line"> C       column=f1:c1, timestamp=1436504941187, value=\x00\x00\x00\x07                                              </span><br><span class="line">3 row(s) in 0.0550 seconds</span><br></pre></td></tr></table></figure></p>
<p>注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。</p>
<hr>
<h3 id="saveAsNewAPIHadoopFile"><a href="#saveAsNewAPIHadoopFile" class="headerlink" title="saveAsNewAPIHadoopFile"></a>saveAsNewAPIHadoopFile</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def saveAsNewAPIHadoopFile[F &lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unit</span><br><span class="line"></span><br><span class="line">def saveAsNewAPIHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[_ &lt;: OutputFormat[, ]], conf: Configuration = self.context.hadoopConfiguration): Unit</span><br></pre></td></tr></table></figure>
<p>saveAsNewAPIHadoopFile用于将RDD数据保存到HDFS上，使用新版本Hadoop API。</p>
<p>用法基本同saveAsHadoopFile。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat</span><br><span class="line">import org.apache.hadoop.io.Text</span><br><span class="line">import org.apache.hadoop.io.IntWritable</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;A&quot;,1),(&quot;B&quot;,6),(&quot;B&quot;,3),(&quot;B&quot;,7)))</span><br><span class="line">rdd1.saveAsNewAPIHadoopFile(&quot;/tmp/lxw1234/&quot;,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]])</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="saveAsNewAPIHadoopDataset"><a href="#saveAsNewAPIHadoopDataset" class="headerlink" title="saveAsNewAPIHadoopDataset"></a>saveAsNewAPIHadoopDataset</h3><p>def saveAsNewAPIHadoopDataset(conf: Configuration): Unit</p>
<p>作用同saveAsHadoopDataset,只不过采用新版本Hadoop API。</p>
<p>以写入HBase为例：</p>
<p>HBase建表：</p>
<p>create ‘lxw1234′,{NAME =&gt; ‘f1′,VERSIONS =&gt; 1},{NAME =&gt; ‘f2′,VERSIONS =&gt; 1},{NAME =&gt; ‘f3′,VERSIONS =&gt; 1}</p>
<p>完整的Spark应用程序：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">package com.lxw1234.test</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration</span><br><span class="line">import org.apache.hadoop.mapreduce.Job</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableOutputFormat</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line">import org.apache.hadoop.hbase.client.Result</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes</span><br><span class="line">import org.apache.hadoop.hbase.client.Put</span><br><span class="line"></span><br><span class="line">object Test &#123;</span><br><span class="line">  def main(args : Array[String]) &#123;</span><br><span class="line">   val sparkConf = new SparkConf().setMaster(&quot;spark://lxw1234.com:7077&quot;).setAppName(&quot;lxw1234.com&quot;)</span><br><span class="line">   val sc = new SparkContext(sparkConf);</span><br><span class="line">   var rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;B&quot;,6),(&quot;C&quot;,7)))</span><br><span class="line"></span><br><span class="line">    sc.hadoopConfiguration.set(&quot;hbase.zookeeper.quorum &quot;,&quot;zkNode1,zkNode2,zkNode3&quot;)</span><br><span class="line">    sc.hadoopConfiguration.set(&quot;zookeeper.znode.parent&quot;,&quot;/hbase&quot;)</span><br><span class="line">    sc.hadoopConfiguration.set(TableOutputFormat.OUTPUT_TABLE,&quot;lxw1234&quot;)</span><br><span class="line">    var job = new Job(sc.hadoopConfiguration)</span><br><span class="line">    job.setOutputKeyClass(classOf[ImmutableBytesWritable])</span><br><span class="line">    job.setOutputValueClass(classOf[Result])</span><br><span class="line">    job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])</span><br><span class="line"></span><br><span class="line">    rdd1.map(</span><br><span class="line">      x =&gt; &#123;</span><br><span class="line">        var put = new Put(Bytes.toBytes(x._1))</span><br><span class="line">        put.add(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;c1&quot;), Bytes.toBytes(x._2))</span><br><span class="line">        (new ImmutableBytesWritable,put)</span><br><span class="line">      &#125;    </span><br><span class="line">    ).saveAsNewAPIHadoopDataset(job.getConfiguration)</span><br><span class="line"></span><br><span class="line">    sc.stop()   </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。</p>

      
    </div>

    
      
      



      
      
    

    
      <footer class="post-footer">
        
        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2018/04/18/Hive环境搭建/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">Hive环境搭建</span>
        <span class="prev-text nav-mobile">上一篇</span>
      </a>
    
    
      <a class="next" href="/2018/04/17/Hbase环境搭建/">
        <span class="next-text nav-default">Hbase环境搭建</span>
        <span class="prev-text nav-mobile">下一篇</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

      </footer>
    

  </article>


          </div>
          
  <div class="comments" id="comments">
    
  </div>


        </div>
      </main>

      <footer id="footer" class="footer">

  <div class="social-links">
    
      
        
          <a href="mailto:pekeyli@qq.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a href="/PekeyLi" class="iconfont icon-weibo" title="weibo"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
    
    
      
      <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    
  </div>


<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">
    
    &copy; 
    
    2018

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Pekey Li</span>
  </span>
</div>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    
  
  

  



    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  


    <script type="text/javascript" src="/js/src/even.js?v=2.6.0"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=2.6.0"></script>

  </body>
</html>
