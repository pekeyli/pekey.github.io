<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    
<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">















  <link rel="alternate" href="/default" title="Pekey">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.6.0" />



<link rel="canonical" href="http://yoursite.com/"/>


<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.6.0" />






  



  <script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>





  <script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
  <script id="leancloud">
    AV.init({
      appId: "qjhxkQG6FwXDocEOjCFjSJXX-gzGzoHsz",
      appKey: "l9zmzMJqV7WdUXj4PbIGEAzb"
    });
  </script>





    <title> Pekey </title>
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/." class="logo">Pekey</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            首页
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            归档
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Pekey</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              首页
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              归档
            
          </a>
        </li>
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  <section id="posts" class="posts">
    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/20/Kylin-Cube构建过程学习/">Kylin Cube构建过程学习</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-20
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/20/Kylin-Cube构建过程学习/"
             data-title="Kylin Cube构建过程学习">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <p>参考文献：<a href="https://blog.bcmeng.com%20https://blog.bcmeng.com" target="_blank" rel="noopener">https://blog.bcmeng.com</a></p>
<hr>
<p>MapReduce 计算引擎 批量计算Cube，其输入是Hive表，输出是HBase的KeyValue，整个构建过程主要包含以下6步：</p>
<ol>
<li>建立Hive的大宽表； （MapReduce计算）</li>
<li>对需要字典编码的列计算列基数； （MapReduce计算）</li>
<li>构建字典； （JobServer计算 or MapReduce计算）</li>
<li>分层构建Cuboid； （MapReduce计算）</li>
<li>将Cuboid转为HBase的KeyValue结构（HFile）； （MapReduce计算）</li>
<li>元数据更新和垃圾回收。<h3 id="Cube-Build流程"><a href="#Cube-Build流程" class="headerlink" title="Cube Build流程"></a>Cube Build流程</h3></li>
</ol>
<p><strong>CubeController</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">private JobInstance buildInternal(String cubeName, TSRange tsRange, SegmentRange segRange, Map&lt;Integer, Long&gt; sourcePartitionOffsetStart, Map&lt;Integer, Long&gt; sourcePartitionOffsetEnd, String buildType, boolean force) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            String submitter = SecurityContextHolder.getContext().getAuthentication().getName();</span><br><span class="line">            CubeInstance cube = jobService.getCubeManager().getCube(cubeName);</span><br><span class="line"></span><br><span class="line">            if (cube == null) &#123;</span><br><span class="line">                throw new InternalErrorException(&quot;Cannot find cube &quot; + cubeName);</span><br><span class="line">            &#125;</span><br><span class="line">            return jobService.submitJob(cube, tsRange, segRange, sourcePartitionOffsetStart, sourcePartitionOffsetEnd,</span><br><span class="line">                    CubeBuildTypeEnum.valueOf(buildType), force, submitter);</span><br><span class="line">        &#125; catch (Throwable e) &#123;</span><br><span class="line">            logger.error(e.getLocalizedMessage(), e);</span><br><span class="line">            throw new InternalErrorException(e.getLocalizedMessage(), e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>JobService</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ISource source = SourceFactory.getSource(cube);</span><br><span class="line">SourcePartition src = new SourcePartition(tsRange, segRange, sourcePartitionOffsetStart,</span><br><span class="line">        sourcePartitionOffsetEnd);</span><br><span class="line">src = source.enrichSourcePartitionBeforeBuild(cube, src);</span><br><span class="line">newSeg = getCubeManager().appendSegment(cube, src);</span><br><span class="line">job = EngineFactory.createBatchCubingJob(newSeg, submitter);</span><br></pre></td></tr></table></figure></p>
<p><strong>EngineFactory</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/** Build a new cube segment, typically its time range appends to the end of current cube. */</span><br><span class="line">    public static DefaultChainedExecutable createBatchCubingJob(CubeSegment newSegment, String submitter) &#123;</span><br><span class="line">        return batchEngine(newSegment).createBatchCubingJob(newSegment, submitter);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p><strong>MRBatchCubingEngine2</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public DefaultChainedExecutable createBatchCubingJob(CubeSegment newSegment, String submitter) &#123;</span><br><span class="line">    return new BatchCubingJobBuilder2(newSegment, submitter).build();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>BatchCubingJobBuilder2</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">public CubingJob build() &#123;</span><br><span class="line">        logger.info(&quot;MR_V2 new job to BUILD segment &quot; + seg);</span><br><span class="line"></span><br><span class="line">        final CubingJob result = CubingJob.createBuildJob(seg, submitter, config);</span><br><span class="line">        final String jobId = result.getId();</span><br><span class="line">        final String cuboidRootPath = getCuboidRootPath(jobId);</span><br><span class="line"></span><br><span class="line">        // Phase 1: Create Flat Table &amp; Materialize Hive View in Lookup Tables</span><br><span class="line">        inputSide.addStepPhase1_CreateFlatTable(result);</span><br><span class="line"></span><br><span class="line">        // Phase 2: Build Dictionary</span><br><span class="line">        result.addTask(createFactDistinctColumnsStep(jobId));</span><br><span class="line"></span><br><span class="line">        if (isEnableUHCDictStep()) &#123;</span><br><span class="line">            result.addTask(createBuildUHCDictStep(jobId));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        result.addTask(createBuildDictionaryStep(jobId));</span><br><span class="line">        result.addTask(createSaveStatisticsStep(jobId));</span><br><span class="line">        outputSide.addStepPhase2_BuildDictionary(result);</span><br><span class="line"></span><br><span class="line">        // Phase 3: Build Cube</span><br><span class="line">        addLayerCubingSteps(result, jobId, cuboidRootPath); // layer cubing, only selected algorithm will execute</span><br><span class="line">        addInMemCubingSteps(result, jobId, cuboidRootPath); // inmem cubing, only selected algorithm will execute</span><br><span class="line">        outputSide.addStepPhase3_BuildCube(result);</span><br><span class="line"></span><br><span class="line">        // Phase 4: Update Metadata &amp; Cleanup</span><br><span class="line">        result.addTask(createUpdateCubeInfoAfterBuildStep(jobId));</span><br><span class="line">        inputSide.addStepPhase4_Cleanup(result);</span><br><span class="line">        outputSide.addStepPhase4_Cleanup(result);</span><br><span class="line"></span><br><span class="line">        return result;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="Phase-1-Create-Flat-Table-amp-Materialize-Hive-View-in-Lookup-Tables"><a href="#Phase-1-Create-Flat-Table-amp-Materialize-Hive-View-in-Lookup-Tables" class="headerlink" title="Phase 1: Create Flat Table &amp; Materialize Hive View in Lookup Tables"></a>Phase 1: Create Flat Table &amp; Materialize Hive View in Lookup Tables</h4><p><strong>BatchCubingJobBuilder2</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// Phase 1: Create Flat Table &amp; Materialize Hive View in Lookup Tables</span><br><span class="line">inputSide.addStepPhase1_CreateFlatTable(result);</span><br></pre></td></tr></table></figure></p>
<p><strong>HiveMRInput</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public void addStepPhase1_CreateFlatTable(DefaultChainedExecutable jobFlow) &#123;</span><br><span class="line">            final String cubeName = CubingExecutableUtil.getCubeName(jobFlow.getParams());</span><br><span class="line">            final KylinConfig cubeConfig = CubeManager.getInstance(KylinConfig.getInstanceFromEnv()).getCube(cubeName)</span><br><span class="line">                    .getConfig();</span><br><span class="line">            final String hiveInitStatements = JoinedFlatTable.generateHiveInitStatements(flatTableDatabase);</span><br><span class="line"></span><br><span class="line">            // create flat table first</span><br><span class="line">            addStepPhase1_DoCreateFlatTable(jobFlow);</span><br><span class="line"></span><br><span class="line">            // then count and redistribute</span><br><span class="line">            if (cubeConfig.isHiveRedistributeEnabled()) &#123;</span><br><span class="line">                jobFlow.addTask(createRedistributeFlatHiveTableStep(hiveInitStatements, cubeName));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            // special for hive</span><br><span class="line">            addStepPhase1_DoMaterializeLookupTable(jobFlow);</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">private AbstractExecutable createFlatHiveTableStep(String hiveInitStatements, String jobWorkingDir,</span><br><span class="line">                String cubeName) &#123;</span><br><span class="line">            //from hive to hive</span><br><span class="line">            final String dropTableHql = JoinedFlatTable.generateDropTableStatement(flatDesc);</span><br><span class="line">            final String createTableHql = JoinedFlatTable.generateCreateTableStatement(flatDesc, jobWorkingDir);</span><br><span class="line">            String insertDataHqls = JoinedFlatTable.generateInsertDataStatement(flatDesc);</span><br><span class="line"></span><br><span class="line">            CreateFlatHiveTableStep step = new CreateFlatHiveTableStep();</span><br><span class="line">            step.setInitStatement(hiveInitStatements);</span><br><span class="line">            step.setCreateTableStatement(dropTableHql + createTableHql + insertDataHqls);</span><br><span class="line">            CubingExecutableUtil.setCubeName(cubeName, step.getParams());</span><br><span class="line">            step.setName(ExecutableConstants.STEP_NAME_CREATE_FLAT_HIVE_TABLE);</span><br><span class="line">            return step;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p><strong>CreateFlatHiveTableStep</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">protected void createFlatHiveTable(KylinConfig config) throws IOException &#123;</span><br><span class="line">        final HiveCmdBuilder hiveCmdBuilder = new HiveCmdBuilder();</span><br><span class="line">        hiveCmdBuilder.overwriteHiveProps(config.getHiveConfigOverride());</span><br><span class="line">        hiveCmdBuilder.addStatement(getInitStatement());</span><br><span class="line">        hiveCmdBuilder.addStatementWithRedistributeBy(getCreateTableStatement());</span><br><span class="line">        final String cmd = hiveCmdBuilder.toString();</span><br><span class="line"></span><br><span class="line">        stepLogger.log(&quot;Create and distribute table, cmd: &quot;);</span><br><span class="line">        stepLogger.log(cmd);</span><br><span class="line"></span><br><span class="line">        Pair&lt;Integer, String&gt; response = config.getCliCommandExecutor().execute(cmd, stepLogger);</span><br><span class="line">        Map&lt;String, String&gt; info = stepLogger.getInfo();</span><br><span class="line"></span><br><span class="line">        //get the flat Hive table size</span><br><span class="line">        Matcher matcher = HDFS_LOCATION.matcher(cmd);</span><br><span class="line">        if (matcher.find()) &#123;</span><br><span class="line">            String hiveFlatTableHdfsUrl = matcher.group(1);</span><br><span class="line">            long size = getFileSize(hiveFlatTableHdfsUrl);</span><br><span class="line">            info.put(ExecutableConstants.HDFS_BYTES_WRITTEN, &quot;&quot; + size);</span><br><span class="line">            logger.info(&quot;HDFS_Bytes_Writen: &quot; + size);</span><br><span class="line">        &#125;</span><br><span class="line">        getManager().addJobInfo(getId(), info);</span><br><span class="line">        if (response.getFirst() != 0) &#123;</span><br><span class="line">            throw new RuntimeException(&quot;Failed to create flat hive table, error code &quot; + response.getFirst());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="Phase-2-Build-Dictionary"><a href="#Phase-2-Build-Dictionary" class="headerlink" title="Phase 2: Build Dictionary"></a>Phase 2: Build Dictionary</h4><h4 id="Phase-3-Build-Cube"><a href="#Phase-3-Build-Cube" class="headerlink" title="Phase 3: Build Cube"></a>Phase 3: Build Cube</h4><p><strong>BatchCubingJobBuilder2</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// Phase 3: Build Cube</span><br><span class="line">addLayerCubingSteps(result, jobId, cuboidRootPath); // layer cubing, only selected algorithm will execute</span><br><span class="line">addInMemCubingSteps(result, jobId, cuboidRootPath); // inmem cubing, only selected algorithm will execute</span><br><span class="line">outputSide.addStepPhase3_BuildCube(result);</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">protected void addLayerCubingSteps(final CubingJob result, final String jobId, final String cuboidRootPath) &#123;</span><br><span class="line">    // Don&apos;t know statistics so that tree cuboid scheduler is not determined. Determine the maxLevel at runtime</span><br><span class="line">    final int maxLevel = CuboidUtil.getLongestDepth(seg.getCuboidScheduler().getAllCuboidIds());</span><br><span class="line">    // base cuboid step</span><br><span class="line">    result.addTask(createBaseCuboidStep(getCuboidOutputPathsByLevel(cuboidRootPath, 0), jobId));</span><br><span class="line">    // n dim cuboid steps</span><br><span class="line">    for (int i = 1; i &lt;= maxLevel; i++) &#123;</span><br><span class="line">        result.addTask(createNDimensionCuboidStep(getCuboidOutputPathsByLevel(cuboidRootPath, i - 1), getCuboidOutputPathsByLevel(cuboidRootPath, i), i, jobId));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">private MapReduceExecutable createBaseCuboidStep(String cuboidOutputPath, String jobId) &#123;</span><br><span class="line">    // base cuboid job</span><br><span class="line">    MapReduceExecutable baseCuboidStep = new MapReduceExecutable();</span><br><span class="line"></span><br><span class="line">    StringBuilder cmd = new StringBuilder();</span><br><span class="line">    appendMapReduceParameters(cmd);</span><br><span class="line"></span><br><span class="line">    baseCuboidStep.setName(ExecutableConstants.STEP_NAME_BUILD_BASE_CUBOID);</span><br><span class="line"></span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_CUBE_NAME, seg.getRealization().getName());</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_SEGMENT_ID, seg.getUuid());</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_INPUT, &quot;FLAT_TABLE&quot;); // marks flat table input</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_OUTPUT, cuboidOutputPath);</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_JOB_NAME, &quot;Kylin_Base_Cuboid_Builder_&quot; + seg.getRealization().getName());</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_LEVEL, &quot;0&quot;);</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_CUBING_JOB_ID, jobId);</span><br><span class="line"></span><br><span class="line">    baseCuboidStep.setMapReduceParams(cmd.toString());</span><br><span class="line">    baseCuboidStep.setMapReduceJobClass(getBaseCuboidJob());</span><br><span class="line">    //        baseCuboidStep.setCounterSaveAs(CubingJob.SOURCE_RECORD_COUNT + &quot;,&quot; + CubingJob.SOURCE_SIZE_BYTES);</span><br><span class="line">    return baseCuboidStep;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">private MapReduceExecutable createNDimensionCuboidStep(String parentPath, String outputPath, int level, String jobId) &#123;</span><br><span class="line">    // ND cuboid job</span><br><span class="line">    MapReduceExecutable ndCuboidStep = new MapReduceExecutable();</span><br><span class="line"></span><br><span class="line">    ndCuboidStep.setName(ExecutableConstants.STEP_NAME_BUILD_N_D_CUBOID + &quot; : level &quot; + level);</span><br><span class="line">    StringBuilder cmd = new StringBuilder();</span><br><span class="line"></span><br><span class="line">    appendMapReduceParameters(cmd);</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_CUBE_NAME, seg.getRealization().getName());</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_SEGMENT_ID, seg.getUuid());</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_INPUT, parentPath);</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_OUTPUT, outputPath);</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_JOB_NAME, &quot;Kylin_ND-Cuboid_Builder_&quot; + seg.getRealization().getName() + &quot;_Step&quot;);</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_LEVEL, &quot;&quot; + level);</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_CUBING_JOB_ID, jobId);</span><br><span class="line"></span><br><span class="line">    ndCuboidStep.setMapReduceParams(cmd.toString());</span><br><span class="line">    ndCuboidStep.setMapReduceJobClass(getNDCuboidJob());</span><br><span class="line">    return ndCuboidStep;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li><p>MR Cube Build</p>
</li>
<li><p>Spark Cube Build</p>
</li>
</ul>
<blockquote>
<p>The “by-layer” Cubing divides a big task into a couple steps, and each step bases on the previous step’s output, so it can reuse the previous calculation and also avoid calculating from very beginning when there is a failure in between. These makes it as a reliable algorithm. When moving to Spark, we decide to keep this algorithm, that’s why we call this feature as “By layer Spark Cubing”.<br>Figure 3 is the DAG of Cubing in Spark, it illustrates the process in detail: In “Stage 5”, Kylin uses a HiveContext to read the intermediate Hive table, and then do a “map” operation, which is an one to one map, to encode the origin values into K-V bytes. On complete Kylin gets an intermediate encoded RDD. In “Stage 6”, the intermediate RDD is aggregated with a “reduceByKey” operation to get RDD-1, which is the base cuboid. Nextly, do an “flatMap” (one to many map) on RDD-1, because the base cuboid has N children cuboids. And so on, all levels’ RDDs get calculated. These RDDs will be persisted to distributed file system on complete, but be cached in memory for next level’s calculation. When child be generated, it will be removed from cache.<br><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqjhotn9owj30y00lsjuj.jpg" alt=""></p>
</blockquote>
<p>As we know, RDD (Resilient Distributed Dataset) is a basic concept in Spark. A collection of N-Dimension cuboids can be well described as an RDD, a N-Dimension Cube will have N+1 RDD. These RDDs have the parent/child relationship as the parent can be used to generate the children. With the parent RDD cached in memory, the child RDD’s generation can be much efficient than reading from disk. Figure 2 describes this process.</p>
<h4 id="Phase-4-Update-Metadata-amp-Cleanup"><a href="#Phase-4-Update-Metadata-amp-Cleanup" class="headerlink" title="Phase 4: Update Metadata &amp; Cleanup"></a>Phase 4: Update Metadata &amp; Cleanup</h4><h3 id="其他代码"><a href="#其他代码" class="headerlink" title="其他代码"></a>其他代码</h3>
        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/20/Speeding-ETL-Processing-in-Data-Warehouses-Using-High-Performance-Joins-for-Changed-Data-Capture/">Speeding ETL Processing in Data Warehouses Using High-Performance Joins for Changed Data Capture 论文学习</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-20
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/20/Speeding-ETL-Processing-in-Data-Warehouses-Using-High-Performance-Joins-for-Changed-Data-Capture/"
             data-title="Speeding ETL Processing in Data Warehouses Using High-Performance Joins for Changed Data Capture 论文学习">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <blockquote>
<p>1.Join and 2.Aggregation – which will play an integral role during preprocessing as well in manipulating and consolidating data in a data warehouse.</p>
</blockquote>
<p>翻译：</p>
<blockquote>
<p>1.加入和2.聚合 - 在预处理过程中以及在数据仓库中操纵和合并数据时，它们将扮演不可或缺的角色。</p>
</blockquote>
<hr>
<blockquote>
<p>ETL systems move data from OLTP systems to a data warehouse, but they can also be used to move data from one data warehouse to another. A heterogeneous architecture for an ETL system is one that extracts data from multiple sources. The complexity of this architecture arises from the fact that data from more than one source must be merged, rather than from the fact that data may be formatted differently in the different sources.</p>
</blockquote>
<p>翻译：</p>
<blockquote>
<p>ETL系统将数据从OLTP系统移动到数据仓库，但它们也可用于将数据从一个数据仓库移动到另一个数据仓库。 ETL系统的异构体系结构是从多个来源提取数据的体系结构。这种体系结构的复杂性源于以下事实：来自多个源的数据必须合并，而不是来自不同来源的数据格式不同的事实。</p>
</blockquote>
<hr>
<blockquote>
<p>Rather than replacing the information in the data warehouse with the data in the entire online transactional database, a join will match the primary key of the previously loaded record with its corresponding new record and then compare the data portions of the two records to determine if they’ve changed. In this way, only added, deleted, and altered records are updated, which significantly reduces elapsed time of database loads. By using a high-performance join for CDC, data warehouse updates can be performed with far greater efficiency.</p>
</blockquote>
<p>翻译：</p>
<blockquote>
<p>与其将数据仓库中的信息替换为整个在线交易数据库中的数据，联合会将先前加载的记录的主键与其相应的新记录相匹配，然后比较两个记录的数据部分以确定它们 已经改变了。 通过这种方式，只有添加，删除和更改的记录才会更新，这会显着减少数据库加载所花费的时间。 通过为CDC使用高性能连接，数据仓库更新可以以更高的效率执行。</p>
</blockquote>

        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/18/Implementation-of-Change-Data-Capture-in-ETL-Process-for-Data-Warehouse-Using-HDFS-and-Apache-Spark-论文学习/">Implementation of Change Data Capture in ETL Process for Data Warehouse Using HDFS and Apache Spark 论文学习</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-18
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/18/Implementation-of-Change-Data-Capture-in-ETL-Process-for-Data-Warehouse-Using-HDFS-and-Apache-Spark-论文学习/"
             data-title="Implementation of Change Data Capture in ETL Process for Data Warehouse Using HDFS and Apache Spark 论文学习">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <blockquote>
<ul>
<li>没什么干货，参考文献中的几篇有价值</li>
<li>可以参考这篇文章关于etl基本术语的介绍</li>
<li>snapshot difference技术 快照差分</li>
</ul>
</blockquote>
<h2 id="用到的技术"><a href="#用到的技术" class="headerlink" title="用到的技术"></a>用到的技术</h2><blockquote>
<p>The whole ETL process are performed using Pentaho Data Integration (PDI). Meanwhile, the proposed ETL process extract new and changed data using map-reduce framework and Apache Spoon. The transformation and loading process are performed using PDI. This section elaborates our big data cluster environment and the implementation of CDC.</p>
</blockquote>
<p>翻译：</p>
<blockquote>
<p>整个ETL过程使用Pentaho数据集成（PDI）执行。 同时，提议的ETL过程使用map-reduce框架和Apache Spoon提取新的和更改的数据。 转换和加载过程使用PDI执行。 本部分阐述了我们的大数据集群环境和CDC的实施。</p>
</blockquote>
<h2 id="mr的应用"><a href="#mr的应用" class="headerlink" title="mr的应用"></a>mr的应用</h2><blockquote>
<p>The CDC method can be implemented using MapReduce by adopting the divide and conquer principle similar to that conducted in the study by [9]. The data are divided into several parts, each to be processed separately. Then, each data processed will enter the reduce phase which will detect changes in the data.</p>
</blockquote>
<p>翻译：</p>
<blockquote>
<p>CDC方法可以通过采用类似于[9]研究中进行的分而治之原则，使用MapReduce来实现。 数据分成几个部分，每个部分分别处理。 然后，每个处理的数据将进入减少阶段，这将检测数据的变化。</p>
</blockquote>
<h2 id="sprak-sql-的应用"><a href="#sprak-sql-的应用" class="headerlink" title="sprak sql 的应用"></a>sprak sql 的应用</h2><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fqh59vdxk5j31ec0tkgra.jpg" alt=""></p>
<blockquote>
<p>In this study, the method used was by comparing data and finding differences between two data. The CDC method using snapshot difference was divided into several stages as illustrated in Figure 8. The first process was to take the data from the source. The data were taken by full load, which took the entire data from the beginning to the final condition. The results of the extraction was entered into the HDFS. The data was processed through the program created to run the CDC process using the snapshot difference technique. Snapshot difference was implemented <strong>using outer-join function from SparkSQL. The program was run by looking at the null value of the outer-join results of the two records, which will be the parameter for the newest data.</strong> If in the process damage to the data source was detected, the program could automatically change the data reference to compare and store the old data as a snapshot.</p>
</blockquote>
<p>翻译：</p>
<blockquote>
<p>在这项研究中，所使用的方法是通过比较数据和发现两个数据之间的差异。 使用快照差异的CDC方法分为几个阶段，如图8所示。第一个过程是从源头获取数据。 数据是以满负荷的方式获取的，这些数据从开始到最终的状态都采用了整个数据。 提取结果被输入到HDFS中。 数据通过创建的程序进行处理，以使用快照差异技术运行CDC进程。 快照差异是使用SparkSQL的外连接函数实现的。 该程序通过查看两条记录的外连接结果的空值来运行，这将成为最新数据的参数。 如果在检测到数据源损坏的过程中，程序可以自动更改数据引用以比较旧数据并将其存储为快照。</p>
</blockquote>

        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/18/Hive环境搭建/">Hive环境搭建</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-18
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/18/Hive环境搭建/"
             data-title="Hive环境搭建">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <h2 id="Mysql安装"><a href="#Mysql安装" class="headerlink" title="Mysql安装"></a>Mysql安装</h2><p>关闭selinux 服务</p>
<blockquote>
<p>vim /etc/selinux/config<br>···<br>SELINUX=disabled<br>···</p>
</blockquote>
<p>卸载MariaDB</p>
<blockquote>
<p>查看当前安装的mariadb包： rpm -qa | grep mariadb<br>强制卸载： rpm -e –nodeps mariadb-libs-5.5.44-2.el7.centos.x86_64</p>
</blockquote>
<p>查看是否已经安装了MySQL</p>
<blockquote>
<p>rpm -qa | grep -i mysql<br>find / -name mysql</p>
</blockquote>
<p><del>删除分散mysql文件</del></p>
<blockquote>
<p><del>find / -name mysql / # whereis mysql</del></p>
</blockquote>
<p><del>删除配置文档</del></p>
<blockquote>
<p><del>rm -rf /etc/my.cnf</del></p>
</blockquote>
<p><del>再次查找机器是否安装mysql</del></p>
<blockquote>
<p><del>rpm -qa|grep -i mysql</del></p>
</blockquote>
<p>下载mysql5.6的安装包，并上传到服务器上</p>
<blockquote>
<p>[root@nodeh1 mysql]# ll<br>total 236180<br>-rw-r–r– 1 root root 20278972 Sep 22 15:41 MySQL-client-5.6.31-1.el7.x86_64.rpm<br>-rw-r–r– 1 root root  3529244 Sep 22 15:40 MySQL-devel-5.6.31-1.el7.x86_64.rpm<br>-rw-r–r– 1 root root 61732192 Sep 22 15:42 MySQL-server-5.6.31-1.el7.x86_64.rpm<br>-rw-r–r– 1 root root  2101912 Sep 22 15:42 MySQL-shared-5.6.31-1.el7.x86_64.rpm<br>-rw-r–r– 1 root root  2299648 Sep 22 15:40 MySQL-shared-compat-5.6.31-1.el7.x86_64.rpm<br>-rw-r–r– 1 root root 59644132 Sep 22 15:40 MySQL-test-5.6.31-1.el7.x86_64.rpm</p>
</blockquote>
<p>安装mysql 的安装包 注意顺序</p>
<blockquote>
<p>rpm -ivh MySQL-common-5.6.31-1.el7.x86_64.rpm<br>rpm -ivh MySQL-libs-5.6.31-1.el7.x86_64.rpm<br>rpm -ivh MySQL-client-5.6.31-1.el7.x86_64.rpm<br>rpm -ivh MySQL-server-5.6.31-1.el7.x86_64.rpm<br>rpm -ivh MySQL-devel-5.6.31-1.el7.x86_64.rpm</p>
</blockquote>
<p>使用rpm安装方式安装mysql，安装的路径如下：</p>
<blockquote>
<p>a 数据库目录<br>/var/lib/mysql/<br>b 配置文件<br>/usr/share/mysql(mysql.server命令及配置文件)<br>c 相关命令<br>/usr/bin(mysqladmin mysqldump等命令)<br>d 启动脚本<br>/etc/rc.d/init.d/(启动脚本文件mysql的目录)<br>e /etc/my.conf</p>
</blockquote>
<p>修改字符集和数据存储路径 配置/etc/my.cnf文件,设置如下键值来启用一起有用的选项和 UTF-8 字符集.</p>
<blockquote>
<p>cat /etc/my.cnf<br>[mysqld]<br>···<br>innodb_file_per_table<br>max_connections = 4096<br>collation-server = utf8_general_ci<br>character-set-server = utf8</p>
</blockquote>
<p>如果无法启动mysql服务</p>
<blockquote>
<p>chown mysql:mysql -R /var/lib/mysql</p>
</blockquote>
<p>如果遇到Access denied for user ‘root‘@’localhost’ (using password: YES)</p>
<blockquote>
<ol>
<li>打开MySQL目录下的my.ini文件，在文件的最后添加一行“skip-grant-tables”，保存并关闭文件，重启服务。</li>
<li>通过cmd行进入MySQL的bin目录，输入“mysql -u root -p”(不输入密码)，回车即可进入数据库;</li>
<li>执行<code>mysql&gt; use mysql;</code>，使用mysql数据库;</li>
<li><code>mysql&gt; update mysql.user set authentication_string=password(&quot;root&quot;) where user=&quot;root&quot; and Host = &quot;localhost&quot;;</code></li>
<li>打开MySQL目录下的my.ini文件，删除最后一行的“skip-grant-tables”，保存并关闭文件</li>
<li><code>mysql&gt; flush privileges;</code></li>
<li><code>mysql&gt; quit;</code></li>
</ol>
</blockquote>
<p>初始化MySQL及设置密码</p>
<blockquote>
<p>mysqld –initialize<br>会生成一个 root 账户密码，密码在log文件里<br>cat /var/log/mysqld.log<br><code>2017-04-13T10:00:37.229524Z 1 [Note] A temporary password is generated for root@localhost: %kWTz,Ml?3Zs</code></p>
</blockquote>
<blockquote>
<p>systemctl start mysqld.service</p>
</blockquote>
<blockquote>
<p>ALTER USER ‘root‘@’localhost’ IDENTIFIED BY ‘new_password’;</p>
</blockquote>
<p>设置mysql开机启动</p>
<blockquote>
<p>systemctl restart mysqld.service<br>systemctl enable mysqld.service</p>
</blockquote>
<p>设置mysql允许远程登陆</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; use mysql;</span><br><span class="line">mysql&gt; select host,user,password from user;</span><br><span class="line">| host                  | user | password                                  |</span><br><span class="line">| localhost             | root | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 |</span><br><span class="line">| localhost.localdomain | root | *1237E2CE819C427B0D8174456DD83C47480D37E8 |</span><br><span class="line">| 127.0.0.1             | root | *1237E2CE819C427B0D8174456DD83C47480D37E8 |</span><br><span class="line">| ::1                   | root | *1237E2CE819C427B0D8174456DD83C47480D37E8 |</span><br><span class="line">mysql&gt; update user set host=&apos;%&apos; where user=&apos;root&apos; and host=&apos;localhost&apos;;</span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure>
<h3 id="MySQL-is-running-but-PID-file-could-not-be-found"><a href="#MySQL-is-running-but-PID-file-could-not-be-found" class="headerlink" title="MySQL is running but PID file could not be found"></a>MySQL is running but PID file could not be found</h3><p>在Linux 中，当你启动或者重启 MySQL 时，报关于 PID file 的错误</p>
<blockquote>
<p>第一步：找到   mysql 中 data 目录下的 mysql-bin.index 文件，然后删除<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">find / -name mysql-bin.index</span><br><span class="line">rm -rf  /phpstudy/data/mysql-bin.index</span><br></pre></td></tr></table></figure></p>
</blockquote>
<blockquote>
<p>第二步：找到 并 kill 所有关于 mysql 或者 mysqld 的进程</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps -aux | grep mysql</span><br><span class="line">kill 进程号</span><br></pre></td></tr></table></figure>
<p>可以在查看下进程中是否有 mysql 进程，确保 kill 干净，再看看 还有没有 mysql-bin.index文件（进程没杀死之前可能会生成）&lt;有必要&gt;</p>
<blockquote>
<p>命令：  service mysqld status</p>
</blockquote>
<hr>
<h3 id="mysqld：未被识别的服务"><a href="#mysqld：未被识别的服务" class="headerlink" title="mysqld：未被识别的服务"></a>mysqld：未被识别的服务</h3><p>遇到这样的错误，是由于 /etc/init.d/  不存在 mysqld 这个命令（有的人安装完环境后存在，是因为你的安装包中有这样的命令将 mysql.server 文件 copy 到 /etc/init.d/ 下面了）</p>
<blockquote>
<p>1.首先你需要找到 mysql.server 文件，这个 和 mysqld 文件是一模一样的，只不过文件名不相同，执行命令：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find  /  -name mysql.server</span><br></pre></td></tr></table></figure>
<blockquote>
<p>2.copy mysql.server 文件到 /etc/init.d/ 目录下，重命名文件为 mysqld，执行命令：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp  /phpstudy/mysql/support-files/mysql.server  /etc/init.d/mysqld</span><br></pre></td></tr></table></figure>
<blockquote>
<p>然后 再  <code>service mysqld status</code> 这个问题就解决了。</p>
</blockquote>
<h2 id="Hive配置过程"><a href="#Hive配置过程" class="headerlink" title="Hive配置过程"></a>Hive配置过程</h2><ul>
<li><p>hive-site.xml</p>
<blockquote>
<p><code>cp $HIVE_HOME/conf/hive-default.xml.template $HIVE_HOME/conf/hive-site.xml</code></p>
</blockquote>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;jdbc:mysql://127.0.0.1:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;new_password&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="iotmp"><a href="#iotmp" class="headerlink" title="iotmp"></a>iotmp</h3><blockquote>
<p>建立临时目录<br>mkdir /home/grid/hive/iotmp<br>关联找到hive-site.xml里面，将iotmp的对应配置修改成真实地址。<br>${system:java.io.tmpdir} 改为真实物理路径。</p>
</blockquote>
<h2 id="Hive启动过程"><a href="#Hive启动过程" class="headerlink" title="Hive启动过程"></a>Hive启动过程</h2><blockquote>
<p>nohup hive –service metastore -v &amp;</p>
</blockquote>

        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/17/Spark-Java-API/">Spark Java API</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-17
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/17/Spark-Java-API/"
             data-title="Spark Java API">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <h3 id="RDD如何创建"><a href="#RDD如何创建" class="headerlink" title="RDD如何创建"></a>RDD如何创建</h3><p>　　　　<br>首先创建JavaSparkContext对象实例sc</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaSparkContext  sc = new JavaSparkContext(&quot;local&quot;,&quot;SparkTest&quot;);</span><br></pre></td></tr></table></figure>
<p>接受2个参数：<br>第一个参数表示运行方式（local、yarn-client、yarn-standalone等）<br>第二个参数表示应用名字</p>
<blockquote>
<p>直接从集合转化 <code>sc.parallelize(List(1,2,3,4,5,6,7,8,9,10))</code><br>从HDFS文件转化  <code>sc.textFile(&quot;hdfs://&quot;)</code><br>从本地文件转化  <code>sc.textFile(&quot;file:/&quot;)</code></p>
</blockquote>
<p>下面例子中list2就是根据data2List生成的一个RDD</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; list1 = new ArrayList&lt;String&gt;();</span><br><span class="line">list1.add(&quot;11,12,13,14,15&quot;);</span><br><span class="line">list1.add(&quot;aa,bb,cc,dd,ee&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; list2 = sc.parallelize(list1);</span><br></pre></td></tr></table></figure>
<h3 id="常用算子"><a href="#常用算子" class="headerlink" title="常用算子"></a>常用算子</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String []&gt; rdd = list2.map(</span><br><span class="line">    new Function&lt;String, String []&gt;() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        public String [] call(String arg0) throws Exception &#123;</span><br><span class="line">            return arg0.split(&quot;,&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">);</span><br><span class="line">System.out.println(Util.printAList(rdd.collect()));</span><br></pre></td></tr></table></figure>

        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/17/Hbase环境搭建/">Hbase环境搭建</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-17
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/17/Hbase环境搭建/"
             data-title="Hbase环境搭建">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <h3 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h3><ul>
<li>hbase-env.sh</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/home/grid/jdk1.7.0_75  </span><br><span class="line">export HBASE_HOME=/home/grid/hbase  </span><br><span class="line">export HBASE_LOG_DIR=/tmp/grid/logs  </span><br><span class="line">export HBASE_MANAGES_ZK=true</span><br></pre></td></tr></table></figure>
<ul>
<li>hbase-site.xml</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;  </span><br><span class="line">    &lt;property&gt;  </span><br><span class="line">        &lt;name&gt;hbase.rootdir&lt;/name&gt; # 设置 hbase 数据库存放数据的目录  </span><br><span class="line">        &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt;  </span><br><span class="line">    &lt;/property&gt;  </span><br><span class="line"></span><br><span class="line">    &lt;property&gt;  </span><br><span class="line">        &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;  # 打开 hbase 分布模式  </span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;  </span><br><span class="line">    &lt;/property&gt;  </span><br><span class="line"></span><br><span class="line">    &lt;property&gt;  </span><br><span class="line">        &lt;name&gt;hbase.master&lt;/name&gt; # 指定 hbase 集群主控节点  </span><br><span class="line">        &lt;value&gt;master:60000&lt;/value&gt;  </span><br><span class="line">    &lt;/property&gt;  </span><br><span class="line"></span><br><span class="line">    &lt;property&gt;  </span><br><span class="line">        &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;  </span><br><span class="line">        &lt;value&gt;master,slave1,slave2&lt;/value&gt; # 指定 zookeeper 集群节点名 , 因为是由 zookeeper 表决算法决定的  </span><br><span class="line">    &lt;/property&gt;  </span><br><span class="line"></span><br><span class="line">    &lt;property&gt;  </span><br><span class="line">        &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; # 指 zookeeper 集群 data 目录  </span><br><span class="line">        &lt;value&gt;/home/grid/hbase/zookeeper&lt;/value&gt;  </span><br><span class="line">    &lt;/property&gt;  </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>regionservers</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slave1  </span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>
<h3 id="启动过程"><a href="#启动过程" class="headerlink" title="启动过程"></a>启动过程</h3><ol>
<li><p>启动Hadoop</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$HADOOP_HOME/sbin/start-dfs.sh  </span><br><span class="line">$HADOOP_HOME/sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动Hbase</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$HBASE_HOME/bin/start-hbase.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看hbase启动情况<br>在master上有HQuorumPeer和HMaster进程，在slave1、slave2上有HQuorumPeer和HRegionServer进程</p>
<blockquote>
<p>通过浏览器验证：<a href="http://nodeh1:16010/master-status" target="_blank" rel="noopener">http://nodeh1:16010/master-status</a></p>
</blockquote>
</li>
<li><p>命令行启动</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/home/grid/hbase/bin/hbase shell</span><br></pre></td></tr></table></figure>
</li>
</ol>

        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/17/Hadoop环境搭建/">Hadoop环境搭建</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-17
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/17/Hadoop环境搭建/"
             data-title="Hadoop环境搭建">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <h3 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h3><ul>
<li><p>hadoop-env.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/java/jdk1.7.0_80</span><br><span class="line">export HADOOP_PREFIX=/opt/hadoop-2.6.4</span><br></pre></td></tr></table></figure>
</li>
<li><p>core-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://master:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/opt/hadoop-2.6.4/tmp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：tmp目录需提前创建</p>
</blockquote>
</li>
<li><p>hdfs-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>数据有三个副本</p>
</blockquote>
</li>
<li><p>mapred-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>yarn-env.sh</p>
<blockquote>
<p>增加 JAVA_HOME 配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/java/jdk1.7.0_80</span><br></pre></td></tr></table></figure>
</blockquote>
</li>
<li><p>yarn-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Site specific YARN configuration properties --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;nodeh1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>slaves</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master</span><br><span class="line">slave01</span><br><span class="line">slave02</span><br></pre></td></tr></table></figure>
<blockquote>
<p>master 即作为 NameNode 也作为 DataNode。</p>
</blockquote>
</li>
</ul>
<h3 id="启动过程"><a href="#启动过程" class="headerlink" title="启动过程"></a>启动过程</h3><ol>
<li><p>格式化文件系统，在 master 上执行以下命令：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动 NameNode 和 DateNode，执行 <code>start-dfs.sh</code>，使用 jps 命令查看进程。</p>
</li>
<li><p>输入地址：<a href="http://master:50070/" target="_blank" rel="noopener">http://master:50070/</a> 可以查看 NameNode 信息。</p>
</li>
<li><p>启动 ResourceManager 和 NodeManager，运行 <code>start-yarn.sh</code>, 使用 jps 查看  进程</p>
</li>
</ol>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount /data/input /data/output/result</span><br></pre></td></tr></table></figure>

        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/17/Spark环境搭建/">Spark环境搭建</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-17
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/17/Spark环境搭建/"
             data-title="Spark环境搭建">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <h3 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h3><p>进入 Spark 安装目录下的 conf 目录， 拷贝 spark-env.sh.template 到 spark-env.sh。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure></p>
<p>编辑 spark-env.sh，在其中添加以下配置信息：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export SCALA_HOME=/opt/scala-2.11.8</span><br><span class="line">export JAVA_HOME=/opt/java/jdk1.7.0_80</span><br><span class="line">export SPARK_MASTER_IP=192.168.109.137</span><br><span class="line">export SPARK_WORKER_MEMORY=1g</span><br><span class="line">export HADOOP_CONF_DIR=/opt/hadoop-2.6.4/etc/hadoop</span><br></pre></td></tr></table></figure></p>
<p>将 slaves.template 拷贝到 slaves， 编辑其内容为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master</span><br><span class="line">slave01</span><br><span class="line">slave02</span><br></pre></td></tr></table></figure></p>
<p>即 master 既是 Master 节点又是 Worker 节点。</p>
<h3 id="启动过程"><a href="#启动过程" class="headerlink" title="启动过程"></a>启动过程</h3><blockquote>
<ol>
<li>运行 start-master.sh，启动Master节点，可以看到master上多了一个新进程Master。</li>
<li>运行 start-slaves.sh，启动所有Worker节点，在 master、slave01和slave02上使用jps命令，可以发现都启动了一个Worker进程</li>
<li>访问 <a href="http://master:8080" target="_blank" rel="noopener">http://master:8080</a>  浏览器查看Spark集群信息。</li>
<li>运行 spark-shell，可以进入Spark的shell控制台</li>
<li>访问 <a href="http://master:4040" target="_blank" rel="noopener">http://master:4040</a>  浏览器访问SparkUI， 可以从SparkUI上查看一些 如环境变量、Job、Executor等信息。</li>
</ol>
</blockquote>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>计算 π 的近似值<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$SPARK_HOME/bin/run-example SparkPi 2&gt;&amp;1 | grep &quot;Pi is roughly&quot;</span><br></pre></td></tr></table></figure></p>

        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/16/环境搭建的常用命令/">集群环境搭建的常用命令</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-16
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/16/环境搭建的常用命令/"
             data-title="集群环境搭建的常用命令">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <h4 id="关闭防火墙和SELinux"><a href="#关闭防火墙和SELinux" class="headerlink" title="关闭防火墙和SELinux"></a>关闭防火墙和SELinux</h4><blockquote>
<p><strong>关闭命令：</strong> service iptables stop<br><strong>永久关闭防火墙：</strong> chkconfig iptables off<br><strong>查看防火墙关闭状态:</strong>  service iptables status</p>
</blockquote>
<blockquote>
<p>关闭selinux 服务<br>vim /etc/selinux/config<br>修改 SELINUX=disabled</p>
</blockquote>
<h4 id="无密码登录"><a href="#无密码登录" class="headerlink" title="无密码登录"></a>无密码登录</h4><blockquote>
<p>ssh-keygen -t rsa<br>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>chmod 600 ~/.ssh/authorized_keys<br>ssh localhost</p>
</blockquote>
<h4 id="显示文件夹大小并排序"><a href="#显示文件夹大小并排序" class="headerlink" title="显示文件夹大小并排序"></a>显示文件夹大小并排序</h4><blockquote>
<p>du -h –max-depth=1 ./  | sort -n</p>
</blockquote>
<h4 id="将旧版本的jdk升级为新版本"><a href="#将旧版本的jdk升级为新版本" class="headerlink" title="将旧版本的jdk升级为新版本"></a>将旧版本的jdk升级为新版本</h4><blockquote>
<p>sudo update-alternatives –install /usr/bin/java java /opt/soft/jdk/bin/java 300<br>sudo update-alternatives –install /usr/bin/javac javac /opt/soft/jdk/bin/javac 300<br>sudo update-alternatives –config java</p>
</blockquote>
<h4 id="ecilpse访问的权限设置"><a href="#ecilpse访问的权限设置" class="headerlink" title="ecilpse访问的权限设置"></a>ecilpse访问的权限设置</h4><blockquote>
<p>groupadd supergroup # 添加supergroup组<br>useradd -g supergroup hadoop # 添加hadoop用户到supergroup组<br>hadoop fs -chmod 777 /</p>
</blockquote>
<h4 id="查看系统版本"><a href="#查看系统版本" class="headerlink" title="查看系统版本"></a>查看系统版本</h4><blockquote>
<p><strong>其他系列</strong> cat /proc/version<br><strong>redhat系列</strong> cat /etc/redhat-release</p>
</blockquote>
<h4 id="修改hostname"><a href="#修改hostname" class="headerlink" title="修改hostname"></a>修改hostname</h4><blockquote>
<p><strong>centos 6</strong><br>vim /etc/sysconfig/network<br>hostname yourhostname<br><strong>centos 7</strong><br>hostnamectl set-hostname yourhostname</p>
</blockquote>
<h4 id="Spark-上传jar包"><a href="#Spark-上传jar包" class="headerlink" title="Spark 上传jar包"></a>Spark 上传jar包</h4><blockquote>
<p>/opt/moudles/spark-1.6.1/bin/spark-submit –class SparkWordCount sparkStudy.jar  –master=spark://192.168.20.171:7077</p>
</blockquote>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><blockquote>
<p>chmod -R 777 ./spark</p>
</blockquote>

        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/11/SpagoBI5.2开发环境搭建/">SpagoBI5.2开发环境搭建</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-11
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/11/SpagoBI5.2开发环境搭建/"
             data-title="SpagoBI5.2开发环境搭建">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <p>本文暂不介绍kylin具体的搭建过程，而是将遇到的问题进行了总结，具体的搭建过程可能在后续进行更新。</p>
<hr>
<p>本地运行SpagoBI时需要JDK1.8的运行环境。</p>
<hr>
<p>表图引擎模块的action</p>
<blockquote>
<p>Bichartengine/WEB-INF/conf/commons/actions.xml</p>
</blockquote>
<p>整个项目的Service</p>
<blockquote>
<p>Spagobi-core/src/it/eng/spagobi/wepp/service</p>
</blockquote>
<hr>
<p>导入sql文件出错时</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; SET GLOBAL sql_mode = &apos;&apos;;   </span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.00 sec)   </span><br><span class="line">mysql&gt; commit;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line">mysql&gt; exit;</span><br></pre></td></tr></table></figure>
<hr>
<p>按照从SVN中导出的代码的Server.xml进行修改</p>
<blockquote>
<p>同时修改hibernate.cfg.xml<br>配置数据源 hibernate.cfg.xml (SpagoBI/src/main/resources)</p>
</blockquote>
<p>注意Server.xml中配置数据库时注意大小写</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&lt;GlobalNamingResources&gt;</span><br><span class="line">    &lt;!-- Editable user database that can also be used by</span><br><span class="line">         UserDatabaseRealm to authenticate users</span><br><span class="line">    --&gt;</span><br><span class="line">    &lt;Resource auth=&quot;Container&quot; description=&quot;User database that can be updated and saved&quot; factory=&quot;org.apache.catalina.users.MemoryUserDatabaseFactory&quot; name=&quot;UserDatabase&quot; pathname=&quot;conf/tomcat-users.xml&quot; type=&quot;org.apache.catalina.UserDatabase”/&gt;</span><br><span class="line"></span><br><span class="line">    &lt;Environment name=&quot;spagobi_resource_path&quot; type=&quot;java.lang.String&quot; value=&quot;C:\progetti\spagobi2.0\workspace\resources&quot;/&gt;</span><br><span class="line">    &lt;Environment name=&quot;spagobi_sso_class&quot; type=&quot;java.lang.String&quot; value=&quot;it.eng.spagobi.services.common.FakeSsoService&quot;/&gt;</span><br><span class="line">    &lt;Environment name=&quot;spagobi_service_url&quot; type=&quot;java.lang.String&quot; value=&quot;http://localhost:8080/SpagoBI&quot;/&gt;    </span><br><span class="line">    &lt;Environment name=&quot;spagobi_host_url&quot; type=&quot;java.lang.String&quot; value=&quot;http://localhost:8080&quot;/&gt;</span><br><span class="line"></span><br><span class="line">    &lt;Resource auth=&quot;Container&quot; factory=&quot;de.myfoo.commonj.work.FooWorkManagerFactory&quot; maxThreads=&quot;5&quot;</span><br><span class="line">          name=&quot;wm/SpagoWorkManager&quot; type=&quot;commonj.work.WorkManager&quot;/&gt;</span><br><span class="line"></span><br><span class="line">    &lt;Resource auth=&quot;Container&quot; driverClassName=&quot;com.mysql.jdbc.Driver&quot;</span><br><span class="line">          maxActive=&quot;20&quot; maxIdle=&quot;10&quot; maxWait=&quot;-1&quot; name=&quot;jdbc/foodmart&quot;</span><br><span class="line">          password=&quot;root&quot; type=&quot;javax.sql.DataSource&quot;</span><br><span class="line">          url=&quot;jdbc:mysql://localhost/foodmart&quot; username=&quot;root&quot;/&gt;          </span><br><span class="line"></span><br><span class="line">    &lt;Resource name=&quot;jdbc/spagobi&quot; auth=&quot;Container&quot;</span><br><span class="line">          type=&quot;javax.sql.DataSource&quot; driverClassName=&quot;com.mysql.jdbc.Driver&quot;</span><br><span class="line">          url=&quot;jdbc:mysql://localhost/spagobi&quot;</span><br><span class="line">          username=&quot;root&quot; password=&quot;root&quot; maxActive=&quot;20&quot; maxIdle=&quot;10&quot;</span><br><span class="line">          maxWait=&quot;-1&quot;/&gt;  </span><br><span class="line"></span><br><span class="line">  &lt;/GlobalNamingResources&gt;</span><br></pre></td></tr></table></figure>
<hr>
<p>虽然代码很多url等都写入了配置文件，但<strong>server.xml</strong>中的<strong>host url</strong>还是需要手动修改</p>
<hr>
<p>记得将符合自己mysql版本的jar包放到tomcat的lib中</p>
<hr>
<blockquote>
<p>Unsupported major.minor version 52.0</p>
</blockquote>
<p>这个错误不要把JRE更换到高版本，更换到高版本会报一个新的15.0错误，没用，而是应该修改<strong>web.xml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;web-app xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot; xmlns:web=&quot;http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd&quot; id=&quot;WebApp_ID&quot; version=&quot;3.0&quot; metadata-complete=&quot;true” &gt;</span><br></pre></td></tr></table></figure>
<p>添加一句：metadata-complete=”true” 就搞定了，我猜想由于项目是原来的项目，web.xml是原来项目的，和原来的什么版本可能有冲突，具体的也不清楚了</p>
<hr>
<p>按照src中的whatiftemplate的替换一下，可以修正错误</p>
<hr>
<p>在部署过多项目，如果报内存溢出异常，则需要修改一下tomcat的设置</p>
<blockquote>
<p>-Xms256m -Xmx512m -XX:MaxNewSize=256m -XX:MaxPermSize=256m</p>
</blockquote>
<hr>
<blockquote>
<p>Servlet mapping specifies an unknown servlet name AdapterHTTP</p>
<p>Servlet 2.3 jar not loaded</p>
</blockquote>
<p>在Java Resource 中找到对应jar包，然后exclude<br>不知道是否正确</p>

        
      
    </div>

    

    

  </article>

    
  </section>

  
  <nav class="pagination">
    
    
      <a class="next" href="/page/2/">
        <span class="next-text">下一页</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>


          </div>
          

        </div>
      </main>

      <footer id="footer" class="footer">

  <div class="social-links">
    
      
        
          <a href="mailto:pekeyli@qq.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a href="/PekeyLi" class="iconfont icon-weibo" title="weibo"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
    
    
      
      <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    
  </div>


<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">
    
    &copy; 
    
    2018

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Pekey Li</span>
  </span>
</div>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  


    <script type="text/javascript" src="/js/src/even.js?v=2.6.0"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=2.6.0"></script>

  </body>
</html>
