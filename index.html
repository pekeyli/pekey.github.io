<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    
<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">















  <link rel="alternate" href="/default" title="Pekey">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.6.0" />



<link rel="canonical" href="http://yoursite.com/"/>


<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.6.0" />






  



  <script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>





  <script src="//cdn1.lncld.net/static/js/3.1.1/av-min.js"></script>
  <script id="leancloud">
    AV.init({
      appId: "qjhxkQG6FwXDocEOjCFjSJXX-gzGzoHsz",
      appKey: "l9zmzMJqV7WdUXj4PbIGEAzb"
    });
  </script>





    <title> Pekey </title>
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/." class="logo">Pekey</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            首页
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            归档
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Pekey</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              首页
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              归档
            
          </a>
        </li>
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  <section id="posts" class="posts">
    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/20/Kylin-Cube构建过程学习/">Kylin Cube构建过程学习</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-20
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/20/Kylin-Cube构建过程学习/"
             data-title="Kylin Cube构建过程学习">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <p>参考文献：<a href="https://blog.bcmeng.com" target="_blank" rel="noopener">https://blog.bcmeng.com</a></p>
<hr>
<p>MapReduce 计算引擎 批量计算Cube，其输入是Hive表，输出是HBase的KeyValue，整个构建过程主要包含以下6步：</p>
<ol>
<li>建立Hive的大宽表； （MapReduce计算）</li>
<li>对需要字典编码的列计算列基数； （MapReduce计算）</li>
<li>构建字典； （JobServer计算 or MapReduce计算）</li>
<li>分层构建Cuboid； （MapReduce计算）</li>
<li>将Cuboid转为HBase的KeyValue结构（HFile）； （MapReduce计算）</li>
<li>元数据更新和垃圾回收。<h2 id="Cube-Build流程"><a href="#Cube-Build流程" class="headerlink" title="Cube Build流程"></a>Cube Build流程</h2></li>
</ol>
<p><strong>CubeController</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">private JobInstance buildInternal(String cubeName, TSRange tsRange, SegmentRange segRange, Map&lt;Integer, Long&gt; sourcePartitionOffsetStart, Map&lt;Integer, Long&gt; sourcePartitionOffsetEnd, String buildType, boolean force) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            String submitter = SecurityContextHolder.getContext().getAuthentication().getName();</span><br><span class="line">            CubeInstance cube = jobService.getCubeManager().getCube(cubeName);</span><br><span class="line"></span><br><span class="line">            if (cube == null) &#123;</span><br><span class="line">                throw new InternalErrorException(&quot;Cannot find cube &quot; + cubeName);</span><br><span class="line">            &#125;</span><br><span class="line">            return jobService.submitJob(cube, tsRange, segRange, sourcePartitionOffsetStart, sourcePartitionOffsetEnd,</span><br><span class="line">                    CubeBuildTypeEnum.valueOf(buildType), force, submitter);</span><br><span class="line">        &#125; catch (Throwable e) &#123;</span><br><span class="line">            logger.error(e.getLocalizedMessage(), e);</span><br><span class="line">            throw new InternalErrorException(e.getLocalizedMessage(), e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>JobService</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ISource source = SourceFactory.getSource(cube);</span><br><span class="line">SourcePartition src = new SourcePartition(tsRange, segRange, sourcePartitionOffsetStart,</span><br><span class="line">        sourcePartitionOffsetEnd);</span><br><span class="line">src = source.enrichSourcePartitionBeforeBuild(cube, src);</span><br><span class="line">newSeg = getCubeManager().appendSegment(cube, src);</span><br><span class="line">job = EngineFactory.createBatchCubingJob(newSeg, submitter);</span><br></pre></td></tr></table></figure></p>
<p><strong>EngineFactory</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/** Build a new cube segment, typically its time range appends to the end of current cube. */</span><br><span class="line">    public static DefaultChainedExecutable createBatchCubingJob(CubeSegment newSegment, String submitter) &#123;</span><br><span class="line">        return batchEngine(newSegment).createBatchCubingJob(newSegment, submitter);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p><strong>MRBatchCubingEngine2</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public DefaultChainedExecutable createBatchCubingJob(CubeSegment newSegment, String submitter) &#123;</span><br><span class="line">    return new BatchCubingJobBuilder2(newSegment, submitter).build();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>BatchCubingJobBuilder2</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">public CubingJob build() &#123;</span><br><span class="line">        logger.info(&quot;MR_V2 new job to BUILD segment &quot; + seg);</span><br><span class="line"></span><br><span class="line">        final CubingJob result = CubingJob.createBuildJob(seg, submitter, config);</span><br><span class="line">        final String jobId = result.getId();</span><br><span class="line">        final String cuboidRootPath = getCuboidRootPath(jobId);</span><br><span class="line"></span><br><span class="line">        // Phase 1: Create Flat Table &amp; Materialize Hive View in Lookup Tables</span><br><span class="line">        inputSide.addStepPhase1_CreateFlatTable(result);</span><br><span class="line"></span><br><span class="line">        // Phase 2: Build Dictionary</span><br><span class="line">        result.addTask(createFactDistinctColumnsStep(jobId));</span><br><span class="line"></span><br><span class="line">        if (isEnableUHCDictStep()) &#123;</span><br><span class="line">            result.addTask(createBuildUHCDictStep(jobId));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        result.addTask(createBuildDictionaryStep(jobId));</span><br><span class="line">        result.addTask(createSaveStatisticsStep(jobId));</span><br><span class="line">        outputSide.addStepPhase2_BuildDictionary(result);</span><br><span class="line"></span><br><span class="line">        // Phase 3: Build Cube</span><br><span class="line">        addLayerCubingSteps(result, jobId, cuboidRootPath); // layer cubing, only selected algorithm will execute</span><br><span class="line">        addInMemCubingSteps(result, jobId, cuboidRootPath); // inmem cubing, only selected algorithm will execute</span><br><span class="line">        outputSide.addStepPhase3_BuildCube(result);</span><br><span class="line"></span><br><span class="line">        // Phase 4: Update Metadata &amp; Cleanup</span><br><span class="line">        result.addTask(createUpdateCubeInfoAfterBuildStep(jobId));</span><br><span class="line">        inputSide.addStepPhase4_Cleanup(result);</span><br><span class="line">        outputSide.addStepPhase4_Cleanup(result);</span><br><span class="line"></span><br><span class="line">        return result;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="Phase-1-Create-Flat-Table-amp-Materialize-Hive-View-in-Lookup-Tables"><a href="#Phase-1-Create-Flat-Table-amp-Materialize-Hive-View-in-Lookup-Tables" class="headerlink" title="Phase 1: Create Flat Table &amp; Materialize Hive View in Lookup Tables"></a>Phase 1: Create Flat Table &amp; Materialize Hive View in Lookup Tables</h3><p><strong>BatchCubingJobBuilder2</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// Phase 1: Create Flat Table &amp; Materialize Hive View in Lookup Tables</span><br><span class="line">inputSide.addStepPhase1_CreateFlatTable(result);</span><br></pre></td></tr></table></figure></p>
<p><strong>HiveMRInput</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public void addStepPhase1_CreateFlatTable(DefaultChainedExecutable jobFlow) &#123;</span><br><span class="line">            final String cubeName = CubingExecutableUtil.getCubeName(jobFlow.getParams());</span><br><span class="line">            final KylinConfig cubeConfig = CubeManager.getInstance(KylinConfig.getInstanceFromEnv()).getCube(cubeName)</span><br><span class="line">                    .getConfig();</span><br><span class="line">            final String hiveInitStatements = JoinedFlatTable.generateHiveInitStatements(flatTableDatabase);</span><br><span class="line"></span><br><span class="line">            // create flat table first</span><br><span class="line">            addStepPhase1_DoCreateFlatTable(jobFlow);</span><br><span class="line"></span><br><span class="line">            // then count and redistribute</span><br><span class="line">            if (cubeConfig.isHiveRedistributeEnabled()) &#123;</span><br><span class="line">                jobFlow.addTask(createRedistributeFlatHiveTableStep(hiveInitStatements, cubeName));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            // special for hive</span><br><span class="line">            addStepPhase1_DoMaterializeLookupTable(jobFlow);</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">private AbstractExecutable createFlatHiveTableStep(String hiveInitStatements, String jobWorkingDir,</span><br><span class="line">                String cubeName) &#123;</span><br><span class="line">            //from hive to hive</span><br><span class="line">            final String dropTableHql = JoinedFlatTable.generateDropTableStatement(flatDesc);</span><br><span class="line">            final String createTableHql = JoinedFlatTable.generateCreateTableStatement(flatDesc, jobWorkingDir);</span><br><span class="line">            String insertDataHqls = JoinedFlatTable.generateInsertDataStatement(flatDesc);</span><br><span class="line"></span><br><span class="line">            CreateFlatHiveTableStep step = new CreateFlatHiveTableStep();</span><br><span class="line">            step.setInitStatement(hiveInitStatements);</span><br><span class="line">            step.setCreateTableStatement(dropTableHql + createTableHql + insertDataHqls);</span><br><span class="line">            CubingExecutableUtil.setCubeName(cubeName, step.getParams());</span><br><span class="line">            step.setName(ExecutableConstants.STEP_NAME_CREATE_FLAT_HIVE_TABLE);</span><br><span class="line">            return step;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p><strong>CreateFlatHiveTableStep</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">protected void createFlatHiveTable(KylinConfig config) throws IOException &#123;</span><br><span class="line">        final HiveCmdBuilder hiveCmdBuilder = new HiveCmdBuilder();</span><br><span class="line">        hiveCmdBuilder.overwriteHiveProps(config.getHiveConfigOverride());</span><br><span class="line">        hiveCmdBuilder.addStatement(getInitStatement());</span><br><span class="line">        hiveCmdBuilder.addStatementWithRedistributeBy(getCreateTableStatement());</span><br><span class="line">        final String cmd = hiveCmdBuilder.toString();</span><br><span class="line"></span><br><span class="line">        stepLogger.log(&quot;Create and distribute table, cmd: &quot;);</span><br><span class="line">        stepLogger.log(cmd);</span><br><span class="line"></span><br><span class="line">        Pair&lt;Integer, String&gt; response = config.getCliCommandExecutor().execute(cmd, stepLogger);</span><br><span class="line">        Map&lt;String, String&gt; info = stepLogger.getInfo();</span><br><span class="line"></span><br><span class="line">        //get the flat Hive table size</span><br><span class="line">        Matcher matcher = HDFS_LOCATION.matcher(cmd);</span><br><span class="line">        if (matcher.find()) &#123;</span><br><span class="line">            String hiveFlatTableHdfsUrl = matcher.group(1);</span><br><span class="line">            long size = getFileSize(hiveFlatTableHdfsUrl);</span><br><span class="line">            info.put(ExecutableConstants.HDFS_BYTES_WRITTEN, &quot;&quot; + size);</span><br><span class="line">            logger.info(&quot;HDFS_Bytes_Writen: &quot; + size);</span><br><span class="line">        &#125;</span><br><span class="line">        getManager().addJobInfo(getId(), info);</span><br><span class="line">        if (response.getFirst() != 0) &#123;</span><br><span class="line">            throw new RuntimeException(&quot;Failed to create flat hive table, error code &quot; + response.getFirst());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="Phase-2-Build-Dictionary"><a href="#Phase-2-Build-Dictionary" class="headerlink" title="Phase 2: Build Dictionary"></a>Phase 2: Build Dictionary</h3><h3 id="Phase-3-Build-Cube"><a href="#Phase-3-Build-Cube" class="headerlink" title="Phase 3: Build Cube"></a>Phase 3: Build Cube</h3><p><strong>BatchCubingJobBuilder2</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// Phase 3: Build Cube</span><br><span class="line">addLayerCubingSteps(result, jobId, cuboidRootPath); // layer cubing, only selected algorithm will execute</span><br><span class="line">addInMemCubingSteps(result, jobId, cuboidRootPath); // inmem cubing, only selected algorithm will execute</span><br><span class="line">outputSide.addStepPhase3_BuildCube(result);</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">protected void addLayerCubingSteps(final CubingJob result, final String jobId, final String cuboidRootPath) &#123;</span><br><span class="line">    // Don&apos;t know statistics so that tree cuboid scheduler is not determined. Determine the maxLevel at runtime</span><br><span class="line">    final int maxLevel = CuboidUtil.getLongestDepth(seg.getCuboidScheduler().getAllCuboidIds());</span><br><span class="line">    // base cuboid step</span><br><span class="line">    result.addTask(createBaseCuboidStep(getCuboidOutputPathsByLevel(cuboidRootPath, 0), jobId));</span><br><span class="line">    // n dim cuboid steps</span><br><span class="line">    for (int i = 1; i &lt;= maxLevel; i++) &#123;</span><br><span class="line">        result.addTask(createNDimensionCuboidStep(getCuboidOutputPathsByLevel(cuboidRootPath, i - 1), getCuboidOutputPathsByLevel(cuboidRootPath, i), i, jobId));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">private MapReduceExecutable createBaseCuboidStep(String cuboidOutputPath, String jobId) &#123;</span><br><span class="line">    // base cuboid job</span><br><span class="line">    MapReduceExecutable baseCuboidStep = new MapReduceExecutable();</span><br><span class="line"></span><br><span class="line">    StringBuilder cmd = new StringBuilder();</span><br><span class="line">    appendMapReduceParameters(cmd);</span><br><span class="line"></span><br><span class="line">    baseCuboidStep.setName(ExecutableConstants.STEP_NAME_BUILD_BASE_CUBOID);</span><br><span class="line"></span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_CUBE_NAME, seg.getRealization().getName());</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_SEGMENT_ID, seg.getUuid());</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_INPUT, &quot;FLAT_TABLE&quot;); // marks flat table input</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_OUTPUT, cuboidOutputPath);</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_JOB_NAME, &quot;Kylin_Base_Cuboid_Builder_&quot; + seg.getRealization().getName());</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_LEVEL, &quot;0&quot;);</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_CUBING_JOB_ID, jobId);</span><br><span class="line"></span><br><span class="line">    baseCuboidStep.setMapReduceParams(cmd.toString());</span><br><span class="line">    baseCuboidStep.setMapReduceJobClass(getBaseCuboidJob());</span><br><span class="line">    //        baseCuboidStep.setCounterSaveAs(CubingJob.SOURCE_RECORD_COUNT + &quot;,&quot; + CubingJob.SOURCE_SIZE_BYTES);</span><br><span class="line">    return baseCuboidStep;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">private MapReduceExecutable createNDimensionCuboidStep(String parentPath, String outputPath, int level, String jobId) &#123;</span><br><span class="line">    // ND cuboid job</span><br><span class="line">    MapReduceExecutable ndCuboidStep = new MapReduceExecutable();</span><br><span class="line"></span><br><span class="line">    ndCuboidStep.setName(ExecutableConstants.STEP_NAME_BUILD_N_D_CUBOID + &quot; : level &quot; + level);</span><br><span class="line">    StringBuilder cmd = new StringBuilder();</span><br><span class="line"></span><br><span class="line">    appendMapReduceParameters(cmd);</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_CUBE_NAME, seg.getRealization().getName());</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_SEGMENT_ID, seg.getUuid());</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_INPUT, parentPath);</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_OUTPUT, outputPath);</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_JOB_NAME, &quot;Kylin_ND-Cuboid_Builder_&quot; + seg.getRealization().getName() + &quot;_Step&quot;);</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_LEVEL, &quot;&quot; + level);</span><br><span class="line">    appendExecCmdParameters(cmd, BatchConstants.ARG_CUBING_JOB_ID, jobId);</span><br><span class="line"></span><br><span class="line">    ndCuboidStep.setMapReduceParams(cmd.toString());</span><br><span class="line">    ndCuboidStep.setMapReduceJobClass(getNDCuboidJob());</span><br><span class="line">    return ndCuboidStep;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="MR-Cube-Build"><a href="#MR-Cube-Build" class="headerlink" title="MR Cube Build"></a>MR Cube Build</h4><hr>
<p><strong>从Hive表生成Base Cuboid</strong></p>
<blockquote>
<p>在实际的cube构建过程中，会首先根据cube的Hive事实表和维表生成一张大宽表，然后计算大宽表列的基数，建立维度字典，估算cuboid的大小，建立cube对应的HBase表，再计算base cuboid。<br>计算base cuboid就是一个MapReduce作业，其输入是上面提到的Hive大宽表，输出是的key是各种维度组合，value是Hive大宽表中指标的值。</p>
</blockquote>
<p><strong>BaseCuboidJob</strong><br><strong>mapper: HiveToBaseCuboidMapper</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public void doMap(KEYIN key, Object value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">    Collection&lt;String[]&gt; rowCollection = flatTableInputFormat.parseMapperInput(value);</span><br><span class="line">    for (String[] row: rowCollection) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            outputKV(row, context);</span><br><span class="line"></span><br><span class="line">        &#125; catch (Exception ex) &#123;</span><br><span class="line">            handleErrorRecord(row, ex);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>HiveTableReader</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public static String[] getRowAsStringArray(HCatRecord record) &#123;</span><br><span class="line">    String[] arr = new String[record.size()];</span><br><span class="line">    for (int i = 0; i &lt; arr.length; i++) &#123;</span><br><span class="line">        Object o = record.get(i);</span><br><span class="line">        arr[i] = (o == null) ? null : o.toString();</span><br><span class="line">    &#125;</span><br><span class="line">    return arr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>BaseCuboidMapperBase</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">protected void  outputKV(String[] flatRow, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">    byte[] rowKey = baseCuboidBuilder.buildKey(flatRow);</span><br><span class="line">    outputKey.set(rowKey, 0, rowKey.length);</span><br><span class="line"></span><br><span class="line">    ByteBuffer valueBuf = baseCuboidBuilder.buildValue(flatRow);</span><br><span class="line">    outputValue.set(valueBuf.array(), 0, valueBuf.position());</span><br><span class="line">    context.write(outputKey, outputValue);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<hr>
<p>从Base Cuboid 逐层计算 Cuboid。</p>
<blockquote>
<p>从base cuboid 逐层计算每层的cuboid，也是MapReduce作业，map阶段每层维度数依次减少，reduce阶段对指标进行聚合。</p>
</blockquote>
<p><strong>reducer: CuboidReducer</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public void doReduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">    aggs.reset();</span><br><span class="line"></span><br><span class="line">    for (Text value : values) &#123;</span><br><span class="line">        if (vcounter++ % BatchConstants.NORMAL_RECORD_LOG_THRESHOLD == 0) &#123;</span><br><span class="line">            logger.info(&quot;Handling value with ordinal (This is not KV number!): &quot; + vcounter);</span><br><span class="line">        &#125;</span><br><span class="line">        codec.decode(ByteBuffer.wrap(value.getBytes(), 0, value.getLength()), input);</span><br><span class="line">        aggs.aggregate(input, needAggrMeasures);</span><br><span class="line">    &#125;</span><br><span class="line">    aggs.collectStates(result);</span><br><span class="line"></span><br><span class="line">    ByteBuffer valueBuf = codec.encode(result);</span><br><span class="line"></span><br><span class="line">    outputValue.set(valueBuf.array(), 0, valueBuf.position());</span><br><span class="line">    context.write(key, outputValue);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>Cuboid 转化为HBase的HFile</strong></p>
<hr>
<h4 id="Spark-Cube-Build"><a href="#Spark-Cube-Build" class="headerlink" title="Spark Cube Build"></a>Spark Cube Build</h4><blockquote>
<p>The “by-layer” Cubing divides a big task into a couple steps, and each step bases on the previous step’s output, so it can reuse the previous calculation and also avoid calculating from very beginning when there is a failure in between. These makes it as a reliable algorithm. When moving to Spark, we decide to keep this algorithm, that’s why we call this feature as “By layer Spark Cubing”.</p>
<p>分层构建Cube可以将一个大任务分成若干步，而且每一步都可以基于上一步的输出进行计算，</p>
<p>Figure 3 is the DAG of Cubing in Spark, it illustrates the process in detail: In “Stage 5”, Kylin uses a HiveContext to read the intermediate Hive table, and then do a “map” operation, which is an one to one map, to encode the origin values into K-V bytes. On complete Kylin gets an intermediate encoded RDD. In “Stage 6”, the intermediate RDD is aggregated with a “reduceByKey” operation to get RDD-1, which is the base cuboid. Nextly, do an “flatMap” (one to many map) on RDD-1, because the base cuboid has N children cuboids. And so on, all levels’ RDDs get calculated. These RDDs will be persisted to distributed file system on complete, but be cached in memory for next level’s calculation. When child be generated, it will be removed from cache.<br><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqjhotn9owj30y00lsjuj.jpg" alt=""></p>
</blockquote>
<p>As we know, RDD (Resilient Distributed Dataset) is a basic concept in Spark. A collection of N-Dimension cuboids can be well described as an RDD, a N-Dimension Cube will have N+1 RDD. These RDDs have the parent/child relationship as the parent can be used to generate the children. With the parent RDD cached in memory, the child RDD’s generation can be much efficient than reading from disk. Figure 2 describes this process.</p>
<h4 id="Phase-4-Update-Metadata-amp-Cleanup"><a href="#Phase-4-Update-Metadata-amp-Cleanup" class="headerlink" title="Phase 4: Update Metadata &amp; Cleanup"></a>Phase 4: Update Metadata &amp; Cleanup</h4><h3 id="其他代码"><a href="#其他代码" class="headerlink" title="其他代码"></a>其他代码</h3>
        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/20/Speeding-ETL-Processing-in-Data-Warehouses-Using-High-Performance-Joins-for-Changed-Data-Capture/">Speeding ETL Processing in Data Warehouses Using High-Performance Joins for Changed Data Capture 论文学习</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-20
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/20/Speeding-ETL-Processing-in-Data-Warehouses-Using-High-Performance-Joins-for-Changed-Data-Capture/"
             data-title="Speeding ETL Processing in Data Warehouses Using High-Performance Joins for Changed Data Capture 论文学习">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <blockquote>
<p>1.Join and 2.Aggregation – which will play an integral role during preprocessing as well in manipulating and consolidating data in a data warehouse.</p>
</blockquote>
<p>翻译：</p>
<blockquote>
<p>1.加入和2.聚合 - 在预处理过程中以及在数据仓库中操纵和合并数据时，它们将扮演不可或缺的角色。</p>
</blockquote>
<hr>
<blockquote>
<p>ETL systems move data from OLTP systems to a data warehouse, but they can also be used to move data from one data warehouse to another. A heterogeneous architecture for an ETL system is one that extracts data from multiple sources. The complexity of this architecture arises from the fact that data from more than one source must be merged, rather than from the fact that data may be formatted differently in the different sources.</p>
</blockquote>
<p>翻译：</p>
<blockquote>
<p>ETL系统将数据从OLTP系统移动到数据仓库，但它们也可用于将数据从一个数据仓库移动到另一个数据仓库。 ETL系统的异构体系结构是从多个来源提取数据的体系结构。这种体系结构的复杂性源于以下事实：来自多个源的数据必须合并，而不是来自不同来源的数据格式不同的事实。</p>
</blockquote>
<hr>
<blockquote>
<p>Rather than replacing the information in the data warehouse with the data in the entire online transactional database, a join will match the primary key of the previously loaded record with its corresponding new record and then compare the data portions of the two records to determine if they’ve changed. In this way, only added, deleted, and altered records are updated, which significantly reduces elapsed time of database loads. By using a high-performance join for CDC, data warehouse updates can be performed with far greater efficiency.</p>
</blockquote>
<p>翻译：</p>
<blockquote>
<p>与其将数据仓库中的信息替换为整个在线交易数据库中的数据，联合会将先前加载的记录的主键与其相应的新记录相匹配，然后比较两个记录的数据部分以确定它们 已经改变了。 通过这种方式，只有添加，删除和更改的记录才会更新，这会显着减少数据库加载所花费的时间。 通过为CDC使用高性能连接，数据仓库更新可以以更高的效率执行。</p>
</blockquote>

        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/18/Implementation-of-Change-Data-Capture-in-ETL-Process-for-Data-Warehouse-Using-HDFS-and-Apache-Spark-论文学习/">Implementation of Change Data Capture in ETL Process for Data Warehouse Using HDFS and Apache Spark 论文学习</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-18
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/18/Implementation-of-Change-Data-Capture-in-ETL-Process-for-Data-Warehouse-Using-HDFS-and-Apache-Spark-论文学习/"
             data-title="Implementation of Change Data Capture in ETL Process for Data Warehouse Using HDFS and Apache Spark 论文学习">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <blockquote>
<ul>
<li>没什么干货，参考文献中的几篇有价值</li>
<li>可以参考这篇文章关于etl基本术语的介绍</li>
<li>snapshot difference技术 快照差分</li>
</ul>
</blockquote>
<h2 id="用到的技术"><a href="#用到的技术" class="headerlink" title="用到的技术"></a>用到的技术</h2><blockquote>
<p>The whole ETL process are performed using Pentaho Data Integration (PDI). Meanwhile, the proposed ETL process extract new and changed data using map-reduce framework and Apache Spoon. The transformation and loading process are performed using PDI. This section elaborates our big data cluster environment and the implementation of CDC.</p>
</blockquote>
<p>翻译：</p>
<blockquote>
<p>整个ETL过程使用Pentaho数据集成（PDI）执行。 同时，提议的ETL过程使用map-reduce框架和Apache Spoon提取新的和更改的数据。 转换和加载过程使用PDI执行。 本部分阐述了我们的大数据集群环境和CDC的实施。</p>
</blockquote>
<h2 id="mr的应用"><a href="#mr的应用" class="headerlink" title="mr的应用"></a>mr的应用</h2><blockquote>
<p>The CDC method can be implemented using MapReduce by adopting the divide and conquer principle similar to that conducted in the study by [9]. The data are divided into several parts, each to be processed separately. Then, each data processed will enter the reduce phase which will detect changes in the data.</p>
</blockquote>
<p>翻译：</p>
<blockquote>
<p>CDC方法可以通过采用类似于[9]研究中进行的分而治之原则，使用MapReduce来实现。 数据分成几个部分，每个部分分别处理。 然后，每个处理的数据将进入减少阶段，这将检测数据的变化。</p>
</blockquote>
<h2 id="sprak-sql-的应用"><a href="#sprak-sql-的应用" class="headerlink" title="sprak sql 的应用"></a>sprak sql 的应用</h2><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fqh59vdxk5j31ec0tkgra.jpg" alt=""></p>
<blockquote>
<p>In this study, the method used was by comparing data and finding differences between two data. The CDC method using snapshot difference was divided into several stages as illustrated in Figure 8. The first process was to take the data from the source. The data were taken by full load, which took the entire data from the beginning to the final condition. The results of the extraction was entered into the HDFS. The data was processed through the program created to run the CDC process using the snapshot difference technique. Snapshot difference was implemented <strong>using outer-join function from SparkSQL. The program was run by looking at the null value of the outer-join results of the two records, which will be the parameter for the newest data.</strong> If in the process damage to the data source was detected, the program could automatically change the data reference to compare and store the old data as a snapshot.</p>
</blockquote>
<p>翻译：</p>
<blockquote>
<p>在这项研究中，所使用的方法是通过比较数据和发现两个数据之间的差异。 使用快照差异的CDC方法分为几个阶段，如图8所示。第一个过程是从源头获取数据。 数据是以满负荷的方式获取的，这些数据从开始到最终的状态都采用了整个数据。 提取结果被输入到HDFS中。 数据通过创建的程序进行处理，以使用快照差异技术运行CDC进程。 快照差异是使用SparkSQL的外连接函数实现的。 该程序通过查看两条记录的外连接结果的空值来运行，这将成为最新数据的参数。 如果在检测到数据源损坏的过程中，程序可以自动更改数据引用以比较旧数据并将其存储为快照。</p>
</blockquote>

        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/18/Hive环境搭建/">Hive环境搭建</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-18
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/18/Hive环境搭建/"
             data-title="Hive环境搭建">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <h2 id="Mysql安装"><a href="#Mysql安装" class="headerlink" title="Mysql安装"></a>Mysql安装</h2><p>关闭selinux 服务</p>
<blockquote>
<p>vim /etc/selinux/config<br>···<br>SELINUX=disabled<br>···</p>
</blockquote>
<p>卸载MariaDB</p>
<blockquote>
<p>查看当前安装的mariadb包： rpm -qa | grep mariadb<br>强制卸载： rpm -e –nodeps mariadb-libs-5.5.44-2.el7.centos.x86_64</p>
</blockquote>
<p>查看是否已经安装了MySQL</p>
<blockquote>
<p>rpm -qa | grep -i mysql<br>find / -name mysql</p>
</blockquote>
<p><del>删除分散mysql文件</del></p>
<blockquote>
<p><del>find / -name mysql / # whereis mysql</del></p>
</blockquote>
<p><del>删除配置文档</del></p>
<blockquote>
<p><del>rm -rf /etc/my.cnf</del></p>
</blockquote>
<p><del>再次查找机器是否安装mysql</del></p>
<blockquote>
<p><del>rpm -qa|grep -i mysql</del></p>
</blockquote>
<p>下载mysql5.6的安装包，并上传到服务器上</p>
<blockquote>
<p>[root@nodeh1 mysql]# ll<br>total 236180<br>-rw-r–r– 1 root root 20278972 Sep 22 15:41 MySQL-client-5.6.31-1.el7.x86_64.rpm<br>-rw-r–r– 1 root root  3529244 Sep 22 15:40 MySQL-devel-5.6.31-1.el7.x86_64.rpm<br>-rw-r–r– 1 root root 61732192 Sep 22 15:42 MySQL-server-5.6.31-1.el7.x86_64.rpm<br>-rw-r–r– 1 root root  2101912 Sep 22 15:42 MySQL-shared-5.6.31-1.el7.x86_64.rpm<br>-rw-r–r– 1 root root  2299648 Sep 22 15:40 MySQL-shared-compat-5.6.31-1.el7.x86_64.rpm<br>-rw-r–r– 1 root root 59644132 Sep 22 15:40 MySQL-test-5.6.31-1.el7.x86_64.rpm</p>
</blockquote>
<p>安装mysql 的安装包 注意顺序</p>
<blockquote>
<p>rpm -ivh MySQL-common-5.6.31-1.el7.x86_64.rpm<br>rpm -ivh MySQL-libs-5.6.31-1.el7.x86_64.rpm<br>rpm -ivh MySQL-client-5.6.31-1.el7.x86_64.rpm<br>rpm -ivh MySQL-server-5.6.31-1.el7.x86_64.rpm<br>rpm -ivh MySQL-devel-5.6.31-1.el7.x86_64.rpm</p>
</blockquote>
<p>使用rpm安装方式安装mysql，安装的路径如下：</p>
<blockquote>
<p>a 数据库目录<br>/var/lib/mysql/<br>b 配置文件<br>/usr/share/mysql(mysql.server命令及配置文件)<br>c 相关命令<br>/usr/bin(mysqladmin mysqldump等命令)<br>d 启动脚本<br>/etc/rc.d/init.d/(启动脚本文件mysql的目录)<br>e /etc/my.conf</p>
</blockquote>
<p>修改字符集和数据存储路径 配置/etc/my.cnf文件,设置如下键值来启用一起有用的选项和 UTF-8 字符集.</p>
<blockquote>
<p>cat /etc/my.cnf<br>[mysqld]<br>···<br>innodb_file_per_table<br>max_connections = 4096<br>collation-server = utf8_general_ci<br>character-set-server = utf8</p>
</blockquote>
<p>如果无法启动mysql服务</p>
<blockquote>
<p>chown mysql:mysql -R /var/lib/mysql</p>
</blockquote>
<p>如果遇到Access denied for user ‘root‘@’localhost’ (using password: YES)</p>
<blockquote>
<ol>
<li>打开MySQL目录下的my.ini文件，在文件的最后添加一行“skip-grant-tables”，保存并关闭文件，重启服务。</li>
<li>通过cmd行进入MySQL的bin目录，输入“mysql -u root -p”(不输入密码)，回车即可进入数据库;</li>
<li>执行<code>mysql&gt; use mysql;</code>，使用mysql数据库;</li>
<li><code>mysql&gt; update mysql.user set authentication_string=password(&quot;root&quot;) where user=&quot;root&quot; and Host = &quot;localhost&quot;;</code></li>
<li>打开MySQL目录下的my.ini文件，删除最后一行的“skip-grant-tables”，保存并关闭文件</li>
<li><code>mysql&gt; flush privileges;</code></li>
<li><code>mysql&gt; quit;</code></li>
</ol>
</blockquote>
<p>初始化MySQL及设置密码</p>
<blockquote>
<p>mysqld –initialize<br>会生成一个 root 账户密码，密码在log文件里<br>cat /var/log/mysqld.log<br><code>2017-04-13T10:00:37.229524Z 1 [Note] A temporary password is generated for root@localhost: %kWTz,Ml?3Zs</code></p>
</blockquote>
<blockquote>
<p>systemctl start mysqld.service</p>
</blockquote>
<blockquote>
<p>ALTER USER ‘root‘@’localhost’ IDENTIFIED BY ‘new_password’;</p>
</blockquote>
<p>设置mysql开机启动</p>
<blockquote>
<p>systemctl restart mysqld.service<br>systemctl enable mysqld.service</p>
</blockquote>
<p>设置mysql允许远程登陆</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; use mysql;</span><br><span class="line">mysql&gt; select host,user,password from user;</span><br><span class="line">| host                  | user | password                                  |</span><br><span class="line">| localhost             | root | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 |</span><br><span class="line">| localhost.localdomain | root | *1237E2CE819C427B0D8174456DD83C47480D37E8 |</span><br><span class="line">| 127.0.0.1             | root | *1237E2CE819C427B0D8174456DD83C47480D37E8 |</span><br><span class="line">| ::1                   | root | *1237E2CE819C427B0D8174456DD83C47480D37E8 |</span><br><span class="line">mysql&gt; update user set host=&apos;%&apos; where user=&apos;root&apos; and host=&apos;localhost&apos;;</span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure>
<h3 id="MySQL-is-running-but-PID-file-could-not-be-found"><a href="#MySQL-is-running-but-PID-file-could-not-be-found" class="headerlink" title="MySQL is running but PID file could not be found"></a>MySQL is running but PID file could not be found</h3><p>在Linux 中，当你启动或者重启 MySQL 时，报关于 PID file 的错误</p>
<blockquote>
<p>第一步：找到   mysql 中 data 目录下的 mysql-bin.index 文件，然后删除<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">find / -name mysql-bin.index</span><br><span class="line">rm -rf  /phpstudy/data/mysql-bin.index</span><br></pre></td></tr></table></figure></p>
</blockquote>
<blockquote>
<p>第二步：找到 并 kill 所有关于 mysql 或者 mysqld 的进程</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps -aux | grep mysql</span><br><span class="line">kill 进程号</span><br></pre></td></tr></table></figure>
<p>可以在查看下进程中是否有 mysql 进程，确保 kill 干净，再看看 还有没有 mysql-bin.index文件（进程没杀死之前可能会生成）&lt;有必要&gt;</p>
<blockquote>
<p>命令：  service mysqld status</p>
</blockquote>
<hr>
<h3 id="mysqld：未被识别的服务"><a href="#mysqld：未被识别的服务" class="headerlink" title="mysqld：未被识别的服务"></a>mysqld：未被识别的服务</h3><p>遇到这样的错误，是由于 /etc/init.d/  不存在 mysqld 这个命令（有的人安装完环境后存在，是因为你的安装包中有这样的命令将 mysql.server 文件 copy 到 /etc/init.d/ 下面了）</p>
<blockquote>
<p>1.首先你需要找到 mysql.server 文件，这个 和 mysqld 文件是一模一样的，只不过文件名不相同，执行命令：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find  /  -name mysql.server</span><br></pre></td></tr></table></figure>
<blockquote>
<p>2.copy mysql.server 文件到 /etc/init.d/ 目录下，重命名文件为 mysqld，执行命令：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp  /phpstudy/mysql/support-files/mysql.server  /etc/init.d/mysqld</span><br></pre></td></tr></table></figure>
<blockquote>
<p>然后 再  <code>service mysqld status</code> 这个问题就解决了。</p>
</blockquote>
<hr>
<h3 id="mysqld启动错误"><a href="#mysqld启动错误" class="headerlink" title="mysqld启动错误"></a>mysqld启动错误</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Redirecting to /bin/systemctl start  mysqld.service</span><br><span class="line">Job for mysqld.service failed. See &apos;systemctl status mysqld.service&apos; and &apos;journalctl -xn&apos; for details.</span><br><span class="line">service mysqld status</span><br><span class="line">Redirecting to /bin/systemctl status  mysqld.service</span><br><span class="line">mysqld.service - MySQL Server</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled)</span><br><span class="line">   Active: failed (Result: start-limit) since Tue 2015-12-08 13:57:22 CST; 17s ago</span><br><span class="line">  Process: 31004 ExecStart=/usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid $MYSQLD_OPTS (code=exited, status=1/FAILURE)</span><br><span class="line">  Process: 30988 ExecStartPre=/usr/bin/mysqld_pre_systemd (code=exited, status=0/SUCCESS)</span><br><span class="line"></span><br><span class="line">Dec 08 13:57:22 iZ25lox0jlhZ systemd[1]: Failed to start MySQL Server.</span><br><span class="line">Dec 08 13:57:22 iZ25lox0jlhZ systemd[1]: Unit mysqld.service entered failed state.</span><br><span class="line">Dec 08 13:57:22 iZ25lox0jlhZ systemd[1]: mysqld.service holdoff time over, scheduling restart.</span><br><span class="line">Dec 08 13:57:22 iZ25lox0jlhZ systemd[1]: Stopping MySQL Server...</span><br><span class="line">Dec 08 13:57:22 iZ25lox0jlhZ systemd[1]: Starting MySQL Server...</span><br><span class="line">Dec 08 13:57:22 iZ25lox0jlhZ systemd[1]: mysqld.service start request repeated too quickly, refusing to start.</span><br><span class="line">Dec 08 13:57:22 iZ25lox0jlhZ systemd[1]: Failed to start MySQL Server.</span><br><span class="line">Dec 08 13:57:22 iZ25lox0jlhZ systemd[1]: Unit mysqld.service entered failed state.</span><br></pre></td></tr></table></figure>
<p>安装完应该先检查一下/var/lib/mysql目录下的文件权限，执行</p>
<blockquote>
<p>chown mysql:mysql -R /var/lib/mysql</p>
</blockquote>
<p>然后重新启动mysql服务</p>
<blockquote>
<p>service mysqld start</p>
</blockquote>
<h2 id="Hive配置过程"><a href="#Hive配置过程" class="headerlink" title="Hive配置过程"></a>Hive配置过程</h2><ul>
<li><p>hive-site.xml</p>
<blockquote>
<p><code>cp $HIVE_HOME/conf/hive-default.xml.template $HIVE_HOME/conf/hive-site.xml</code></p>
</blockquote>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;jdbc:mysql://127.0.0.1:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;new_password&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="iotmp"><a href="#iotmp" class="headerlink" title="iotmp"></a>iotmp</h3><blockquote>
<p>建立临时目录<br>mkdir /home/grid/hive/iotmp<br>关联找到hive-site.xml里面，将iotmp的对应配置修改成真实地址。<br>${system:java.io.tmpdir} 改为真实物理路径。</p>
</blockquote>
<h2 id="Hive启动过程"><a href="#Hive启动过程" class="headerlink" title="Hive启动过程"></a>Hive启动过程</h2><blockquote>
<p>nohup hive –service metastore -v &amp;</p>
</blockquote>

        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/17/Spark-Java-API/">Spark Java API</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-17
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/17/Spark-Java-API/"
             data-title="Spark Java API">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <p>参考文献：<a href="http://lxw1234.com" target="_blank" rel="noopener">http://lxw1234.com</a></p>
<h2 id="RDD如何创建"><a href="#RDD如何创建" class="headerlink" title="RDD如何创建"></a>RDD如何创建</h2><p>　　　　<br>首先创建JavaSparkContext对象实例sc</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaSparkContext  sc = new JavaSparkContext(&quot;local&quot;,&quot;SparkTest&quot;);</span><br></pre></td></tr></table></figure>
<p>接受2个参数：<br>第一个参数表示运行方式（local、yarn-client、yarn-standalone等）<br>第二个参数表示应用名字</p>
<blockquote>
<p>直接从集合转化 <code>sc.parallelize(List(1,2,3,4,5,6,7,8,9,10))</code><br>从HDFS文件转化  <code>sc.textFile(&quot;hdfs://&quot;)</code><br>从本地文件转化  <code>sc.textFile(&quot;file:/&quot;)</code></p>
</blockquote>
<p>下面例子中list2就是根据data2List生成的一个RDD</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; list1 = new ArrayList&lt;String&gt;();</span><br><span class="line">list1.add(&quot;11,12,13,14,15&quot;);</span><br><span class="line">list1.add(&quot;aa,bb,cc,dd,ee&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; list2 = sc.parallelize(list1);</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="常用算子"><a href="#常用算子" class="headerlink" title="常用算子"></a>常用算子</h2><hr>
<h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><p>举例，在F:\sparktest\sample.txt 文件的内容如下</p>
<blockquote>
<p>aa bb cc aa aa aa dd dd ee ee ee ee<br>ff aa bb zks<br>ee kks<br>ee  zz zks<br>我要将包含zks的行的内容给找出来<br>scala版本</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val lines = sc.textFile(&quot;F:\\sparktest\\sample.txt&quot;).filter(line=&gt;line.contains(&quot;zks&quot;))</span><br><span class="line">//打印内容</span><br><span class="line">lines.collect().foreach(println(_));</span><br><span class="line">-------------输出------------------</span><br><span class="line">ff aa bb zks</span><br><span class="line">ee  zz zks</span><br></pre></td></tr></table></figure>
<p>java版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;F:\\sparktest\\sample.txt&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; zksRDD = lines.filter(new Function&lt;String, Boolean&gt;() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Boolean call(String s) throws Exception &#123;</span><br><span class="line">        return s.contains(&quot;zks&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">//打印内容</span><br><span class="line">List&lt;String&gt; zksCollect = zksRDD.collect();</span><br><span class="line">for (String str:zksCollect) &#123;</span><br><span class="line">    System.out.println(str);</span><br><span class="line">&#125;</span><br><span class="line">----------------输出-------------------</span><br><span class="line">ff aa bb zks</span><br><span class="line">ee  zz zks</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p>map() 接收一个函数，把这个函数用于 RDD 中的每个元素，将函数的返回结果作为结果RDD编程 ｜ 31<br>RDD 中对应元素的值 map是一对一的关系<br>举例，在F:\sparktest\sample.txt 文件的内容如下</p>
<blockquote>
<p>aa bb cc aa aa aa dd dd ee ee ee ee<br>ff aa bb zks<br>ee kks<br>ee  zz zks</p>
</blockquote>
<p>把每一行变成一个数组<br>scala版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//读取数据</span><br><span class="line">scala&gt; val lines = sc.textFile(&quot;F:\\sparktest\\sample.txt&quot;)</span><br><span class="line">//用map，对于每一行数据，按照空格分割成一个一个数组，然后返回的是一对一的关系</span><br><span class="line">scala&gt; var mapRDD = lines.map(line =&gt; line.split(&quot;\\s+&quot;))</span><br><span class="line">---------------输出-----------</span><br><span class="line">res0: Array[Array[String]] = Array(Array(aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee), Array(ff, aa, bb, zks), Array(ee, kks), Array(ee, zz, zks))</span><br><span class="line"></span><br><span class="line">//读取第一个元素</span><br><span class="line">scala&gt; mapRDD.first</span><br><span class="line">---输出----</span><br><span class="line">res1: Array[String] = Array(aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee)</span><br></pre></td></tr></table></figure>
<p>java版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Iterable&lt;String&gt;&gt; mapRDD = lines.map(new Function&lt;String, Iterable&lt;String&gt;&gt;() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Iterable&lt;String&gt; call(String s) throws Exception &#123;</span><br><span class="line">        String[] split = s.split(&quot;\\s+&quot;);</span><br><span class="line">        return Arrays.asList(split);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">//读取第一个元素</span><br><span class="line">System.out.println(mapRDD.first());</span><br><span class="line">---------------输出-------------</span><br><span class="line">[aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee]</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><p>有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap()<br>faltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器(想要了解更多，请参考scala的flatMap和map用法)<br>例如我们将数据切分为单词<br>scala版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val lines = sc.textFile(&quot;F:\\sparktest\\sample.txt&quot;)</span><br><span class="line">scala&gt; val flatMapRDD = lines.flatMap(line=&gt;line.split(&quot;\\s&quot;))</span><br><span class="line">scala&gt; flatMapRDD.first()</span><br><span class="line">---输出----</span><br><span class="line">res0: String = aa</span><br></pre></td></tr></table></figure>
<p>java版本，spark2.0以下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(&quot;F:\\sparktest\\sample.txt&quot;);</span><br><span class="line">JavaRDD&lt;String&gt; flatMapRDD = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Iterable&lt;String&gt; call(String s) throws Exception &#123;</span><br><span class="line">        String[] split = s.split(&quot;\\s+&quot;);</span><br><span class="line">        return Arrays.asList(split);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">//输出第一个</span><br><span class="line">System.out.println(flatMapRDD.first());</span><br><span class="line">------------输出----------</span><br><span class="line">aa</span><br></pre></td></tr></table></figure>
<p>java版本，spark2.0以上<br>spark2.0以上，对flatMap的方法有所修改，就是flatMap中的Iterator和Iteratable的小区别</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; flatMapRDD = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Iterator&lt;String&gt; call(String s) throws Exception &#123;</span><br><span class="line">        String[] split = s.split(&quot;\\s+&quot;);</span><br><span class="line">        return Arrays.asList(split).iterator();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h3><p>distinct用于去重， 我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销<br>scala版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var RDD1 = sc.parallelize(List(&quot;aa&quot;,&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;,&quot;dd&quot;))</span><br><span class="line"></span><br><span class="line">scala&gt; RDD1.collect</span><br><span class="line">res3: Array[String] = Array(aa, aa, bb, cc, dd)</span><br><span class="line"></span><br><span class="line">scala&gt; var distinctRDD = RDD1.distinct</span><br><span class="line"></span><br><span class="line">scala&gt; distinctRDD.collect</span><br><span class="line">res5: Array[String] = Array(aa, dd, bb, cc)</span><br></pre></td></tr></table></figure></p>
<p>java版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));</span><br><span class="line">JavaRDD&lt;String&gt; distinctRDD = RDD1.distinct();</span><br><span class="line">List&lt;String&gt; collect = distinctRDD.collect();</span><br><span class="line">for (String str:collect) &#123;</span><br><span class="line">    System.out.print(str+&quot;, &quot;);</span><br><span class="line">&#125;</span><br><span class="line">---------输出----------</span><br><span class="line">aa, dd, bb, cc,</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><p>两个RDD进行合并<br>scala版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var RDD1 = sc.parallelize(List(&quot;aa&quot;,&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;,&quot;dd&quot;))</span><br><span class="line">scala&gt; var RDD2 = sc.parallelize(List(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;))</span><br><span class="line"></span><br><span class="line">scala&gt; RDD1.collect</span><br><span class="line">res6: Array[String] = Array(aa, aa, bb, cc, dd)</span><br><span class="line"></span><br><span class="line">scala&gt; RDD2.collect</span><br><span class="line">res7: Array[String] = Array(aa, dd, ff)</span><br><span class="line"></span><br><span class="line">scala&gt; RDD1.union(RDD2).collect</span><br><span class="line">res8: Array[String] = Array(aa, aa, bb, cc, dd, aa, dd, ff)</span><br></pre></td></tr></table></figure></p>
<p>java版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));</span><br><span class="line">JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));</span><br><span class="line">JavaRDD&lt;String&gt; unionRDD = RDD1.union(RDD2);</span><br><span class="line">List&lt;String&gt; collect = unionRDD.collect();</span><br><span class="line">for (String str:collect) &#123;</span><br><span class="line">    System.out.print(str+&quot;, &quot;);</span><br><span class="line">&#125;</span><br><span class="line">-----------输出---------</span><br><span class="line">aa, aa, bb, cc, dd, aa, dd, ff,</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h3><p>RDD1.intersection(RDD2) 返回两个RDD的交集，并且去重<br>intersection 需要混洗数据，比较浪费性能<br>scala版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var RDD1 = sc.parallelize(List(&quot;aa&quot;,&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;,&quot;dd&quot;))</span><br><span class="line">scala&gt; var RDD2 = sc.parallelize(List(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;))</span><br><span class="line"></span><br><span class="line">scala&gt; RDD1.collect</span><br><span class="line">res6: Array[String] = Array(aa, aa, bb, cc, dd)</span><br><span class="line"></span><br><span class="line">scala&gt; RDD2.collect</span><br><span class="line">res7: Array[String] = Array(aa, dd, ff)</span><br><span class="line"></span><br><span class="line">scala&gt; var insertsectionRDD = RDD1.intersection(RDD2)</span><br><span class="line">scala&gt; insertsectionRDD.collect</span><br><span class="line"></span><br><span class="line">res9: Array[String] = Array(aa, dd)</span><br></pre></td></tr></table></figure></p>
<p>java版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));</span><br><span class="line">JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));</span><br><span class="line">JavaRDD&lt;String&gt; intersectionRDD = RDD1.intersection(RDD2);</span><br><span class="line">List&lt;String&gt; collect = intersectionRDD.collect();</span><br><span class="line">for (String str:collect) &#123;</span><br><span class="line">    System.out.print(str+&quot; &quot;);</span><br><span class="line">&#125;</span><br><span class="line">-------------输出-----------</span><br><span class="line">aa dd</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h3><p>RDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重<br>scala版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;,&quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));</span><br><span class="line"></span><br><span class="line">scala&gt; var substractRDD =RDD1.subtract(RDD2)</span><br><span class="line"></span><br><span class="line">scala&gt;  substractRDD.collect</span><br><span class="line">res10: Array[String] = Array(bb, cc)</span><br></pre></td></tr></table></figure></p>
<p>java版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;,&quot;cc&quot;, &quot;dd&quot;));</span><br><span class="line">JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));</span><br><span class="line">JavaRDD&lt;String&gt; subtractRDD = RDD1.subtract(RDD2);</span><br><span class="line">List&lt;String&gt; collect = subtractRDD.collect();</span><br><span class="line">for (String str:collect) &#123;</span><br><span class="line">    System.out.print(str+&quot; &quot;);</span><br><span class="line">&#125;</span><br><span class="line">------------输出-----------------</span><br><span class="line">bb  cc</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="cartesian"><a href="#cartesian" class="headerlink" title="cartesian"></a>cartesian</h3><p>RDD1.cartesian(RDD2) 返回RDD1和RDD2的笛卡儿积，这个开销非常大</p>
<p>scala版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  var RDD1 = sc.parallelize(List(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;))</span><br><span class="line"></span><br><span class="line">scala&gt; var RDD2 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;))</span><br><span class="line"></span><br><span class="line">scala&gt; var cartesianRDD = RDD1.cartesian(RDD2)</span><br><span class="line"></span><br><span class="line">scala&gt; cartesianRDD.collect</span><br><span class="line">res11: Array[(String, String)] = Array((1,a), (1,b), (1,c), (2,a), (2,b), (2,c), (3,a), (3,b), (3,c))</span><br></pre></td></tr></table></figure></p>
<p>java版本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;));</span><br><span class="line">JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;));</span><br><span class="line">JavaPairRDD&lt;String, String&gt; cartesian = RDD1.cartesian(RDD2);</span><br><span class="line"></span><br><span class="line">List&lt;Tuple2&lt;String, String&gt;&gt; collect1 = cartesian.collect();</span><br><span class="line">for (Tuple2&lt;String, String&gt; tp:collect1) &#123;</span><br><span class="line">    System.out.println(&quot;(&quot;+tp._1+&quot; &quot;+tp._2+&quot;)&quot;);</span><br><span class="line">&#125;</span><br><span class="line">------------输出-----------------</span><br><span class="line">(1 a)</span><br><span class="line">(1 b)</span><br><span class="line">(1 c)</span><br><span class="line">(2 a)</span><br><span class="line">(2 b)</span><br><span class="line">(2 c)</span><br><span class="line">(3 a)</span><br><span class="line">(3 b)</span><br><span class="line">(3 c)</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="mapValues"><a href="#mapValues" class="headerlink" title="mapValues"></a>mapValues</h3><p>def mapValues<a href="f: (V" target="_blank" rel="noopener">U</a> =&gt; U): RDD[(K, U)]</p>
<p>同基本转换操作中的map，只不过mapValues是针对[K,V]中的V值进行map操作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var rdd1 = sc.makeRDD(Array((1,&quot;A&quot;),(2,&quot;B&quot;),(3,&quot;C&quot;),(4,&quot;D&quot;)),2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[27] at makeRDD at :21</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.mapValues(x =&gt; x + &quot;_&quot;).collect</span><br><span class="line">res26: Array[(Int, String)] = Array((1,A_), (2,B_), (3,C_), (4,D_))</span><br></pre></td></tr></table></figure>
<h3 id="flatMapValues"><a href="#flatMapValues" class="headerlink" title="flatMapValues"></a>flatMapValues</h3><p>def flatMapValues<a href="f: (V" target="_blank" rel="noopener">U</a> =&gt; TraversableOnce[U]): RDD[(K, U)]</p>
<p>同基本转换操作中的flatMap，只不过flatMapValues是针对[K,V]中的V值进行flatMap操作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.flatMapValues(x =&gt; x + &quot;_&quot;).collect</span><br><span class="line">res36: Array[(Int, Char)] = Array((1,A), (1,_), (2,B), (2,_), (3,C), (3,_), (4,D), (4,_))</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile"></a>saveAsTextFile</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def saveAsTextFile(path: String): Unit</span><br><span class="line"></span><br><span class="line">def saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec]): Unit</span><br></pre></td></tr></table></figure>
<p>saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。</p>
<p>codec参数可以指定压缩的类名。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">scala&gt; rdd1.saveAsTextFile(&quot;hdfs://cdh5/tmp/test/&quot;) //保存到HDFS</span><br><span class="line">hadoop fs -ls /tmp/test</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   2 bob supergroup        0 2015-07-10 09:15 /tmp/test/_SUCCESS</span><br><span class="line">-rw-r--r--   2 bob supergroup        21 2015-07-10 09:15 /tmp/test/part-00000</span><br><span class="line"></span><br><span class="line">hadoop fs -cat /tmp/test/part-00000</span><br></pre></td></tr></table></figure></p>
<p>注意：如果使用rdd1.saveAsTextFile(“file:///tmp/test”)将文件保存到本地文件系统，那么只会保存在Executor所在机器的本地目录。<br>指定压缩格式保存<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rdd1.saveAsTextFile(&quot;hdfs://cdh5/tmp/test/&quot;,classOf[com.hadoop.compression.lzo.LzopCodec])</span><br><span class="line"></span><br><span class="line">hadoop fs -ls /tmp/test</span><br><span class="line">-rw-r--r--   2 bob supergroup    0 2015-07-10 09:20 /tmp/test/_SUCCESS</span><br><span class="line">-rw-r--r--   2 bob supergroup    71 2015-07-10 09:20 /tmp/test/part-00000.lzo</span><br><span class="line"></span><br><span class="line">hadoop fs -text /tmp/test/part-00000.lzo</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="saveAsSequenceFile"><a href="#saveAsSequenceFile" class="headerlink" title="saveAsSequenceFile"></a>saveAsSequenceFile</h3><p>saveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。</p>
<p>用法同saveAsTextFile。</p>
<hr>
<h3 id="saveAsObjectFile"><a href="#saveAsObjectFile" class="headerlink" title="saveAsObjectFile"></a>saveAsObjectFile</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def saveAsObjectFile(path: String): Unit</span><br><span class="line"></span><br><span class="line">saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。</span><br><span class="line"></span><br><span class="line">对于HDFS，默认采用SequenceFile保存。</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">rdd1.saveAsObjectFile(&quot;hdfs://cdh5/tmp/test/&quot;)</span><br><span class="line"></span><br><span class="line">hadoop fs -cat /tmp/test/part-00000</span><br><span class="line">SEQ !org.apache.hadoop.io.NullWritable&quot;org.apache.hadoop.io.BytesWritableT</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="saveAsHadoopFile"><a href="#saveAsHadoopFile" class="headerlink" title="saveAsHadoopFile"></a>saveAsHadoopFile</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def saveAsHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[_ &lt;: OutputFormat[, ]], codec: Class[_ &lt;: CompressionCodec]): Unit</span><br><span class="line"></span><br><span class="line">def saveAsHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[_ &lt;: OutputFormat[, ]], conf: JobConf = …, codec: Option[Class[_ &lt;: CompressionCodec]] = None): Unit</span><br><span class="line"></span><br><span class="line">saveAsHadoopFile是将RDD存储在HDFS上的文件中，支持老版本Hadoop API。</span><br></pre></td></tr></table></figure>
<p>可以指定outputKeyClass、outputValueClass以及压缩格式。</p>
<p>每个分区输出一个文件。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">var rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;A&quot;,1),(&quot;B&quot;,6),(&quot;B&quot;,3),(&quot;B&quot;,7)))</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.mapred.TextOutputFormat</span><br><span class="line">import org.apache.hadoop.io.Text</span><br><span class="line">import org.apache.hadoop.io.IntWritable</span><br><span class="line"></span><br><span class="line">rdd1.saveAsHadoopFile(&quot;/tmp/test/&quot;,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]])</span><br><span class="line"></span><br><span class="line">rdd1.saveAsHadoopFile(&quot;/tmp/test/&quot;,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]],</span><br><span class="line">                      classOf[com.hadoop.compression.lzo.LzopCodec])</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="saveAsHadoopDataset"><a href="#saveAsHadoopDataset" class="headerlink" title="saveAsHadoopDataset"></a>saveAsHadoopDataset</h3><p>def saveAsHadoopDataset(conf: JobConf): Unit</p>
<p>saveAsHadoopDataset用于将RDD保存到除了HDFS的其他存储中，比如HBase。</p>
<p>在JobConf中，通常需要关注或者设置五个参数：</p>
<p>文件的保存路径、key值的class类型、value值的class类型、RDD的输出格式(OutputFormat)、以及压缩相关的参数。<br>使用saveAsHadoopDataset将RDD保存到HDFS中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line">import org.apache.hadoop.mapred.TextOutputFormat</span><br><span class="line">import org.apache.hadoop.io.Text</span><br><span class="line">import org.apache.hadoop.io.IntWritable</span><br><span class="line">import org.apache.hadoop.mapred.JobConf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;A&quot;,1),(&quot;B&quot;,6),(&quot;B&quot;,3),(&quot;B&quot;,7)))</span><br><span class="line">var jobConf = new JobConf()</span><br><span class="line">jobConf.setOutputFormat(classOf[TextOutputFormat[Text,IntWritable]])</span><br><span class="line">jobConf.setOutputKeyClass(classOf[Text])</span><br><span class="line">jobConf.setOutputValueClass(classOf[IntWritable])</span><br><span class="line">jobConf.set(&quot;mapred.output.dir&quot;,&quot;/tmp/bob/&quot;)</span><br><span class="line">rdd1.saveAsHadoopDataset(jobConf)</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">hadoop fs -cat /tmp/bob/part-00000</span><br><span class="line">A       2</span><br><span class="line">A       1</span><br><span class="line">hadoop fs -cat /tmp/bob/part-00001</span><br><span class="line">B       6</span><br><span class="line">B       3</span><br><span class="line">B       7</span><br></pre></td></tr></table></figure></p>
<p>保存数据到HBASE<br>HBase建表：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">create ‘bob′,&#123;NAME =&gt; ‘f1′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f2′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f3′,VERSIONS =&gt; 1&#125;</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line">import org.apache.hadoop.mapred.TextOutputFormat</span><br><span class="line">import org.apache.hadoop.io.Text</span><br><span class="line">import org.apache.hadoop.io.IntWritable</span><br><span class="line">import org.apache.hadoop.mapred.JobConf</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration</span><br><span class="line">import org.apache.hadoop.hbase.mapred.TableOutputFormat</span><br><span class="line">import org.apache.hadoop.hbase.client.Put</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line"></span><br><span class="line">var conf = HBaseConfiguration.create()</span><br><span class="line">    var jobConf = new JobConf(conf)</span><br><span class="line">    jobConf.set(&quot;hbase.zookeeper.quorum&quot;,&quot;zkNode1,zkNode2,zkNode3&quot;)</span><br><span class="line">    jobConf.set(&quot;zookeeper.znode.parent&quot;,&quot;/hbase&quot;)</span><br><span class="line">    jobConf.set(TableOutputFormat.OUTPUT_TABLE,&quot;bob&quot;)</span><br><span class="line">    jobConf.setOutputFormat(classOf[TableOutputFormat])</span><br><span class="line"></span><br><span class="line">    var rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;B&quot;,6),(&quot;C&quot;,7)))</span><br><span class="line">    rdd1.map(x =&gt;</span><br><span class="line">      &#123;</span><br><span class="line">        var put = new Put(Bytes.toBytes(x._1))</span><br><span class="line">        put.add(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;c1&quot;), Bytes.toBytes(x._2))</span><br><span class="line">        (new ImmutableBytesWritable,put)</span><br><span class="line">      &#125;</span><br><span class="line">    ).saveAsHadoopDataset(jobConf)</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">hbase(main):005:0&gt; scan &apos;bob&apos;</span><br><span class="line">ROW     COLUMN+CELL                                                                                                </span><br><span class="line"> A       column=f1:c1, timestamp=1436504941187, value=\x00\x00\x00\x02                                              </span><br><span class="line"> B       column=f1:c1, timestamp=1436504941187, value=\x00\x00\x00\x06                                              </span><br><span class="line"> C       column=f1:c1, timestamp=1436504941187, value=\x00\x00\x00\x07                                              </span><br><span class="line">3 row(s) in 0.0550 seconds</span><br></pre></td></tr></table></figure></p>
<p>注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。</p>
<hr>
<h3 id="saveAsNewAPIHadoopFile"><a href="#saveAsNewAPIHadoopFile" class="headerlink" title="saveAsNewAPIHadoopFile"></a>saveAsNewAPIHadoopFile</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def saveAsNewAPIHadoopFile[F &lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unit</span><br><span class="line"></span><br><span class="line">def saveAsNewAPIHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[_ &lt;: OutputFormat[, ]], conf: Configuration = self.context.hadoopConfiguration): Unit</span><br></pre></td></tr></table></figure>
<p>saveAsNewAPIHadoopFile用于将RDD数据保存到HDFS上，使用新版本Hadoop API。</p>
<p>用法基本同saveAsHadoopFile。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat</span><br><span class="line">import org.apache.hadoop.io.Text</span><br><span class="line">import org.apache.hadoop.io.IntWritable</span><br><span class="line"></span><br><span class="line">var rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;A&quot;,1),(&quot;B&quot;,6),(&quot;B&quot;,3),(&quot;B&quot;,7)))</span><br><span class="line">rdd1.saveAsNewAPIHadoopFile(&quot;/tmp/bob/&quot;,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]])</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="saveAsNewAPIHadoopDataset"><a href="#saveAsNewAPIHadoopDataset" class="headerlink" title="saveAsNewAPIHadoopDataset"></a>saveAsNewAPIHadoopDataset</h3><p>def saveAsNewAPIHadoopDataset(conf: Configuration): Unit</p>
<p>作用同saveAsHadoopDataset,只不过采用新版本Hadoop API。</p>
<p>以写入HBase为例：</p>
<p>HBase建表：</p>
<p>create ‘bob′,{NAME =&gt; ‘f1′,VERSIONS =&gt; 1},{NAME =&gt; ‘f2′,VERSIONS =&gt; 1},{NAME =&gt; ‘f3′,VERSIONS =&gt; 1}</p>
<p>完整的Spark应用程序：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">package com.bob.test</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import SparkContext._</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration</span><br><span class="line">import org.apache.hadoop.mapreduce.Job</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableOutputFormat</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line">import org.apache.hadoop.hbase.client.Result</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes</span><br><span class="line">import org.apache.hadoop.hbase.client.Put</span><br><span class="line"></span><br><span class="line">object Test &#123;</span><br><span class="line">  def main(args : Array[String]) &#123;</span><br><span class="line">   val sparkConf = new SparkConf().setMaster(&quot;spark://test:7077&quot;).setAppName(&quot;test&quot;)</span><br><span class="line">   val sc = new SparkContext(sparkConf);</span><br><span class="line">   var rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;B&quot;,6),(&quot;C&quot;,7)))</span><br><span class="line"></span><br><span class="line">    sc.hadoopConfiguration.set(&quot;hbase.zookeeper.quorum &quot;,&quot;zkNode1,zkNode2,zkNode3&quot;)</span><br><span class="line">    sc.hadoopConfiguration.set(&quot;zookeeper.znode.parent&quot;,&quot;/hbase&quot;)</span><br><span class="line">    sc.hadoopConfiguration.set(TableOutputFormat.OUTPUT_TABLE,&quot;bob&quot;)</span><br><span class="line">    var job = new Job(sc.hadoopConfiguration)</span><br><span class="line">    job.setOutputKeyClass(classOf[ImmutableBytesWritable])</span><br><span class="line">    job.setOutputValueClass(classOf[Result])</span><br><span class="line">    job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])</span><br><span class="line"></span><br><span class="line">    rdd1.map(</span><br><span class="line">      x =&gt; &#123;</span><br><span class="line">        var put = new Put(Bytes.toBytes(x._1))</span><br><span class="line">        put.add(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;c1&quot;), Bytes.toBytes(x._2))</span><br><span class="line">        (new ImmutableBytesWritable,put)</span><br><span class="line">      &#125;    </span><br><span class="line">    ).saveAsNewAPIHadoopDataset(job.getConfiguration)</span><br><span class="line"></span><br><span class="line">    sc.stop()   </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。</p>

        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/17/Hbase环境搭建/">Hbase环境搭建</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-17
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/17/Hbase环境搭建/"
             data-title="Hbase环境搭建">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <h3 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h3><ul>
<li>hbase-env.sh</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/home/grid/jdk1.7.0_75  </span><br><span class="line">export HBASE_HOME=/home/grid/hbase  </span><br><span class="line">export HBASE_LOG_DIR=/tmp/grid/logs  </span><br><span class="line">export HBASE_MANAGES_ZK=true</span><br></pre></td></tr></table></figure>
<ul>
<li>hbase-site.xml</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;  </span><br><span class="line">    &lt;property&gt;  </span><br><span class="line">        &lt;name&gt;hbase.rootdir&lt;/name&gt; # 设置 hbase 数据库存放数据的目录  </span><br><span class="line">        &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt;  </span><br><span class="line">    &lt;/property&gt;  </span><br><span class="line"></span><br><span class="line">    &lt;property&gt;  </span><br><span class="line">        &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;  # 打开 hbase 分布模式  </span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;  </span><br><span class="line">    &lt;/property&gt;  </span><br><span class="line"></span><br><span class="line">    &lt;property&gt;  </span><br><span class="line">        &lt;name&gt;hbase.master&lt;/name&gt; # 指定 hbase 集群主控节点  </span><br><span class="line">        &lt;value&gt;master:60000&lt;/value&gt;  </span><br><span class="line">    &lt;/property&gt;  </span><br><span class="line"></span><br><span class="line">    &lt;property&gt;  </span><br><span class="line">        &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;  </span><br><span class="line">        &lt;value&gt;master,slave1,slave2&lt;/value&gt; # 指定 zookeeper 集群节点名 , 因为是由 zookeeper 表决算法决定的  </span><br><span class="line">    &lt;/property&gt;  </span><br><span class="line"></span><br><span class="line">    &lt;property&gt;  </span><br><span class="line">        &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; # 指 zookeeper 集群 data 目录  </span><br><span class="line">        &lt;value&gt;/home/grid/hbase/zookeeper&lt;/value&gt;  </span><br><span class="line">    &lt;/property&gt;  </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>regionservers</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slave1  </span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>
<h3 id="启动过程"><a href="#启动过程" class="headerlink" title="启动过程"></a>启动过程</h3><ol>
<li><p>启动Hadoop</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$HADOOP_HOME/sbin/start-dfs.sh  </span><br><span class="line">$HADOOP_HOME/sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动Hbase</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$HBASE_HOME/bin/start-hbase.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看hbase启动情况<br>在master上有HQuorumPeer和HMaster进程，在slave1、slave2上有HQuorumPeer和HRegionServer进程</p>
<blockquote>
<p>通过浏览器验证：<a href="http://nodeh1:16010/master-status" target="_blank" rel="noopener">http://nodeh1:16010/master-status</a></p>
</blockquote>
</li>
<li><p>命令行启动</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/home/grid/hbase/bin/hbase shell</span><br></pre></td></tr></table></figure>
</li>
</ol>

        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/17/Hadoop环境搭建/">Hadoop环境搭建</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-17
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/17/Hadoop环境搭建/"
             data-title="Hadoop环境搭建">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <h3 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h3><ul>
<li><p>hadoop-env.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/java/jdk1.7.0_80</span><br><span class="line">export HADOOP_PREFIX=/opt/hadoop-2.6.4</span><br></pre></td></tr></table></figure>
</li>
<li><p>core-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://master:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/opt/hadoop-2.6.4/tmp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：tmp目录需提前创建</p>
</blockquote>
</li>
<li><p>hdfs-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>数据有三个副本</p>
</blockquote>
</li>
<li><p>mapred-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>yarn-env.sh</p>
<blockquote>
<p>增加 JAVA_HOME 配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/java/jdk1.7.0_80</span><br></pre></td></tr></table></figure>
</blockquote>
</li>
<li><p>yarn-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Site specific YARN configuration properties --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;nodeh1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>slaves</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master</span><br><span class="line">slave01</span><br><span class="line">slave02</span><br></pre></td></tr></table></figure>
<blockquote>
<p>master 即作为 NameNode 也作为 DataNode。</p>
</blockquote>
</li>
</ul>
<h3 id="启动过程"><a href="#启动过程" class="headerlink" title="启动过程"></a>启动过程</h3><ol>
<li><p>格式化文件系统，在 master 上执行以下命令：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动 NameNode 和 DateNode，执行 <code>start-dfs.sh</code>，使用 jps 命令查看进程。</p>
</li>
<li><p>输入地址：<a href="http://master:50070/" target="_blank" rel="noopener">http://master:50070/</a> 可以查看 NameNode 信息。</p>
</li>
<li><p>启动 ResourceManager 和 NodeManager，运行 <code>start-yarn.sh</code>, 使用 jps 查看  进程</p>
</li>
</ol>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount /data/input /data/output/result</span><br></pre></td></tr></table></figure>

        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/17/Spark环境搭建/">Spark环境搭建</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-17
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/17/Spark环境搭建/"
             data-title="Spark环境搭建">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <h3 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h3><p>进入 Spark 安装目录下的 conf 目录， 拷贝 spark-env.sh.template 到 spark-env.sh。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure></p>
<p>编辑 spark-env.sh，在其中添加以下配置信息：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export SCALA_HOME=/opt/scala-2.11.8</span><br><span class="line">export JAVA_HOME=/opt/java/jdk1.7.0_80</span><br><span class="line">export SPARK_MASTER_IP=192.168.109.137</span><br><span class="line">export SPARK_WORKER_MEMORY=1g</span><br><span class="line">export HADOOP_CONF_DIR=/opt/hadoop-2.6.4/etc/hadoop</span><br></pre></td></tr></table></figure></p>
<p>将 slaves.template 拷贝到 slaves， 编辑其内容为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master</span><br><span class="line">slave01</span><br><span class="line">slave02</span><br></pre></td></tr></table></figure></p>
<p>即 master 既是 Master 节点又是 Worker 节点。</p>
<h3 id="启动过程"><a href="#启动过程" class="headerlink" title="启动过程"></a>启动过程</h3><blockquote>
<ol>
<li>运行 start-master.sh，启动Master节点，可以看到master上多了一个新进程Master。</li>
<li>运行 start-slaves.sh，启动所有Worker节点，在 master、slave01和slave02上使用jps命令，可以发现都启动了一个Worker进程</li>
<li>访问 <a href="http://master:8080" target="_blank" rel="noopener">http://master:8080</a>  浏览器查看Spark集群信息。</li>
<li>运行 spark-shell，可以进入Spark的shell控制台</li>
<li>访问 <a href="http://master:4040" target="_blank" rel="noopener">http://master:4040</a>  浏览器访问SparkUI， 可以从SparkUI上查看一些 如环境变量、Job、Executor等信息。</li>
</ol>
</blockquote>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>计算 π 的近似值<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$SPARK_HOME/bin/run-example SparkPi 2&gt;&amp;1 | grep &quot;Pi is roughly&quot;</span><br></pre></td></tr></table></figure></p>

        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/16/环境搭建的常用命令/">集群环境搭建的常用命令</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-16
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/16/环境搭建的常用命令/"
             data-title="集群环境搭建的常用命令">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <h4 id="关闭防火墙和SELinux"><a href="#关闭防火墙和SELinux" class="headerlink" title="关闭防火墙和SELinux"></a>关闭防火墙和SELinux</h4><blockquote>
<p><strong>关闭命令：</strong> service iptables stop<br><strong>永久关闭防火墙：</strong> chkconfig iptables off<br><strong>查看防火墙关闭状态:</strong>  service iptables status</p>
</blockquote>
<blockquote>
<p>关闭selinux 服务<br>vim /etc/selinux/config<br>修改 SELINUX=disabled</p>
</blockquote>
<h4 id="无密码登录"><a href="#无密码登录" class="headerlink" title="无密码登录"></a>无密码登录</h4><blockquote>
<p>ssh-keygen -t rsa<br>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>chmod 600 ~/.ssh/authorized_keys<br>ssh localhost</p>
</blockquote>
<h4 id="显示文件夹大小并排序"><a href="#显示文件夹大小并排序" class="headerlink" title="显示文件夹大小并排序"></a>显示文件夹大小并排序</h4><blockquote>
<p>du -h –max-depth=1 ./  | sort -n</p>
</blockquote>
<h4 id="将旧版本的jdk升级为新版本"><a href="#将旧版本的jdk升级为新版本" class="headerlink" title="将旧版本的jdk升级为新版本"></a>将旧版本的jdk升级为新版本</h4><blockquote>
<p>sudo update-alternatives –install /usr/bin/java java /opt/soft/jdk/bin/java 300<br>sudo update-alternatives –install /usr/bin/javac javac /opt/soft/jdk/bin/javac 300<br>sudo update-alternatives –config java</p>
</blockquote>
<h4 id="ecilpse访问的权限设置"><a href="#ecilpse访问的权限设置" class="headerlink" title="ecilpse访问的权限设置"></a>ecilpse访问的权限设置</h4><blockquote>
<p>groupadd supergroup # 添加supergroup组<br>useradd -g supergroup hadoop # 添加hadoop用户到supergroup组<br>hadoop fs -chmod 777 /</p>
</blockquote>
<h4 id="查看系统版本"><a href="#查看系统版本" class="headerlink" title="查看系统版本"></a>查看系统版本</h4><blockquote>
<p><strong>其他系列</strong> cat /proc/version<br><strong>redhat系列</strong> cat /etc/redhat-release</p>
</blockquote>
<h4 id="修改hostname"><a href="#修改hostname" class="headerlink" title="修改hostname"></a>修改hostname</h4><blockquote>
<p><strong>centos 6</strong><br>vim /etc/sysconfig/network<br>hostname yourhostname<br><strong>centos 7</strong><br>hostnamectl set-hostname yourhostname</p>
</blockquote>
<h4 id="Spark-上传jar包"><a href="#Spark-上传jar包" class="headerlink" title="Spark 上传jar包"></a>Spark 上传jar包</h4><blockquote>
<p>/opt/moudles/spark-1.6.1/bin/spark-submit –class SparkWordCount sparkStudy.jar  –master=spark://192.168.20.171:7077</p>
</blockquote>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><blockquote>
<p>chmod -R 777 ./spark</p>
</blockquote>
<h4 id="时区修改"><a href="#时区修改" class="headerlink" title="时区修改"></a>时区修改</h4><blockquote>
<p>时区的配置文件是/etc/sysconfig/clock。用tzselect命令就可以修改这个配置文件，根据命令的提示进行修改就好了。<br>但是在实际工作中，发现这种方式是不能够使得服务器上的时间设置马上生效的，而且使用ntpdate去同步时间服务器也不能够更改时间。即使你使用了 date命令手工设置了时间的话，如果使用ntpdate去进行时间同步的话，时间又会被改动到原来的错误时区的时间。而生产的机器往往是非常重要的，不能够进行重启等操作。<br>如果要修改时区并且马上生效，可以更换/etc/localtime 文件来实现。比如修改时区为中国上海，那么就可以使用如下的命令来使得时区的更改生效。<br>cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime<br>然后最好使用下面的命令将更改写入bios。<br>hwclock</p>
</blockquote>

        
      
    </div>

    

    

  </article>

    
      
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          <a class="post-link" href="/2018/04/11/SpagoBI5.2开发环境搭建/">SpagoBI5.2开发环境搭建</a>
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2018-04-11
        </span>
        
        
        <div class="post-visits"
             data-url="/2018/04/11/SpagoBI5.2开发环境搭建/"
             data-title="SpagoBI5.2开发环境搭建">
            posts.visits
          </div>
        
      </div>
    </header>

    
    

    <div class="post-content">
      
        
        

        
          <p>本文暂不介绍kylin具体的搭建过程，而是将遇到的问题进行了总结，具体的搭建过程可能在后续进行更新。</p>
<hr>
<p>本地运行SpagoBI时需要JDK1.8的运行环境。</p>
<hr>
<p>表图引擎模块的action</p>
<blockquote>
<p>Bichartengine/WEB-INF/conf/commons/actions.xml</p>
</blockquote>
<p>整个项目的Service</p>
<blockquote>
<p>Spagobi-core/src/it/eng/spagobi/wepp/service</p>
</blockquote>
<hr>
<p>导入sql文件出错时</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; SET GLOBAL sql_mode = &apos;&apos;;   </span><br><span class="line">Query OK, 0 rows affected, 1 warning (0.00 sec)   </span><br><span class="line">mysql&gt; commit;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line">mysql&gt; exit;</span><br></pre></td></tr></table></figure>
<hr>
<p>按照从SVN中导出的代码的Server.xml进行修改</p>
<blockquote>
<p>同时修改hibernate.cfg.xml<br>配置数据源 hibernate.cfg.xml (SpagoBI/src/main/resources)</p>
</blockquote>
<p>注意Server.xml中配置数据库时注意大小写</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&lt;GlobalNamingResources&gt;</span><br><span class="line">    &lt;!-- Editable user database that can also be used by</span><br><span class="line">         UserDatabaseRealm to authenticate users</span><br><span class="line">    --&gt;</span><br><span class="line">    &lt;Resource auth=&quot;Container&quot; description=&quot;User database that can be updated and saved&quot; factory=&quot;org.apache.catalina.users.MemoryUserDatabaseFactory&quot; name=&quot;UserDatabase&quot; pathname=&quot;conf/tomcat-users.xml&quot; type=&quot;org.apache.catalina.UserDatabase”/&gt;</span><br><span class="line"></span><br><span class="line">    &lt;Environment name=&quot;spagobi_resource_path&quot; type=&quot;java.lang.String&quot; value=&quot;C:\progetti\spagobi2.0\workspace\resources&quot;/&gt;</span><br><span class="line">    &lt;Environment name=&quot;spagobi_sso_class&quot; type=&quot;java.lang.String&quot; value=&quot;it.eng.spagobi.services.common.FakeSsoService&quot;/&gt;</span><br><span class="line">    &lt;Environment name=&quot;spagobi_service_url&quot; type=&quot;java.lang.String&quot; value=&quot;http://localhost:8080/SpagoBI&quot;/&gt;    </span><br><span class="line">    &lt;Environment name=&quot;spagobi_host_url&quot; type=&quot;java.lang.String&quot; value=&quot;http://localhost:8080&quot;/&gt;</span><br><span class="line"></span><br><span class="line">    &lt;Resource auth=&quot;Container&quot; factory=&quot;de.myfoo.commonj.work.FooWorkManagerFactory&quot; maxThreads=&quot;5&quot;</span><br><span class="line">          name=&quot;wm/SpagoWorkManager&quot; type=&quot;commonj.work.WorkManager&quot;/&gt;</span><br><span class="line"></span><br><span class="line">    &lt;Resource auth=&quot;Container&quot; driverClassName=&quot;com.mysql.jdbc.Driver&quot;</span><br><span class="line">          maxActive=&quot;20&quot; maxIdle=&quot;10&quot; maxWait=&quot;-1&quot; name=&quot;jdbc/foodmart&quot;</span><br><span class="line">          password=&quot;root&quot; type=&quot;javax.sql.DataSource&quot;</span><br><span class="line">          url=&quot;jdbc:mysql://localhost/foodmart&quot; username=&quot;root&quot;/&gt;          </span><br><span class="line"></span><br><span class="line">    &lt;Resource name=&quot;jdbc/spagobi&quot; auth=&quot;Container&quot;</span><br><span class="line">          type=&quot;javax.sql.DataSource&quot; driverClassName=&quot;com.mysql.jdbc.Driver&quot;</span><br><span class="line">          url=&quot;jdbc:mysql://localhost/spagobi&quot;</span><br><span class="line">          username=&quot;root&quot; password=&quot;root&quot; maxActive=&quot;20&quot; maxIdle=&quot;10&quot;</span><br><span class="line">          maxWait=&quot;-1&quot;/&gt;  </span><br><span class="line"></span><br><span class="line">  &lt;/GlobalNamingResources&gt;</span><br></pre></td></tr></table></figure>
<hr>
<p>虽然代码很多url等都写入了配置文件，但<strong>server.xml</strong>中的<strong>host url</strong>还是需要手动修改</p>
<hr>
<p>记得将符合自己mysql版本的jar包放到tomcat的lib中</p>
<hr>
<blockquote>
<p>Unsupported major.minor version 52.0</p>
</blockquote>
<p>这个错误不要把JRE更换到高版本，更换到高版本会报一个新的15.0错误，没用，而是应该修改<strong>web.xml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;web-app xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot; xmlns:web=&quot;http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd&quot; id=&quot;WebApp_ID&quot; version=&quot;3.0&quot; metadata-complete=&quot;true” &gt;</span><br></pre></td></tr></table></figure>
<p>添加一句：metadata-complete=”true” 就搞定了，我猜想由于项目是原来的项目，web.xml是原来项目的，和原来的什么版本可能有冲突，具体的也不清楚了</p>
<hr>
<p>按照src中的whatiftemplate的替换一下，可以修正错误</p>
<hr>
<p>在部署过多项目，如果报内存溢出异常，则需要修改一下tomcat的设置</p>
<blockquote>
<p>-Xms256m -Xmx512m -XX:MaxNewSize=256m -XX:MaxPermSize=256m</p>
</blockquote>
<hr>
<blockquote>
<p>Servlet mapping specifies an unknown servlet name AdapterHTTP</p>
<p>Servlet 2.3 jar not loaded</p>
</blockquote>
<p>在Java Resource 中找到对应jar包，然后exclude<br>不知道是否正确</p>

        
      
    </div>

    

    

  </article>

    
  </section>

  
  <nav class="pagination">
    
    
      <a class="next" href="/page/2/">
        <span class="next-text">下一页</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>


          </div>
          

        </div>
      </main>

      <footer id="footer" class="footer">

  <div class="social-links">
    
      
        
          <a href="mailto:pekeyli@qq.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
          <a href="/PekeyLi" class="iconfont icon-weibo" title="weibo"></a>
        
      
    
      
    
      
    
      
    
      
    
      
    
    
    
      
      <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    
  </div>


<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">
    
    &copy; 
    
    2018

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Pekey Li</span>
  </span>
</div>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  


    <script type="text/javascript" src="/js/src/even.js?v=2.6.0"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=2.6.0"></script>

  </body>
</html>
