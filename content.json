{"meta":{"title":"Pekey","subtitle":"Pekey的笔记本","description":null,"author":"Pekey Li","url":"http://yoursite.com"},"pages":[{"title":"关于","date":"2018-04-02T13:38:48.400Z","updated":"2018-04-02T13:38:48.389Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"QQ: 421820144 Email: pekeyli@qq.com Github: @pekeyli"}],"posts":[{"title":"kylin开发环境搭建","slug":"kylin开发环境搭建","date":"2018-05-06T07:29:46.000Z","updated":"2018-05-06T07:30:23.874Z","comments":true,"path":"2018/05/06/kylin开发环境搭建/","link":"","permalink":"http://yoursite.com/2018/05/06/kylin开发环境搭建/","excerpt":"","text":"maven安装下载所需的文件http://maven.apache.org/download.cgi 1234cd ~wget http://xenia.sote.hu/ftp/mirrors/www.apache.org/maven/maven-3/3.2.5/binaries/apache-maven-3.2.5-bin.tar.gztar -xzvf apache-maven-3.2.5-bin.tar.gzln -s /root/apache-maven-3.2.5/bin/mvn /usr/bin/mvn nodejs安装下载所需的包文件，当下node的最新稳定版本为6.10.3 (具体可根据node的英文官网查看)1wget https://npm.taobao.org/mirrors/node/v6.10.3/node-v6.10.3-linux-x64.tar.xz 然后对xz文件进行解压1xz -d node-v6.10.3-linux-x64.tar.xz 对tar目录进行解压1tar -xvf node-v6.10.3-linux-x64.tar 返回根目录，对文件设置软连接1ln -s /node-v6.10.3-linux-x64/bin/node /usr/local/bin/node 同样对npm也设置软连接1ln -s /node-v6.10.3-linux-x64/bin/npm /usr/local/bin/npm 查看版本号是否安装成功1node -v 使用淘宝镜像1npm config set registry https://registry.npm.taobao.org Kylin编译First clone the Kylin project to your local:1git clone https://github.com/apache/kylin.git or1tar -xzvf apache-kylin-2.1.0-src.tar.gz Install Kylin artifacts to the maven repo1mvn clean install -DskipTests","categories":[],"tags":[]},{"title":"Kylin Cube构建过程学习","slug":"Kylin-Cube构建过程学习","date":"2018-04-20T14:31:09.000Z","updated":"2018-04-26T14:14:09.673Z","comments":true,"path":"2018/04/20/Kylin-Cube构建过程学习/","link":"","permalink":"http://yoursite.com/2018/04/20/Kylin-Cube构建过程学习/","excerpt":"","text":"参考文献：https://blog.bcmeng.com MapReduce 计算引擎 批量计算Cube，其输入是Hive表，输出是HBase的KeyValue，整个构建过程主要包含以下6步： 建立Hive的大宽表； （MapReduce计算） 对需要字典编码的列计算列基数； （MapReduce计算） 构建字典； （JobServer计算 or MapReduce计算） 分层构建Cuboid； （MapReduce计算） 将Cuboid转为HBase的KeyValue结构（HFile）； （MapReduce计算） 元数据更新和垃圾回收。Cube Build流程 CubeController123456789101112131415private JobInstance buildInternal(String cubeName, TSRange tsRange, SegmentRange segRange, Map&lt;Integer, Long&gt; sourcePartitionOffsetStart, Map&lt;Integer, Long&gt; sourcePartitionOffsetEnd, String buildType, boolean force) &#123; try &#123; String submitter = SecurityContextHolder.getContext().getAuthentication().getName(); CubeInstance cube = jobService.getCubeManager().getCube(cubeName); if (cube == null) &#123; throw new InternalErrorException(&quot;Cannot find cube &quot; + cubeName); &#125; return jobService.submitJob(cube, tsRange, segRange, sourcePartitionOffsetStart, sourcePartitionOffsetEnd, CubeBuildTypeEnum.valueOf(buildType), force, submitter); &#125; catch (Throwable e) &#123; logger.error(e.getLocalizedMessage(), e); throw new InternalErrorException(e.getLocalizedMessage(), e); &#125; &#125; JobService123456ISource source = SourceFactory.getSource(cube);SourcePartition src = new SourcePartition(tsRange, segRange, sourcePartitionOffsetStart, sourcePartitionOffsetEnd);src = source.enrichSourcePartitionBeforeBuild(cube, src);newSeg = getCubeManager().appendSegment(cube, src);job = EngineFactory.createBatchCubingJob(newSeg, submitter); EngineFactory 1234/** Build a new cube segment, typically its time range appends to the end of current cube. */ public static DefaultChainedExecutable createBatchCubingJob(CubeSegment newSegment, String submitter) &#123; return batchEngine(newSegment).createBatchCubingJob(newSegment, submitter); &#125; MRBatchCubingEngine2123public DefaultChainedExecutable createBatchCubingJob(CubeSegment newSegment, String submitter) &#123; return new BatchCubingJobBuilder2(newSegment, submitter).build();&#125; BatchCubingJobBuilder2 123456789101112131415161718192021222324252627282930313233public CubingJob build() &#123; logger.info(&quot;MR_V2 new job to BUILD segment &quot; + seg); final CubingJob result = CubingJob.createBuildJob(seg, submitter, config); final String jobId = result.getId(); final String cuboidRootPath = getCuboidRootPath(jobId); // Phase 1: Create Flat Table &amp; Materialize Hive View in Lookup Tables inputSide.addStepPhase1_CreateFlatTable(result); // Phase 2: Build Dictionary result.addTask(createFactDistinctColumnsStep(jobId)); if (isEnableUHCDictStep()) &#123; result.addTask(createBuildUHCDictStep(jobId)); &#125; result.addTask(createBuildDictionaryStep(jobId)); result.addTask(createSaveStatisticsStep(jobId)); outputSide.addStepPhase2_BuildDictionary(result); // Phase 3: Build Cube addLayerCubingSteps(result, jobId, cuboidRootPath); // layer cubing, only selected algorithm will execute addInMemCubingSteps(result, jobId, cuboidRootPath); // inmem cubing, only selected algorithm will execute outputSide.addStepPhase3_BuildCube(result); // Phase 4: Update Metadata &amp; Cleanup result.addTask(createUpdateCubeInfoAfterBuildStep(jobId)); inputSide.addStepPhase4_Cleanup(result); outputSide.addStepPhase4_Cleanup(result); return result; &#125; Phase 1: Create Flat Table &amp; Materialize Hive View in Lookup TablesBatchCubingJobBuilder212// Phase 1: Create Flat Table &amp; Materialize Hive View in Lookup TablesinputSide.addStepPhase1_CreateFlatTable(result); HiveMRInput 1234567891011121314151617public void addStepPhase1_CreateFlatTable(DefaultChainedExecutable jobFlow) &#123; final String cubeName = CubingExecutableUtil.getCubeName(jobFlow.getParams()); final KylinConfig cubeConfig = CubeManager.getInstance(KylinConfig.getInstanceFromEnv()).getCube(cubeName) .getConfig(); final String hiveInitStatements = JoinedFlatTable.generateHiveInitStatements(flatTableDatabase); // create flat table first addStepPhase1_DoCreateFlatTable(jobFlow); // then count and redistribute if (cubeConfig.isHiveRedistributeEnabled()) &#123; jobFlow.addTask(createRedistributeFlatHiveTableStep(hiveInitStatements, cubeName)); &#125; // special for hive addStepPhase1_DoMaterializeLookupTable(jobFlow); &#125; 1234567891011121314private AbstractExecutable createFlatHiveTableStep(String hiveInitStatements, String jobWorkingDir, String cubeName) &#123; //from hive to hive final String dropTableHql = JoinedFlatTable.generateDropTableStatement(flatDesc); final String createTableHql = JoinedFlatTable.generateCreateTableStatement(flatDesc, jobWorkingDir); String insertDataHqls = JoinedFlatTable.generateInsertDataStatement(flatDesc); CreateFlatHiveTableStep step = new CreateFlatHiveTableStep(); step.setInitStatement(hiveInitStatements); step.setCreateTableStatement(dropTableHql + createTableHql + insertDataHqls); CubingExecutableUtil.setCubeName(cubeName, step.getParams()); step.setName(ExecutableConstants.STEP_NAME_CREATE_FLAT_HIVE_TABLE); return step; &#125; CreateFlatHiveTableStep 1234567891011121314151617181920212223242526protected void createFlatHiveTable(KylinConfig config) throws IOException &#123; final HiveCmdBuilder hiveCmdBuilder = new HiveCmdBuilder(); hiveCmdBuilder.overwriteHiveProps(config.getHiveConfigOverride()); hiveCmdBuilder.addStatement(getInitStatement()); hiveCmdBuilder.addStatementWithRedistributeBy(getCreateTableStatement()); final String cmd = hiveCmdBuilder.toString(); stepLogger.log(&quot;Create and distribute table, cmd: &quot;); stepLogger.log(cmd); Pair&lt;Integer, String&gt; response = config.getCliCommandExecutor().execute(cmd, stepLogger); Map&lt;String, String&gt; info = stepLogger.getInfo(); //get the flat Hive table size Matcher matcher = HDFS_LOCATION.matcher(cmd); if (matcher.find()) &#123; String hiveFlatTableHdfsUrl = matcher.group(1); long size = getFileSize(hiveFlatTableHdfsUrl); info.put(ExecutableConstants.HDFS_BYTES_WRITTEN, &quot;&quot; + size); logger.info(&quot;HDFS_Bytes_Writen: &quot; + size); &#125; getManager().addJobInfo(getId(), info); if (response.getFirst() != 0) &#123; throw new RuntimeException(&quot;Failed to create flat hive table, error code &quot; + response.getFirst()); &#125; &#125; Phase 2: Build DictionaryPhase 3: Build CubeBatchCubingJobBuilder21234// Phase 3: Build CubeaddLayerCubingSteps(result, jobId, cuboidRootPath); // layer cubing, only selected algorithm will executeaddInMemCubingSteps(result, jobId, cuboidRootPath); // inmem cubing, only selected algorithm will executeoutputSide.addStepPhase3_BuildCube(result); 12345678910protected void addLayerCubingSteps(final CubingJob result, final String jobId, final String cuboidRootPath) &#123; // Don&apos;t know statistics so that tree cuboid scheduler is not determined. Determine the maxLevel at runtime final int maxLevel = CuboidUtil.getLongestDepth(seg.getCuboidScheduler().getAllCuboidIds()); // base cuboid step result.addTask(createBaseCuboidStep(getCuboidOutputPathsByLevel(cuboidRootPath, 0), jobId)); // n dim cuboid steps for (int i = 1; i &lt;= maxLevel; i++) &#123; result.addTask(createNDimensionCuboidStep(getCuboidOutputPathsByLevel(cuboidRootPath, i - 1), getCuboidOutputPathsByLevel(cuboidRootPath, i), i, jobId)); &#125;&#125; 12345678910111213141516171819202122private MapReduceExecutable createBaseCuboidStep(String cuboidOutputPath, String jobId) &#123; // base cuboid job MapReduceExecutable baseCuboidStep = new MapReduceExecutable(); StringBuilder cmd = new StringBuilder(); appendMapReduceParameters(cmd); baseCuboidStep.setName(ExecutableConstants.STEP_NAME_BUILD_BASE_CUBOID); appendExecCmdParameters(cmd, BatchConstants.ARG_CUBE_NAME, seg.getRealization().getName()); appendExecCmdParameters(cmd, BatchConstants.ARG_SEGMENT_ID, seg.getUuid()); appendExecCmdParameters(cmd, BatchConstants.ARG_INPUT, &quot;FLAT_TABLE&quot;); // marks flat table input appendExecCmdParameters(cmd, BatchConstants.ARG_OUTPUT, cuboidOutputPath); appendExecCmdParameters(cmd, BatchConstants.ARG_JOB_NAME, &quot;Kylin_Base_Cuboid_Builder_&quot; + seg.getRealization().getName()); appendExecCmdParameters(cmd, BatchConstants.ARG_LEVEL, &quot;0&quot;); appendExecCmdParameters(cmd, BatchConstants.ARG_CUBING_JOB_ID, jobId); baseCuboidStep.setMapReduceParams(cmd.toString()); baseCuboidStep.setMapReduceJobClass(getBaseCuboidJob()); // baseCuboidStep.setCounterSaveAs(CubingJob.SOURCE_RECORD_COUNT + &quot;,&quot; + CubingJob.SOURCE_SIZE_BYTES); return baseCuboidStep;&#125; 1234567891011121314151617181920private MapReduceExecutable createNDimensionCuboidStep(String parentPath, String outputPath, int level, String jobId) &#123; // ND cuboid job MapReduceExecutable ndCuboidStep = new MapReduceExecutable(); ndCuboidStep.setName(ExecutableConstants.STEP_NAME_BUILD_N_D_CUBOID + &quot; : level &quot; + level); StringBuilder cmd = new StringBuilder(); appendMapReduceParameters(cmd); appendExecCmdParameters(cmd, BatchConstants.ARG_CUBE_NAME, seg.getRealization().getName()); appendExecCmdParameters(cmd, BatchConstants.ARG_SEGMENT_ID, seg.getUuid()); appendExecCmdParameters(cmd, BatchConstants.ARG_INPUT, parentPath); appendExecCmdParameters(cmd, BatchConstants.ARG_OUTPUT, outputPath); appendExecCmdParameters(cmd, BatchConstants.ARG_JOB_NAME, &quot;Kylin_ND-Cuboid_Builder_&quot; + seg.getRealization().getName() + &quot;_Step&quot;); appendExecCmdParameters(cmd, BatchConstants.ARG_LEVEL, &quot;&quot; + level); appendExecCmdParameters(cmd, BatchConstants.ARG_CUBING_JOB_ID, jobId); ndCuboidStep.setMapReduceParams(cmd.toString()); ndCuboidStep.setMapReduceJobClass(getNDCuboidJob()); return ndCuboidStep;&#125; MR Cube Build 从Hive表生成Base Cuboid 在实际的cube构建过程中，会首先根据cube的Hive事实表和维表生成一张大宽表，然后计算大宽表列的基数，建立维度字典，估算cuboid的大小，建立cube对应的HBase表，再计算base cuboid。计算base cuboid就是一个MapReduce作业，其输入是上面提到的Hive大宽表，输出是的key是各种维度组合，value是Hive大宽表中指标的值。 BaseCuboidJobmapper: HiveToBaseCuboidMapper1234567891011public void doMap(KEYIN key, Object value, Context context) throws IOException, InterruptedException &#123; Collection&lt;String[]&gt; rowCollection = flatTableInputFormat.parseMapperInput(value); for (String[] row: rowCollection) &#123; try &#123; outputKV(row, context); &#125; catch (Exception ex) &#123; handleErrorRecord(row, ex); &#125; &#125;&#125; HiveTableReader12345678public static String[] getRowAsStringArray(HCatRecord record) &#123; String[] arr = new String[record.size()]; for (int i = 0; i &lt; arr.length; i++) &#123; Object o = record.get(i); arr[i] = (o == null) ? null : o.toString(); &#125; return arr;&#125; BaseCuboidMapperBase12345678protected void outputKV(String[] flatRow, Context context) throws IOException, InterruptedException &#123; byte[] rowKey = baseCuboidBuilder.buildKey(flatRow); outputKey.set(rowKey, 0, rowKey.length); ByteBuffer valueBuf = baseCuboidBuilder.buildValue(flatRow); outputValue.set(valueBuf.array(), 0, valueBuf.position()); context.write(outputKey, outputValue);&#125; 从Base Cuboid 逐层计算 Cuboid。 从base cuboid 逐层计算每层的cuboid，也是MapReduce作业，map阶段每层维度数依次减少，reduce阶段对指标进行聚合。 reducer: CuboidReducer 1234567891011121314151617public void doReduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; aggs.reset(); for (Text value : values) &#123; if (vcounter++ % BatchConstants.NORMAL_RECORD_LOG_THRESHOLD == 0) &#123; logger.info(&quot;Handling value with ordinal (This is not KV number!): &quot; + vcounter); &#125; codec.decode(ByteBuffer.wrap(value.getBytes(), 0, value.getLength()), input); aggs.aggregate(input, needAggrMeasures); &#125; aggs.collectStates(result); ByteBuffer valueBuf = codec.encode(result); outputValue.set(valueBuf.array(), 0, valueBuf.position()); context.write(key, outputValue);&#125; Cuboid 转化为HBase的HFile Spark Cube Build The “by-layer” Cubing divides a big task into a couple steps, and each step bases on the previous step’s output, so it can reuse the previous calculation and also avoid calculating from very beginning when there is a failure in between. These makes it as a reliable algorithm. When moving to Spark, we decide to keep this algorithm, that’s why we call this feature as “By layer Spark Cubing”. 分层构建Cube可以将一个大任务分成若干步，而且每一步都可以基于上一步的输出进行计算， Figure 3 is the DAG of Cubing in Spark, it illustrates the process in detail: In “Stage 5”, Kylin uses a HiveContext to read the intermediate Hive table, and then do a “map” operation, which is an one to one map, to encode the origin values into K-V bytes. On complete Kylin gets an intermediate encoded RDD. In “Stage 6”, the intermediate RDD is aggregated with a “reduceByKey” operation to get RDD-1, which is the base cuboid. Nextly, do an “flatMap” (one to many map) on RDD-1, because the base cuboid has N children cuboids. And so on, all levels’ RDDs get calculated. These RDDs will be persisted to distributed file system on complete, but be cached in memory for next level’s calculation. When child be generated, it will be removed from cache. As we know, RDD (Resilient Distributed Dataset) is a basic concept in Spark. A collection of N-Dimension cuboids can be well described as an RDD, a N-Dimension Cube will have N+1 RDD. These RDDs have the parent/child relationship as the parent can be used to generate the children. With the parent RDD cached in memory, the child RDD’s generation can be much efficient than reading from disk. Figure 2 describes this process. Phase 4: Update Metadata &amp; Cleanup其他代码","categories":[],"tags":[]},{"title":"Speeding ETL Processing in Data Warehouses Using High-Performance Joins for Changed Data Capture 论文学习","slug":"Speeding-ETL-Processing-in-Data-Warehouses-Using-High-Performance-Joins-for-Changed-Data-Capture","date":"2018-04-20T12:43:03.000Z","updated":"2018-04-20T12:45:21.783Z","comments":true,"path":"2018/04/20/Speeding-ETL-Processing-in-Data-Warehouses-Using-High-Performance-Joins-for-Changed-Data-Capture/","link":"","permalink":"http://yoursite.com/2018/04/20/Speeding-ETL-Processing-in-Data-Warehouses-Using-High-Performance-Joins-for-Changed-Data-Capture/","excerpt":"","text":"1.Join and 2.Aggregation – which will play an integral role during preprocessing as well in manipulating and consolidating data in a data warehouse. 翻译： 1.加入和2.聚合 - 在预处理过程中以及在数据仓库中操纵和合并数据时，它们将扮演不可或缺的角色。 ETL systems move data from OLTP systems to a data warehouse, but they can also be used to move data from one data warehouse to another. A heterogeneous architecture for an ETL system is one that extracts data from multiple sources. The complexity of this architecture arises from the fact that data from more than one source must be merged, rather than from the fact that data may be formatted differently in the different sources. 翻译： ETL系统将数据从OLTP系统移动到数据仓库，但它们也可用于将数据从一个数据仓库移动到另一个数据仓库。 ETL系统的异构体系结构是从多个来源提取数据的体系结构。这种体系结构的复杂性源于以下事实：来自多个源的数据必须合并，而不是来自不同来源的数据格式不同的事实。 Rather than replacing the information in the data warehouse with the data in the entire online transactional database, a join will match the primary key of the previously loaded record with its corresponding new record and then compare the data portions of the two records to determine if they’ve changed. In this way, only added, deleted, and altered records are updated, which significantly reduces elapsed time of database loads. By using a high-performance join for CDC, data warehouse updates can be performed with far greater efficiency. 翻译： 与其将数据仓库中的信息替换为整个在线交易数据库中的数据，联合会将先前加载的记录的主键与其相应的新记录相匹配，然后比较两个记录的数据部分以确定它们 已经改变了。 通过这种方式，只有添加，删除和更改的记录才会更新，这会显着减少数据库加载所花费的时间。 通过为CDC使用高性能连接，数据仓库更新可以以更高的效率执行。","categories":[],"tags":[]},{"title":"Implementation of Change Data Capture in ETL Process for Data Warehouse Using HDFS and Apache Spark 论文学习","slug":"Implementation-of-Change-Data-Capture-in-ETL-Process-for-Data-Warehouse-Using-HDFS-and-Apache-Spark-论文学习","date":"2018-04-18T13:55:51.000Z","updated":"2018-04-18T13:57:30.490Z","comments":true,"path":"2018/04/18/Implementation-of-Change-Data-Capture-in-ETL-Process-for-Data-Warehouse-Using-HDFS-and-Apache-Spark-论文学习/","link":"","permalink":"http://yoursite.com/2018/04/18/Implementation-of-Change-Data-Capture-in-ETL-Process-for-Data-Warehouse-Using-HDFS-and-Apache-Spark-论文学习/","excerpt":"","text":"没什么干货，参考文献中的几篇有价值 可以参考这篇文章关于etl基本术语的介绍 snapshot difference技术 快照差分 用到的技术 The whole ETL process are performed using Pentaho Data Integration (PDI). Meanwhile, the proposed ETL process extract new and changed data using map-reduce framework and Apache Spoon. The transformation and loading process are performed using PDI. This section elaborates our big data cluster environment and the implementation of CDC. 翻译： 整个ETL过程使用Pentaho数据集成（PDI）执行。 同时，提议的ETL过程使用map-reduce框架和Apache Spoon提取新的和更改的数据。 转换和加载过程使用PDI执行。 本部分阐述了我们的大数据集群环境和CDC的实施。 mr的应用 The CDC method can be implemented using MapReduce by adopting the divide and conquer principle similar to that conducted in the study by [9]. The data are divided into several parts, each to be processed separately. Then, each data processed will enter the reduce phase which will detect changes in the data. 翻译： CDC方法可以通过采用类似于[9]研究中进行的分而治之原则，使用MapReduce来实现。 数据分成几个部分，每个部分分别处理。 然后，每个处理的数据将进入减少阶段，这将检测数据的变化。 sprak sql 的应用 In this study, the method used was by comparing data and finding differences between two data. The CDC method using snapshot difference was divided into several stages as illustrated in Figure 8. The first process was to take the data from the source. The data were taken by full load, which took the entire data from the beginning to the final condition. The results of the extraction was entered into the HDFS. The data was processed through the program created to run the CDC process using the snapshot difference technique. Snapshot difference was implemented using outer-join function from SparkSQL. The program was run by looking at the null value of the outer-join results of the two records, which will be the parameter for the newest data. If in the process damage to the data source was detected, the program could automatically change the data reference to compare and store the old data as a snapshot. 翻译： 在这项研究中，所使用的方法是通过比较数据和发现两个数据之间的差异。 使用快照差异的CDC方法分为几个阶段，如图8所示。第一个过程是从源头获取数据。 数据是以满负荷的方式获取的，这些数据从开始到最终的状态都采用了整个数据。 提取结果被输入到HDFS中。 数据通过创建的程序进行处理，以使用快照差异技术运行CDC进程。 快照差异是使用SparkSQL的外连接函数实现的。 该程序通过查看两条记录的外连接结果的空值来运行，这将成为最新数据的参数。 如果在检测到数据源损坏的过程中，程序可以自动更改数据引用以比较旧数据并将其存储为快照。","categories":[],"tags":[]},{"title":"Hive环境搭建","slug":"Hive环境搭建","date":"2018-04-17T16:10:04.000Z","updated":"2018-04-25T02:54:04.517Z","comments":true,"path":"2018/04/18/Hive环境搭建/","link":"","permalink":"http://yoursite.com/2018/04/18/Hive环境搭建/","excerpt":"","text":"Mysql安装关闭selinux 服务 vim /etc/selinux/config···SELINUX=disabled··· 卸载MariaDB 查看当前安装的mariadb包： rpm -qa | grep mariadb强制卸载： rpm -e –nodeps mariadb-libs-5.5.44-2.el7.centos.x86_64 查看是否已经安装了MySQL rpm -qa | grep -i mysqlfind / -name mysql 删除分散mysql文件 find / -name mysql / # whereis mysql 删除配置文档 rm -rf /etc/my.cnf 再次查找机器是否安装mysql rpm -qa|grep -i mysql 下载mysql5.6的安装包，并上传到服务器上 [root@nodeh1 mysql]# lltotal 236180-rw-r–r– 1 root root 20278972 Sep 22 15:41 MySQL-client-5.6.31-1.el7.x86_64.rpm-rw-r–r– 1 root root 3529244 Sep 22 15:40 MySQL-devel-5.6.31-1.el7.x86_64.rpm-rw-r–r– 1 root root 61732192 Sep 22 15:42 MySQL-server-5.6.31-1.el7.x86_64.rpm-rw-r–r– 1 root root 2101912 Sep 22 15:42 MySQL-shared-5.6.31-1.el7.x86_64.rpm-rw-r–r– 1 root root 2299648 Sep 22 15:40 MySQL-shared-compat-5.6.31-1.el7.x86_64.rpm-rw-r–r– 1 root root 59644132 Sep 22 15:40 MySQL-test-5.6.31-1.el7.x86_64.rpm 安装mysql 的安装包 注意顺序 rpm -ivh MySQL-common-5.6.31-1.el7.x86_64.rpmrpm -ivh MySQL-libs-5.6.31-1.el7.x86_64.rpmrpm -ivh MySQL-client-5.6.31-1.el7.x86_64.rpmrpm -ivh MySQL-server-5.6.31-1.el7.x86_64.rpmrpm -ivh MySQL-devel-5.6.31-1.el7.x86_64.rpm 使用rpm安装方式安装mysql，安装的路径如下： a 数据库目录/var/lib/mysql/b 配置文件/usr/share/mysql(mysql.server命令及配置文件)c 相关命令/usr/bin(mysqladmin mysqldump等命令)d 启动脚本/etc/rc.d/init.d/(启动脚本文件mysql的目录)e /etc/my.conf 修改字符集和数据存储路径 配置/etc/my.cnf文件,设置如下键值来启用一起有用的选项和 UTF-8 字符集. cat /etc/my.cnf[mysqld]···innodb_file_per_tablemax_connections = 4096collation-server = utf8_general_cicharacter-set-server = utf8 如果无法启动mysql服务 chown mysql:mysql -R /var/lib/mysql 如果遇到Access denied for user ‘root‘@’localhost’ (using password: YES) 打开MySQL目录下的my.ini文件，在文件的最后添加一行“skip-grant-tables”，保存并关闭文件，重启服务。 通过cmd行进入MySQL的bin目录，输入“mysql -u root -p”(不输入密码)，回车即可进入数据库; 执行mysql&gt; use mysql;，使用mysql数据库; mysql&gt; update mysql.user set authentication_string=password(&quot;root&quot;) where user=&quot;root&quot; and Host = &quot;localhost&quot;; 打开MySQL目录下的my.ini文件，删除最后一行的“skip-grant-tables”，保存并关闭文件 mysql&gt; flush privileges; mysql&gt; quit; 初始化MySQL及设置密码 mysqld –initialize会生成一个 root 账户密码，密码在log文件里cat /var/log/mysqld.log2017-04-13T10:00:37.229524Z 1 [Note] A temporary password is generated for root@localhost: %kWTz,Ml?3Zs systemctl start mysqld.service ALTER USER ‘root‘@’localhost’ IDENTIFIED BY ‘new_password’; 设置mysql开机启动 systemctl restart mysqld.servicesystemctl enable mysqld.service 设置mysql允许远程登陆 123456789mysql&gt; use mysql;mysql&gt; select host,user,password from user;| host | user | password || localhost | root | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 || localhost.localdomain | root | *1237E2CE819C427B0D8174456DD83C47480D37E8 || 127.0.0.1 | root | *1237E2CE819C427B0D8174456DD83C47480D37E8 || ::1 | root | *1237E2CE819C427B0D8174456DD83C47480D37E8 |mysql&gt; update user set host=&apos;%&apos; where user=&apos;root&apos; and host=&apos;localhost&apos;;mysql&gt; flush privileges; MySQL is running but PID file could not be found在Linux 中，当你启动或者重启 MySQL 时，报关于 PID file 的错误 第一步：找到 mysql 中 data 目录下的 mysql-bin.index 文件，然后删除12find / -name mysql-bin.indexrm -rf /phpstudy/data/mysql-bin.index 第二步：找到 并 kill 所有关于 mysql 或者 mysqld 的进程 12ps -aux | grep mysqlkill 进程号 可以在查看下进程中是否有 mysql 进程，确保 kill 干净，再看看 还有没有 mysql-bin.index文件（进程没杀死之前可能会生成）&lt;有必要&gt; 命令： service mysqld status mysqld：未被识别的服务遇到这样的错误，是由于 /etc/init.d/ 不存在 mysqld 这个命令（有的人安装完环境后存在，是因为你的安装包中有这样的命令将 mysql.server 文件 copy 到 /etc/init.d/ 下面了） 1.首先你需要找到 mysql.server 文件，这个 和 mysqld 文件是一模一样的，只不过文件名不相同，执行命令： 1find / -name mysql.server 2.copy mysql.server 文件到 /etc/init.d/ 目录下，重命名文件为 mysqld，执行命令： 1cp /phpstudy/mysql/support-files/mysql.server /etc/init.d/mysqld 然后 再 service mysqld status 这个问题就解决了。 mysqld启动错误123456789101112131415161718Redirecting to /bin/systemctl start mysqld.serviceJob for mysqld.service failed. See &apos;systemctl status mysqld.service&apos; and &apos;journalctl -xn&apos; for details.service mysqld statusRedirecting to /bin/systemctl status mysqld.servicemysqld.service - MySQL Server Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled) Active: failed (Result: start-limit) since Tue 2015-12-08 13:57:22 CST; 17s ago Process: 31004 ExecStart=/usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid $MYSQLD_OPTS (code=exited, status=1/FAILURE) Process: 30988 ExecStartPre=/usr/bin/mysqld_pre_systemd (code=exited, status=0/SUCCESS)Dec 08 13:57:22 iZ25lox0jlhZ systemd[1]: Failed to start MySQL Server.Dec 08 13:57:22 iZ25lox0jlhZ systemd[1]: Unit mysqld.service entered failed state.Dec 08 13:57:22 iZ25lox0jlhZ systemd[1]: mysqld.service holdoff time over, scheduling restart.Dec 08 13:57:22 iZ25lox0jlhZ systemd[1]: Stopping MySQL Server...Dec 08 13:57:22 iZ25lox0jlhZ systemd[1]: Starting MySQL Server...Dec 08 13:57:22 iZ25lox0jlhZ systemd[1]: mysqld.service start request repeated too quickly, refusing to start.Dec 08 13:57:22 iZ25lox0jlhZ systemd[1]: Failed to start MySQL Server.Dec 08 13:57:22 iZ25lox0jlhZ systemd[1]: Unit mysqld.service entered failed state. 安装完应该先检查一下/var/lib/mysql目录下的文件权限，执行 chown mysql:mysql -R /var/lib/mysql 然后重新启动mysql服务 service mysqld start Hive配置过程 hive-site.xml cp $HIVE_HOME/conf/hive-default.xml.template $HIVE_HOME/conf/hive-site.xml 123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://127.0.0.1:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;new_password&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; iotmp 建立临时目录mkdir /home/grid/hive/iotmp关联找到hive-site.xml里面，将iotmp的对应配置修改成真实地址。${system:java.io.tmpdir} 改为真实物理路径。 Hive启动过程 nohup hive –service metastore -v &amp;","categories":[],"tags":[]},{"title":"Spark Java API","slug":"Spark-Java-API","date":"2018-04-17T12:04:49.000Z","updated":"2018-05-05T09:35:12.016Z","comments":true,"path":"2018/04/17/Spark-Java-API/","link":"","permalink":"http://yoursite.com/2018/04/17/Spark-Java-API/","excerpt":"","text":"参考文献：http://lxw1234.comRDD如何创建 首先创建JavaSparkContext对象实例sc 1JavaSparkContext sc = new JavaSparkContext(&quot;local&quot;,&quot;SparkTest&quot;); 接受2个参数：第一个参数表示运行方式（local、yarn-client、yarn-standalone等）第二个参数表示应用名字 直接从集合转化 sc.parallelize(List(1,2,3,4,5,6,7,8,9,10))从HDFS文件转化 sc.textFile(&quot;hdfs://&quot;)从本地文件转化 sc.textFile(&quot;file:/&quot;) 下面例子中list2就是根据data2List生成的一个RDD 1234List&lt;String&gt; list1 = new ArrayList&lt;String&gt;();list1.add(&quot;11,12,13,14,15&quot;);list1.add(&quot;aa,bb,cc,dd,ee&quot;);JavaRDD&lt;String&gt; list2 = sc.parallelize(list1); 常用算子 基本算子filter举例，在F:\\sparktest\\sample.txt 文件的内容如下 aa bb cc aa aa aa dd dd ee ee ee eeff aa bb zksee kksee zz zks我要将包含zks的行的内容给找出来scala版本 123456val lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;).filter(line=&gt;line.contains(&quot;zks&quot;))//打印内容lines.collect().foreach(println(_));-------------输出------------------ff aa bb zksee zz zks java版本 123456789101112131415JavaRDD&lt;String&gt; lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;);JavaRDD&lt;String&gt; zksRDD = lines.filter(new Function&lt;String, Boolean&gt;() &#123; @Override public Boolean call(String s) throws Exception &#123; return s.contains(&quot;zks&quot;); &#125;&#125;);//打印内容List&lt;String&gt; zksCollect = zksRDD.collect();for (String str:zksCollect) &#123; System.out.println(str);&#125;----------------输出-------------------ff aa bb zksee zz zks mapmap() 接收一个函数，把这个函数用于 RDD 中的每个元素，将函数的返回结果作为结果RDD编程 ｜ 31RDD 中对应元素的值 map是一对一的关系举例，在F:\\sparktest\\sample.txt 文件的内容如下 aa bb cc aa aa aa dd dd ee ee ee eeff aa bb zksee kksee zz zks 把每一行变成一个数组scala版本 1234567891011//读取数据scala&gt; val lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;)//用map，对于每一行数据，按照空格分割成一个一个数组，然后返回的是一对一的关系scala&gt; var mapRDD = lines.map(line =&gt; line.split(&quot;\\\\s+&quot;))---------------输出-----------res0: Array[Array[String]] = Array(Array(aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee), Array(ff, aa, bb, zks), Array(ee, kks), Array(ee, zz, zks))//读取第一个元素scala&gt; mapRDD.first---输出----res1: Array[String] = Array(aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee) java版本 1234567891011JavaRDD&lt;Iterable&lt;String&gt;&gt; mapRDD = lines.map(new Function&lt;String, Iterable&lt;String&gt;&gt;() &#123; @Override public Iterable&lt;String&gt; call(String s) throws Exception &#123; String[] split = s.split(&quot;\\\\s+&quot;); return Arrays.asList(split); &#125;&#125;);//读取第一个元素System.out.println(mapRDD.first());---------------输出-------------[aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee] flatMap有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap()faltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器(想要了解更多，请参考scala的flatMap和map用法)例如我们将数据切分为单词scala版本 12345scala&gt; val lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;)scala&gt; val flatMapRDD = lines.flatMap(line=&gt;line.split(&quot;\\\\s&quot;))scala&gt; flatMapRDD.first()---输出----res0: String = aa java版本，spark2.0以下 123456789101112JavaRDD&lt;String&gt; lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;);JavaRDD&lt;String&gt; flatMapRDD = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterable&lt;String&gt; call(String s) throws Exception &#123; String[] split = s.split(&quot;\\\\s+&quot;); return Arrays.asList(split); &#125;&#125;);//输出第一个System.out.println(flatMapRDD.first());------------输出----------aa java版本，spark2.0以上spark2.0以上，对flatMap的方法有所修改，就是flatMap中的Iterator和Iteratable的小区别 1234567JavaRDD&lt;String&gt; flatMapRDD = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterator&lt;String&gt; call(String s) throws Exception &#123; String[] split = s.split(&quot;\\\\s+&quot;); return Arrays.asList(split).iterator(); &#125;&#125;); distinctdistinct用于去重， 我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销scala版本123456789scala&gt; var RDD1 = sc.parallelize(List(&quot;aa&quot;,&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;,&quot;dd&quot;))scala&gt; RDD1.collectres3: Array[String] = Array(aa, aa, bb, cc, dd)scala&gt; var distinctRDD = RDD1.distinctscala&gt; distinctRDD.collectres5: Array[String] = Array(aa, dd, bb, cc) java版本 12345678JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));JavaRDD&lt;String&gt; distinctRDD = RDD1.distinct();List&lt;String&gt; collect = distinctRDD.collect();for (String str:collect) &#123; System.out.print(str+&quot;, &quot;);&#125;---------输出----------aa, dd, bb, cc, union两个RDD进行合并scala版本1234567891011scala&gt; var RDD1 = sc.parallelize(List(&quot;aa&quot;,&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;,&quot;dd&quot;))scala&gt; var RDD2 = sc.parallelize(List(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;))scala&gt; RDD1.collectres6: Array[String] = Array(aa, aa, bb, cc, dd)scala&gt; RDD2.collectres7: Array[String] = Array(aa, dd, ff)scala&gt; RDD1.union(RDD2).collectres8: Array[String] = Array(aa, aa, bb, cc, dd, aa, dd, ff) java版本123456789JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));JavaRDD&lt;String&gt; unionRDD = RDD1.union(RDD2);List&lt;String&gt; collect = unionRDD.collect();for (String str:collect) &#123; System.out.print(str+&quot;, &quot;);&#125;-----------输出---------aa, aa, bb, cc, dd, aa, dd, ff, intersectionRDD1.intersection(RDD2) 返回两个RDD的交集，并且去重intersection 需要混洗数据，比较浪费性能scala版本12345678910111213scala&gt; var RDD1 = sc.parallelize(List(&quot;aa&quot;,&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;,&quot;dd&quot;))scala&gt; var RDD2 = sc.parallelize(List(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;))scala&gt; RDD1.collectres6: Array[String] = Array(aa, aa, bb, cc, dd)scala&gt; RDD2.collectres7: Array[String] = Array(aa, dd, ff)scala&gt; var insertsectionRDD = RDD1.intersection(RDD2)scala&gt; insertsectionRDD.collectres9: Array[String] = Array(aa, dd) java版本123456789JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));JavaRDD&lt;String&gt; intersectionRDD = RDD1.intersection(RDD2);List&lt;String&gt; collect = intersectionRDD.collect();for (String str:collect) &#123; System.out.print(str+&quot; &quot;);&#125;-------------输出-----------aa dd subtractRDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重scala版本12345678JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;,&quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));scala&gt; var substractRDD =RDD1.subtract(RDD2)scala&gt; substractRDD.collectres10: Array[String] = Array(bb, cc) java版本123456789JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;,&quot;cc&quot;, &quot;dd&quot;));JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));JavaRDD&lt;String&gt; subtractRDD = RDD1.subtract(RDD2);List&lt;String&gt; collect = subtractRDD.collect();for (String str:collect) &#123; System.out.print(str+&quot; &quot;);&#125;------------输出-----------------bb cc cartesianRDD1.cartesian(RDD2) 返回RDD1和RDD2的笛卡儿积，这个开销非常大 scala版本12345678scala&gt; var RDD1 = sc.parallelize(List(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;))scala&gt; var RDD2 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;))scala&gt; var cartesianRDD = RDD1.cartesian(RDD2)scala&gt; cartesianRDD.collectres11: Array[(String, String)] = Array((1,a), (1,b), (1,c), (2,a), (2,b), (2,c), (3,a), (3,b), (3,c)) java版本123456789101112131415161718JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;));JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;));JavaPairRDD&lt;String, String&gt; cartesian = RDD1.cartesian(RDD2);List&lt;Tuple2&lt;String, String&gt;&gt; collect1 = cartesian.collect();for (Tuple2&lt;String, String&gt; tp:collect1) &#123; System.out.println(&quot;(&quot;+tp._1+&quot; &quot;+tp._2+&quot;)&quot;);&#125;------------输出-----------------(1 a)(1 b)(1 c)(2 a)(2 b)(2 c)(3 a)(3 b)(3 c) xxxxxmapValuesdef mapValuesU =&gt; U): RDD[(K, U)] 同基本转换操作中的map，只不过mapValues是针对[K,V]中的V值进行map操作。 12345scala&gt; var rdd1 = sc.makeRDD(Array((1,&quot;A&quot;),(2,&quot;B&quot;),(3,&quot;C&quot;),(4,&quot;D&quot;)),2)rdd1: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[27] at makeRDD at :21scala&gt; rdd1.mapValues(x =&gt; x + &quot;_&quot;).collectres26: Array[(Int, String)] = Array((1,A_), (2,B_), (3,C_), (4,D_)) flatMapValuesdef flatMapValuesU =&gt; TraversableOnce[U]): RDD[(K, U)] 同基本转换操作中的flatMap，只不过flatMapValues是针对[K,V]中的V值进行flatMap操作。 12scala&gt; rdd1.flatMapValues(x =&gt; x + &quot;_&quot;).collectres36: Array[(Int, Char)] = Array((1,A), (1,_), (2,B), (2,_), (3,C), (3,_), (4,D), (4,_)) 键值对操作mapToPair举例，在F:\\sparktest\\sample.txt 文件的内容如下 aa bb cc aa aa aa dd dd ee ee ee eeff aa bb zksee kksee zz zks 将每一行的第一个单词作为键，1 作为value创建pairRDDscala版本scala是没有mapToPair函数的，scala版本只需要map就可以了 123456scala&gt; val lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;)scala&gt; val pairs = lines.map(x =&gt; (x.split(&quot;\\\\s+&quot;)(0), 1))scala&gt; pairs.collectres0: Array[(String, Int)] = Array((aa,1), (ff,1), (ee,1), (ee,1)) java版本 12345678JavaRDD&lt;String&gt; lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;);//输入的是一个string的字符串，输出的是一个(String, Integer) 的mapJavaPairRDD&lt;String, Integer&gt; pairRDD = lines.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(String s) throws Exception &#123; return new Tuple2&lt;String, Integer&gt;(s.split(&quot;\\\\s+&quot;)[0], 1); &#125;&#125;); flatMapToPair类似于xxx连接 mapToPair是一对一，一个元素返回一个元素，而flatMapToPair可以一个元素返回多个，相当于先flatMap,在mapToPair例子: 将每一个单词都分成键为scala版本123456val lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;)val flatRDD = lines.flatMap(x =&gt; (x.split(&quot;\\\\s+&quot;)))val pairs = flatRDD.map(x=&gt;(x,1))scala&gt; pairs.collectres1: Array[(String, Int)] = Array((aa,1), (bb,1), (cc,1), (aa,1), (aa,1), (aa,1), (dd,1), (dd,1), (ee,1), (ee,1), (ee,1), (ee,1), (ff,1), (aa,1), (bb,1), (zks,1), (ee,1), (kks,1), (ee,1), (zz,1), (zks,1)) java版本 spark2.0以下 123456789101112JavaPairRDD&lt;String, Integer&gt; wordPairRDD = lines.flatMapToPair(new PairFlatMapFunction&lt;String, String, Integer&gt;() &#123; @Override public Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; call(String s) throws Exception &#123; ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; tpLists = new ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;(); String[] split = s.split(&quot;\\\\s+&quot;); for (int i = 0; i &lt;split.length ; i++) &#123; Tuple2 tp = new Tuple2&lt;String,Integer&gt;(split[i], 1); tpLists.add(tp); &#125; return tpLists; &#125;&#125;); java版本 spark2.0以上主要是iterator和iteratable的一些区别 123456789101112JavaPairRDD&lt;String, Integer&gt; wordPairRDD = lines.flatMapToPair(new PairFlatMapFunction&lt;String, String, Integer&gt;() &#123; @Override public Iterator&lt;Tuple2&lt;String, Integer&gt;&gt; call(String s) throws Exception &#123; ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; tpLists = new ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;(); String[] split = s.split(&quot;\\\\s+&quot;); for (int i = 0; i &lt;split.length ; i++) &#123; Tuple2 tp = new Tuple2&lt;String,Integer&gt;(split[i], 1); tpLists.add(tp); &#125; return tpLists.iterator(); &#125;&#125;); reduceByKey12345def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)]def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)] 接收一个函数，按照相同的key进行reduce操作，类似于scala的reduce的操作例如RDD {(1, 2), (3, 4), (3, 6)}进行reduce scala版本123456var mapRDD = sc.parallelize(List((1,2),(3,4),(3,6)))var reduceRDD = mapRDD.reduceByKey((x,y)=&gt;x+y)reduceRDD.foreach(x=&gt;println(x))------输出---------(1,2)(3,10) 再举例 单词计数 F:\\sparktest\\sample.txt中的内容如下aa bb cc aa aa aa dd dd ee ee ee eeff aa bb zksee kksee zz zks scala版本 1234567891011121314 val lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;) val wordsRDD = lines.flatMap(x=&gt;x.split(&quot;\\\\s+&quot;)).map(x=&gt;(x,1)) val wordCountRDD = wordsRDD.reduceByKey((x,y)=&gt;x+y) wordCountRDD.foreach(x=&gt;println(x))---------输出-----------(ee,6)(aa,5)(dd,2)(zz,1)(zks,2)(kks,1)(ff,1)(bb,2)(cc,1) java版本 1234567891011121314151617181920212223242526272829303132333435JavaRDD&lt;String&gt; lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;);JavaPairRDD&lt;String, Integer&gt; wordPairRDD = lines.flatMapToPair(new PairFlatMapFunction&lt;String, String, Integer&gt;() &#123; @Override public Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; call(String s) throws Exception &#123; ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; tpLists = new ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;(); String[] split = s.split(&quot;\\\\s+&quot;); for (int i = 0; i &lt;split.length ; i++) &#123; Tuple2 tp = new Tuple2&lt;String,Integer&gt;(split[i], 1); tpLists.add(tp); &#125; return tpLists; &#125;&#125;);JavaPairRDD&lt;String, Integer&gt; wordCountRDD = wordPairRDD.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer i1, Integer i2) throws Exception &#123; return i1 + i2; &#125;&#125;);Map&lt;String, Integer&gt; collectAsMap = wordCountRDD.collectAsMap();for (String key:collectAsMap.keySet()) &#123; System.out.println(&quot;(&quot;+key+&quot;,&quot;+collectAsMap.get(key)+&quot;)&quot;);&#125;----------输出-------------------------------(kks,1)(ee,6)(bb,2)(zz,1)(ff,1)(cc,1)(zks,2)(dd,2)(aa,5) foldByKey12345def foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]def foldByKey(zeroValue: V, numPartitions: Int)(func: (V, V) =&gt; V): RDD[(K, V)]def foldByKey(zeroValue: V, partitioner: Partitioner)(func: (V, V) =&gt; V): RDD[(K, V)] 该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V.foldByKey可以参考我之前的scala的fold的介绍与reduce不同的是 foldByKey开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素参考LXW的博客 scala的例子直接看例子123456scala&gt; var rdd1 = sc.makeRDD(Array((&quot;A&quot;,0),(&quot;A&quot;,2),(&quot;B&quot;,1),(&quot;B&quot;,2),(&quot;C&quot;,1)))scala&gt; rdd1.foldByKey(0)(_+_).collectres75: Array[(String, Int)] = Array((A,2), (B,3), (C,1))//将rdd1中每个key对应的V进行累加，注意zeroValue=0,需要先初始化V,映射函数为+操//作，比如(&quot;A&quot;,0), (&quot;A&quot;,2)，先将zeroValue应用于每个V,得到：(&quot;A&quot;,0+0), (&quot;A&quot;,2+0)，即：//(&quot;A&quot;,0), (&quot;A&quot;,2)，再将映射函数应用于初始化后的V，最后得到(A,0+2),即(A,2) 再看：1234scala&gt; rdd1.foldByKey(2)(_+_).collectres76: Array[(String, Int)] = Array((A,6), (B,7), (C,3))//先将zeroValue=2应用于每个V,得到：(&quot;A&quot;,0+2), (&quot;A&quot;,2+2)，即：(&quot;A&quot;,2), (&quot;A&quot;,4)，再将映射函//数应用于初始化后的V，最后得到：(A,2+4)，即：(A,6) 再看乘法操作：123456789scala&gt; rdd1.foldByKey(0)(_*_).collectres77: Array[(String, Int)] = Array((A,0), (B,0), (C,0))//先将zeroValue=0应用于每个V,注意，这次映射函数为乘法，得到：(&quot;A&quot;,0*0), (&quot;A&quot;,2*0)，//即：(&quot;A&quot;,0), (&quot;A&quot;,0)，再将映射函//数应用于初始化后的V，最后得到：(A,0*0)，即：(A,0)//其他K也一样，最终都得到了V=0scala&gt; rdd1.foldByKey(1)(_*_).collectres78: Array[(String, Int)] = Array((A,0), (B,2), (C,1))//映射函数为乘法时，需要将zeroValue设为1，才能得到我们想要的结果。 SortByKey1def sortByKey(ascending : scala.Boolean = &#123; /* compiled code */ &#125;, numPartitions : scala.Int = &#123; /* compiled code */ &#125;) : org.apache.spark.rdd.RDD[scala.Tuple2[K, V]] = &#123; /* compiled code */ &#125; SortByKey用于对pairRDD按照key进行排序，第一个参数可以设置true或者false，默认是truescala例子 12345678910111213scala&gt; val rdd = sc.parallelize(Array((3, 4),(1, 2),(4,4),(2,5), (6,5), (5, 6))) // sortByKey不是Action操作，只能算是转换操作scala&gt; rdd.sortByKey()res9: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[28] at sortByKey at &lt;console&gt;:24//看看sortByKey后是什么类型scala&gt; rdd.sortByKey().collect()res10: Array[(Int, Int)] = Array((1,2), (2,5), (3,4), (4,4), (5,6), (6,5))//降序排序scala&gt; rdd.sortByKey(false).collect()res12: Array[(Int, Int)] = Array((6,5), (5,6), (4,4), (3,4), (2,5), (1,2)) 存储相关算子saveAsTextFile123def saveAsTextFile(path: String): Unitdef saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec]): Unit saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。 codec参数可以指定压缩的类名。12345678var rdd1 = sc.makeRDD(1 to 10,2)scala&gt; rdd1.saveAsTextFile(&quot;hdfs://cdh5/tmp/test/&quot;) //保存到HDFShadoop fs -ls /tmp/testFound 2 items-rw-r--r-- 2 bob supergroup 0 2015-07-10 09:15 /tmp/test/_SUCCESS-rw-r--r-- 2 bob supergroup 21 2015-07-10 09:15 /tmp/test/part-00000hadoop fs -cat /tmp/test/part-00000 注意：如果使用rdd1.saveAsTextFile(“file:///tmp/test”)将文件保存到本地文件系统，那么只会保存在Executor所在机器的本地目录。指定压缩格式保存1234567rdd1.saveAsTextFile(&quot;hdfs://cdh5/tmp/test/&quot;,classOf[com.hadoop.compression.lzo.LzopCodec])hadoop fs -ls /tmp/test-rw-r--r-- 2 bob supergroup 0 2015-07-10 09:20 /tmp/test/_SUCCESS-rw-r--r-- 2 bob supergroup 71 2015-07-10 09:20 /tmp/test/part-00000.lzohadoop fs -text /tmp/test/part-00000.lzo saveAsSequenceFilesaveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。 用法同saveAsTextFile。 saveAsObjectFile1234567891011def saveAsObjectFile(path: String): UnitsaveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。对于HDFS，默认采用SequenceFile保存。var rdd1 = sc.makeRDD(1 to 10,2)rdd1.saveAsObjectFile(&quot;hdfs://cdh5/tmp/test/&quot;)hadoop fs -cat /tmp/test/part-00000SEQ !org.apache.hadoop.io.NullWritable&quot;org.apache.hadoop.io.BytesWritableT saveAsHadoopFile12345def saveAsHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[_ &lt;: OutputFormat[, ]], codec: Class[_ &lt;: CompressionCodec]): Unitdef saveAsHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[_ &lt;: OutputFormat[, ]], conf: JobConf = …, codec: Option[Class[_ &lt;: CompressionCodec]] = None): UnitsaveAsHadoopFile是将RDD存储在HDFS上的文件中，支持老版本Hadoop API。 可以指定outputKeyClass、outputValueClass以及压缩格式。 每个分区输出一个文件。12345678910var rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;A&quot;,1),(&quot;B&quot;,6),(&quot;B&quot;,3),(&quot;B&quot;,7)))import org.apache.hadoop.mapred.TextOutputFormatimport org.apache.hadoop.io.Textimport org.apache.hadoop.io.IntWritablerdd1.saveAsHadoopFile(&quot;/tmp/test/&quot;,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]])rdd1.saveAsHadoopFile(&quot;/tmp/test/&quot;,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]], classOf[com.hadoop.compression.lzo.LzopCodec]) saveAsHadoopDatasetdef saveAsHadoopDataset(conf: JobConf): Unit saveAsHadoopDataset用于将RDD保存到除了HDFS的其他存储中，比如HBase。 在JobConf中，通常需要关注或者设置五个参数： 文件的保存路径、key值的class类型、value值的class类型、RDD的输出格式(OutputFormat)、以及压缩相关的参数。使用saveAsHadoopDataset将RDD保存到HDFS中1234567891011121314151617181920212223242526import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport SparkContext._import org.apache.hadoop.mapred.TextOutputFormatimport org.apache.hadoop.io.Textimport org.apache.hadoop.io.IntWritableimport org.apache.hadoop.mapred.JobConfvar rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;A&quot;,1),(&quot;B&quot;,6),(&quot;B&quot;,3),(&quot;B&quot;,7)))var jobConf = new JobConf()jobConf.setOutputFormat(classOf[TextOutputFormat[Text,IntWritable]])jobConf.setOutputKeyClass(classOf[Text])jobConf.setOutputValueClass(classOf[IntWritable])jobConf.set(&quot;mapred.output.dir&quot;,&quot;/tmp/bob/&quot;)rdd1.saveAsHadoopDataset(jobConf)结果：hadoop fs -cat /tmp/bob/part-00000A 2A 1hadoop fs -cat /tmp/bob/part-00001B 6B 3B 7 保存数据到HBASEHBase建表：1234567891011121314151617181920212223242526272829303132333435363738create ‘bob′,&#123;NAME =&gt; ‘f1′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f2′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f3′,VERSIONS =&gt; 1&#125;import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport SparkContext._import org.apache.hadoop.mapred.TextOutputFormatimport org.apache.hadoop.io.Textimport org.apache.hadoop.io.IntWritableimport org.apache.hadoop.mapred.JobConfimport org.apache.hadoop.hbase.HBaseConfigurationimport org.apache.hadoop.hbase.mapred.TableOutputFormatimport org.apache.hadoop.hbase.client.Putimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.hbase.io.ImmutableBytesWritablevar conf = HBaseConfiguration.create() var jobConf = new JobConf(conf) jobConf.set(&quot;hbase.zookeeper.quorum&quot;,&quot;zkNode1,zkNode2,zkNode3&quot;) jobConf.set(&quot;zookeeper.znode.parent&quot;,&quot;/hbase&quot;) jobConf.set(TableOutputFormat.OUTPUT_TABLE,&quot;bob&quot;) jobConf.setOutputFormat(classOf[TableOutputFormat]) var rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;B&quot;,6),(&quot;C&quot;,7))) rdd1.map(x =&gt; &#123; var put = new Put(Bytes.toBytes(x._1)) put.add(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;c1&quot;), Bytes.toBytes(x._2)) (new ImmutableBytesWritable,put) &#125; ).saveAsHadoopDataset(jobConf)结果：hbase(main):005:0&gt; scan &apos;bob&apos;ROW COLUMN+CELL A column=f1:c1, timestamp=1436504941187, value=\\x00\\x00\\x00\\x02 B column=f1:c1, timestamp=1436504941187, value=\\x00\\x00\\x00\\x06 C column=f1:c1, timestamp=1436504941187, value=\\x00\\x00\\x00\\x07 3 row(s) in 0.0550 seconds 注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。 saveAsNewAPIHadoopFile123def saveAsNewAPIHadoopFile[F &lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unitdef saveAsNewAPIHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[_ &lt;: OutputFormat[, ]], conf: Configuration = self.context.hadoopConfiguration): Unit saveAsNewAPIHadoopFile用于将RDD数据保存到HDFS上，使用新版本Hadoop API。 用法基本同saveAsHadoopFile。123456789import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport SparkContext._import org.apache.hadoop.mapreduce.lib.output.TextOutputFormatimport org.apache.hadoop.io.Textimport org.apache.hadoop.io.IntWritablevar rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;A&quot;,1),(&quot;B&quot;,6),(&quot;B&quot;,3),(&quot;B&quot;,7)))rdd1.saveAsNewAPIHadoopFile(&quot;/tmp/bob/&quot;,classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]]) saveAsNewAPIHadoopDatasetdef saveAsNewAPIHadoopDataset(conf: Configuration): Unit 作用同saveAsHadoopDataset,只不过采用新版本Hadoop API。 以写入HBase为例： HBase建表： create ‘bob′,{NAME =&gt; ‘f1′,VERSIONS =&gt; 1},{NAME =&gt; ‘f2′,VERSIONS =&gt; 1},{NAME =&gt; ‘f3′,VERSIONS =&gt; 1} 完整的Spark应用程序：1234567891011121314151617181920212223242526272829303132333435363738package com.bob.testimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport SparkContext._import org.apache.hadoop.hbase.HBaseConfigurationimport org.apache.hadoop.mapreduce.Jobimport org.apache.hadoop.hbase.mapreduce.TableOutputFormatimport org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.client.Resultimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.hbase.client.Putobject Test &#123; def main(args : Array[String]) &#123; val sparkConf = new SparkConf().setMaster(&quot;spark://test:7077&quot;).setAppName(&quot;test&quot;) val sc = new SparkContext(sparkConf); var rdd1 = sc.makeRDD(Array((&quot;A&quot;,2),(&quot;B&quot;,6),(&quot;C&quot;,7))) sc.hadoopConfiguration.set(&quot;hbase.zookeeper.quorum &quot;,&quot;zkNode1,zkNode2,zkNode3&quot;) sc.hadoopConfiguration.set(&quot;zookeeper.znode.parent&quot;,&quot;/hbase&quot;) sc.hadoopConfiguration.set(TableOutputFormat.OUTPUT_TABLE,&quot;bob&quot;) var job = new Job(sc.hadoopConfiguration) job.setOutputKeyClass(classOf[ImmutableBytesWritable]) job.setOutputValueClass(classOf[Result]) job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]]) rdd1.map( x =&gt; &#123; var put = new Put(Bytes.toBytes(x._1)) put.add(Bytes.toBytes(&quot;f1&quot;), Bytes.toBytes(&quot;c1&quot;), Bytes.toBytes(x._2)) (new ImmutableBytesWritable,put) &#125; ).saveAsNewAPIHadoopDataset(job.getConfiguration) sc.stop() &#125;&#125; 注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。","categories":[],"tags":[]},{"title":"Hbase环境搭建","slug":"Hbase环境搭建","date":"2018-04-17T03:59:11.000Z","updated":"2018-04-17T03:59:23.502Z","comments":true,"path":"2018/04/17/Hbase环境搭建/","link":"","permalink":"http://yoursite.com/2018/04/17/Hbase环境搭建/","excerpt":"","text":"配置过程 hbase-env.sh 1234export JAVA_HOME=/home/grid/jdk1.7.0_75 export HBASE_HOME=/home/grid/hbase export HBASE_LOG_DIR=/tmp/grid/logs export HBASE_MANAGES_ZK=true hbase-site.xml 1234567891011121314151617181920212223242526&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; # 设置 hbase 数据库存放数据的目录 &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; # 打开 hbase 分布模式 &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master&lt;/name&gt; # 指定 hbase 集群主控节点 &lt;value&gt;master:60000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master,slave1,slave2&lt;/value&gt; # 指定 zookeeper 集群节点名 , 因为是由 zookeeper 表决算法决定的 &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; # 指 zookeeper 集群 data 目录 &lt;value&gt;/home/grid/hbase/zookeeper&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; regionservers 12slave1 slave2 启动过程 启动Hadoop 12$HADOOP_HOME/sbin/start-dfs.sh $HADOOP_HOME/sbin/start-yarn.sh 启动Hbase 1$HBASE_HOME/bin/start-hbase.sh 查看hbase启动情况在master上有HQuorumPeer和HMaster进程，在slave1、slave2上有HQuorumPeer和HRegionServer进程 通过浏览器验证：http://nodeh1:16010/master-status 命令行启动 1/home/grid/hbase/bin/hbase shell","categories":[],"tags":[]},{"title":"Hadoop环境搭建","slug":"Hadoop环境搭建","date":"2018-04-17T03:22:11.000Z","updated":"2018-04-17T08:08:33.921Z","comments":true,"path":"2018/04/17/Hadoop环境搭建/","link":"","permalink":"http://yoursite.com/2018/04/17/Hadoop环境搭建/","excerpt":"","text":"配置过程 hadoop-env.sh 12export JAVA_HOME=/opt/java/jdk1.7.0_80export HADOOP_PREFIX=/opt/hadoop-2.6.4 core-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop-2.6.4/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 注意：tmp目录需提前创建 hdfs-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 数据有三个副本 mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-env.sh 增加 JAVA_HOME 配置 1export JAVA_HOME=/opt/java/jdk1.7.0_80 yarn-site.xml 123456789101112&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;nodeh1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; slaves 123masterslave01slave02 master 即作为 NameNode 也作为 DataNode。 启动过程 格式化文件系统，在 master 上执行以下命令： 1hdfs namenode -format 启动 NameNode 和 DateNode，执行 start-dfs.sh，使用 jps 命令查看进程。 输入地址：http://master:50070/ 可以查看 NameNode 信息。 启动 ResourceManager 和 NodeManager，运行 start-yarn.sh, 使用 jps 查看 进程 测试1hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount /data/input /data/output/result","categories":[],"tags":[]},{"title":"Spark环境搭建","slug":"Spark环境搭建","date":"2018-04-16T16:28:46.000Z","updated":"2018-04-17T08:02:25.069Z","comments":true,"path":"2018/04/17/Spark环境搭建/","link":"","permalink":"http://yoursite.com/2018/04/17/Spark环境搭建/","excerpt":"","text":"配置过程进入 Spark 安装目录下的 conf 目录， 拷贝 spark-env.sh.template 到 spark-env.sh。1cp spark-env.sh.template spark-env.sh 编辑 spark-env.sh，在其中添加以下配置信息：12345export SCALA_HOME=/opt/scala-2.11.8export JAVA_HOME=/opt/java/jdk1.7.0_80export SPARK_MASTER_IP=192.168.109.137export SPARK_WORKER_MEMORY=1gexport HADOOP_CONF_DIR=/opt/hadoop-2.6.4/etc/hadoop 将 slaves.template 拷贝到 slaves， 编辑其内容为：123masterslave01slave02 即 master 既是 Master 节点又是 Worker 节点。 启动过程 运行 start-master.sh，启动Master节点，可以看到master上多了一个新进程Master。 运行 start-slaves.sh，启动所有Worker节点，在 master、slave01和slave02上使用jps命令，可以发现都启动了一个Worker进程 访问 http://master:8080 浏览器查看Spark集群信息。 运行 spark-shell，可以进入Spark的shell控制台 访问 http://master:4040 浏览器访问SparkUI， 可以从SparkUI上查看一些 如环境变量、Job、Executor等信息。 测试计算 π 的近似值1$SPARK_HOME/bin/run-example SparkPi 2&gt;&amp;1 | grep &quot;Pi is roughly&quot;","categories":[],"tags":[]},{"title":"集群环境搭建的常用命令","slug":"环境搭建的常用命令","date":"2018-04-16T13:16:30.000Z","updated":"2018-04-21T11:32:12.892Z","comments":true,"path":"2018/04/16/环境搭建的常用命令/","link":"","permalink":"http://yoursite.com/2018/04/16/环境搭建的常用命令/","excerpt":"","text":"关闭防火墙和SELinux 关闭命令： service iptables stop永久关闭防火墙： chkconfig iptables off查看防火墙关闭状态: service iptables status 关闭selinux 服务vim /etc/selinux/config修改 SELINUX=disabled 无密码登录 ssh-keygen -t rsacat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keyschmod 600 ~/.ssh/authorized_keysssh localhost 显示文件夹大小并排序 du -h –max-depth=1 ./ | sort -n 将旧版本的jdk升级为新版本 sudo update-alternatives –install /usr/bin/java java /opt/soft/jdk/bin/java 300sudo update-alternatives –install /usr/bin/javac javac /opt/soft/jdk/bin/javac 300sudo update-alternatives –config java ecilpse访问的权限设置 groupadd supergroup # 添加supergroup组useradd -g supergroup hadoop # 添加hadoop用户到supergroup组hadoop fs -chmod 777 / 查看系统版本 其他系列 cat /proc/versionredhat系列 cat /etc/redhat-release 修改hostname centos 6vim /etc/sysconfig/networkhostname yourhostnamecentos 7hostnamectl set-hostname yourhostname Spark 上传jar包 /opt/moudles/spark-1.6.1/bin/spark-submit –class SparkWordCount sparkStudy.jar –master=spark://192.168.20.171:7077 其他 chmod -R 777 ./spark 时区修改 时区的配置文件是/etc/sysconfig/clock。用tzselect命令就可以修改这个配置文件，根据命令的提示进行修改就好了。但是在实际工作中，发现这种方式是不能够使得服务器上的时间设置马上生效的，而且使用ntpdate去同步时间服务器也不能够更改时间。即使你使用了 date命令手工设置了时间的话，如果使用ntpdate去进行时间同步的话，时间又会被改动到原来的错误时区的时间。而生产的机器往往是非常重要的，不能够进行重启等操作。如果要修改时区并且马上生效，可以更换/etc/localtime 文件来实现。比如修改时区为中国上海，那么就可以使用如下的命令来使得时区的更改生效。cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime然后最好使用下面的命令将更改写入bios。hwclock","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"}]},{"title":"SpagoBI5.2开发环境搭建","slug":"SpagoBI5.2开发环境搭建","date":"2018-04-11T04:00:38.000Z","updated":"2018-04-16T15:46:24.813Z","comments":true,"path":"2018/04/11/SpagoBI5.2开发环境搭建/","link":"","permalink":"http://yoursite.com/2018/04/11/SpagoBI5.2开发环境搭建/","excerpt":"","text":"本文暂不介绍kylin具体的搭建过程，而是将遇到的问题进行了总结，具体的搭建过程可能在后续进行更新。 本地运行SpagoBI时需要JDK1.8的运行环境。 表图引擎模块的action Bichartengine/WEB-INF/conf/commons/actions.xml 整个项目的Service Spagobi-core/src/it/eng/spagobi/wepp/service 导入sql文件出错时 12345mysql&gt; SET GLOBAL sql_mode = &apos;&apos;; Query OK, 0 rows affected, 1 warning (0.00 sec) mysql&gt; commit;Query OK, 0 rows affected (0.00 sec)mysql&gt; exit; 按照从SVN中导出的代码的Server.xml进行修改 同时修改hibernate.cfg.xml配置数据源 hibernate.cfg.xml (SpagoBI/src/main/resources) 注意Server.xml中配置数据库时注意大小写 1234567891011121314151617181920212223242526&lt;GlobalNamingResources&gt; &lt;!-- Editable user database that can also be used by UserDatabaseRealm to authenticate users --&gt; &lt;Resource auth=&quot;Container&quot; description=&quot;User database that can be updated and saved&quot; factory=&quot;org.apache.catalina.users.MemoryUserDatabaseFactory&quot; name=&quot;UserDatabase&quot; pathname=&quot;conf/tomcat-users.xml&quot; type=&quot;org.apache.catalina.UserDatabase”/&gt; &lt;Environment name=&quot;spagobi_resource_path&quot; type=&quot;java.lang.String&quot; value=&quot;C:\\progetti\\spagobi2.0\\workspace\\resources&quot;/&gt; &lt;Environment name=&quot;spagobi_sso_class&quot; type=&quot;java.lang.String&quot; value=&quot;it.eng.spagobi.services.common.FakeSsoService&quot;/&gt; &lt;Environment name=&quot;spagobi_service_url&quot; type=&quot;java.lang.String&quot; value=&quot;http://localhost:8080/SpagoBI&quot;/&gt; &lt;Environment name=&quot;spagobi_host_url&quot; type=&quot;java.lang.String&quot; value=&quot;http://localhost:8080&quot;/&gt; &lt;Resource auth=&quot;Container&quot; factory=&quot;de.myfoo.commonj.work.FooWorkManagerFactory&quot; maxThreads=&quot;5&quot; name=&quot;wm/SpagoWorkManager&quot; type=&quot;commonj.work.WorkManager&quot;/&gt; &lt;Resource auth=&quot;Container&quot; driverClassName=&quot;com.mysql.jdbc.Driver&quot; maxActive=&quot;20&quot; maxIdle=&quot;10&quot; maxWait=&quot;-1&quot; name=&quot;jdbc/foodmart&quot; password=&quot;root&quot; type=&quot;javax.sql.DataSource&quot; url=&quot;jdbc:mysql://localhost/foodmart&quot; username=&quot;root&quot;/&gt; &lt;Resource name=&quot;jdbc/spagobi&quot; auth=&quot;Container&quot; type=&quot;javax.sql.DataSource&quot; driverClassName=&quot;com.mysql.jdbc.Driver&quot; url=&quot;jdbc:mysql://localhost/spagobi&quot; username=&quot;root&quot; password=&quot;root&quot; maxActive=&quot;20&quot; maxIdle=&quot;10&quot; maxWait=&quot;-1&quot;/&gt; &lt;/GlobalNamingResources&gt; 虽然代码很多url等都写入了配置文件，但server.xml中的host url还是需要手动修改 记得将符合自己mysql版本的jar包放到tomcat的lib中 Unsupported major.minor version 52.0 这个错误不要把JRE更换到高版本，更换到高版本会报一个新的15.0错误，没用，而是应该修改web.xml 1&lt;web-app xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot; xmlns:web=&quot;http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd&quot; id=&quot;WebApp_ID&quot; version=&quot;3.0&quot; metadata-complete=&quot;true” &gt; 添加一句：metadata-complete=”true” 就搞定了，我猜想由于项目是原来的项目，web.xml是原来项目的，和原来的什么版本可能有冲突，具体的也不清楚了 按照src中的whatiftemplate的替换一下，可以修正错误 在部署过多项目，如果报内存溢出异常，则需要修改一下tomcat的设置 -Xms256m -Xmx512m -XX:MaxNewSize=256m -XX:MaxPermSize=256m Servlet mapping specifies an unknown servlet name AdapterHTTP Servlet 2.3 jar not loaded 在Java Resource 中找到对应jar包，然后exclude不知道是否正确","categories":[],"tags":[{"name":"spagobi","slug":"spagobi","permalink":"http://yoursite.com/tags/spagobi/"}]},{"title":"Kylin2.0环境搭建","slug":"Kylin2-0环境搭建","date":"2018-04-11T04:00:15.000Z","updated":"2018-04-16T15:44:32.844Z","comments":true,"path":"2018/04/11/Kylin2-0环境搭建/","link":"","permalink":"http://yoursite.com/2018/04/11/Kylin2-0环境搭建/","excerpt":"","text":"本文暂不介绍kylin具体的搭建过程，而是将遇到的问题进行了总结，具体的搭建过程可能在后续进行更新。 kylin相关组件启动命令12345678$ZOOKEEPER_HOME/bin/zkServer.sh start $HADOOP_HOME/sbin/start-dfs.sh $HADOOP_HOME/sbin/start-yarn.sh $HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver service mysqld start nohup $HIVE_HOME/bin/hive --service metastore &gt; /tmp/hive_metastore.log 2&gt;&amp;1 &amp; $HBASE_HOME/bin/start-hbase.sh$KYLIN_HOME/bin/kylin.sh start 其他命令1jar cv0f spark-libs.jar -C $KYLIN_HOME/spark/jars/ . 如出现以下错误 123Failed to load keystore type JKS with pathconf/.keystore due to /home/hadoop/apache-kylin-2.0.0-bin/tomcat/conf/.keystore(没有那个文件或目录)java.io.FileNotFoundException: /home/hadoop/apache-kylin-2.0.0-bin/tomcat/conf/.keystore(没有那个文件或目录) at java.io.FileInputStream.open(Native Method) 则去掉tomcat下的https在kylin内置tomcat的server.xml中里边有个对https的支持没启用的话 注释掉 自从spark2.0.0发布之后，每次启动hive的时候，总会发现一个小问题，启动 hive –service metastore的时候，会报一个小BUG: 无法访问/lib/spark-assembly-.jar分析其源码架构，发现主要原因是：在/bin/hive文件中，有这样的命令：加载spark中相关的JAR包。但是spark升级到spark2以后，原有lib目录下的大JAR包被分散成多个小JAR包，原来的spark-assembly-.jar已经不存在，所以hive没有办法找到这个JAR包。 1234hive&gt; show tables;OKFailed with exceptionjava.io.IOException:java.lang.IllegalArgumentException:java.net.URISyntaxException: Relative path in absolute URI:$&#123;system:user.name&#125;Time taken: 0.193 seconds 解决办法：把下列system:删除 123hive.exec.local.scratchdir$&#123;system:java.io.tmpdir&#125;/$&#123; system:user.name&#125;Localscratch space for Hive jobs 变成 123hive.exec.local.scratchdir$&#123;java.io.tmpdir&#125;/$&#123; user.name&#125;Localscratch space for Hive jobs","categories":[],"tags":[{"name":"kylin","slug":"kylin","permalink":"http://yoursite.com/tags/kylin/"}]},{"title":"项目总结-水环境管理系统","slug":"项目总结-水环境管理系统","date":"2018-04-11T02:48:21.000Z","updated":"2018-04-16T15:38:51.767Z","comments":true,"path":"2018/04/11/项目总结-水环境管理系统/","link":"","permalink":"http://yoursite.com/2018/04/11/项目总结-水环境管理系统/","excerpt":"","text":"待更新","categories":[],"tags":[]},{"title":"项目总结-BI系统","slug":"项目总结-BI系统","date":"2018-04-11T02:47:59.000Z","updated":"2018-04-16T15:38:35.275Z","comments":true,"path":"2018/04/11/项目总结-BI系统/","link":"","permalink":"http://yoursite.com/2018/04/11/项目总结-BI系统/","excerpt":"","text":"待更新","categories":[],"tags":[]},{"title":"项目总结-OLAP系统","slug":"项目总结-OLAP系统","date":"2018-04-11T02:47:31.000Z","updated":"2018-04-16T15:38:40.293Z","comments":true,"path":"2018/04/11/项目总结-OLAP系统/","link":"","permalink":"http://yoursite.com/2018/04/11/项目总结-OLAP系统/","excerpt":"","text":"待更新","categories":[],"tags":[]},{"title":"项目总结-离校未就业网站","slug":"项目总结-离校未就业转换网站","date":"2018-04-11T02:47:04.000Z","updated":"2018-04-16T15:39:05.567Z","comments":true,"path":"2018/04/11/项目总结-离校未就业转换网站/","link":"","permalink":"http://yoursite.com/2018/04/11/项目总结-离校未就业转换网站/","excerpt":"","text":"待更新","categories":[],"tags":[]},{"title":"Hello World","slug":"Hello World","date":"2018-04-02T04:33:09.872Z","updated":"2018-04-02T07:22:28.692Z","comments":true,"path":"2018/04/02/Hello World/","link":"","permalink":"http://yoursite.com/2018/04/02/Hello World/","excerpt":"","text":"Hello World","categories":[],"tags":[]}]}